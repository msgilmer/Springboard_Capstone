import streamlit as st
import numpy as np
import pandas as pd

from keras.models import load_model
import custom_funcs as cf

from music21 import instrument, note, chord, tempo, duration, stream

from PIL import Image

# Following hash_funcs arg was needed to resolve an error:
import tensorflow.python.keras.engine as e
@st.cache(hash_funcs={e.sequential.Sequential: id})
def get_model(filepath):
    harshness = 0.05    # weight factor for the duration component
                        # of the loss. This is the value the model
                        # was trained with.
    custom_objects = { 'maestro_loss': cf.maestro_loss_wr(harshness), \
        'f1_score_mod': cf.f1_score_mod, 'recall_mod': cf.recall_mod, \
        'precision_mod': cf.precision_mod, 'dur_error': cf.dur_error, \
        'maestro_dur_loss': cf.maestro_dur_loss_wr(harshness)}
    
    model = load_model(filepath, custom_objects = custom_objects)
    summary = []
    model.summary(print_fn = lambda x: summary.append(x))
    return model, '\n'.join(summary)

@st.cache()
def load_validation_set(filepath):
    
    maximum_duration = 6.545454545454545   # Would need to load in all
                                           # the data to calculate this
                                           # here
    X_val = np.load(filepath, allow_pickle = True)
    X_val[:, :, -1] *= maximum_duration    # convert back into seconds
    return X_val

@st.cache(hash_funcs={e.sequential.Sequential: id}, \
                      allow_output_mutation = True)
# to suppress a warning when no_of_timesteps or threshold are changed
                    
def generate_musical_sequence(model, random_music, \
                              no_of_timesteps = 16, threshold = 0.5):
    """Generate a musical sequence using model and from the sequence
    random_music as a starting point. We will perform an inference
    (and evaluation using threshold) no_of_timesteps times, each time
    appending the prediction onto random_music. After each prediction,
    the oldest vector in the sequence is popped until all of the
    original vectors in random_music are gone. We still keep adding
    on to random_music but we perform inference on the last 16 (as
    this was the window_size used during training). Finally, return
    both the original random_music and the new one."""

    n_keys_piano = 88
    window_size = 16
    original_random_music = random_music.copy()
    for i in range(no_of_timesteps):
        reshaped = random_music.reshape(1, max(window_size, i), \
                                            n_keys_piano + 1)
        prob = model.predict(reshaped)[0]
        y_pred = [0 if p < threshold else 1 for p in prob[:-1]] + [prob[-1]]
        if (i >= window_size):
            random_music = np.insert(random_music, len(random_music), \
                                     y_pred, axis = 0)
        else:
            random_music = np.insert(random_music, len(random_music), \
                                     y_pred, axis = 0)[1:, :]
    
    return original_random_music, np.array(random_music[:no_of_timesteps]).\
           astype(np.float64)        


def transpose_sequence(sequence, transposition = 0):
    """ Perform a left-shift on the keys' part of the vectors
      Effectively, this outputs a new sequence repesenting
      a song but transposed. The size of the shift is
      transposition"""
    if (transposition == 0):
        return sequence
    shift = transposition
    sequence, durations = sequence[:, :-1], sequence[:, -1]
    for i in range(len(sequence)):
        sequence[i] = np.concatenate((sequence[i][shift:], \
                                      sequence[i][:shift]))
    return np.insert(sequence, len(sequence[0]), durations, axis = 1)

@st.cache()
def convert_to_midi(sequence, bpm = 60, output_file = '../midi_output/music.mid'):
    """Save sequence as a midi file (with path = output_file). sequence
    can be from the original dataset or a new sequence generated by a
    trained model"""
    offset = 0    # the distance in quarter-notes of the note/chord/rest
                  # being written from the beginning
    output_notes = [instrument.Piano(), tempo.MetronomeMark(number = bpm)]
    
    all_notes = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', \
                 'G', 'G#']

    bps = bpm / 60    # beats per second
    converted_duration = duration.Duration()
    # create note, chord, and rest objects
    for vector in sequence:
        # convert from seconds to beats
        converted_duration.quarterLength = vector[-1] * bps
        
        if (np.sum(vector[:-1]) > 1):      # chord
            indices_in_chord = np.argsort(vector[:-1])[-int(np.sum(\
                vector[:-1])):]
            notes_in_chord = [all_notes[i % len(all_notes)] + str((i // \
                           len(all_notes)) + 1) for i in indices_in_chord]
            notes = []
            for cur_note in notes_in_chord:
                new_note = note.Note(cur_note)
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            new_chord.duration = converted_duration
            output_notes.append(new_chord)
            
        elif (np.sum(vector[:-1]) == 1):   # note
            index = np.argmax(vector[:-1])
            new_note = all_notes[index % len(all_notes)] + str((index // \
                                                    len(all_notes)) + 1)
            new_note = note.Note(new_note)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            new_note.duration = converted_duration
            output_notes.append(new_note)
        
        elif (np.sum(vector[:-1]) == 0):   # rest
            new_rest = note.Rest()
            new_rest.offset = offset
            new_rest.duration = converted_duration
            output_notes.append(new_rest)
                                                               
        offset += vector[-1]
                                                               
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp = output_file)

from os import path
from base64 import b64encode
def get_binary_file_downloader_html(bin_file, file_label='File'):
    with open(bin_file, 'rb') as f:
        data = f.read()
    bin_str = b64encode(data).decode()
    href = f'<a href="data:application/octet-stream;base64,{bin_str}" download="{path.basename(bin_file)}">Download {file_label}</a>'
    return href
    
if __name__ == '__main__':

    st.title('ChopinBot 1.0: Music Generation with an Long Short-Term' \
             + ' Memory (LSTM) Neural Network')

    portrait = Image.open('../images/chopin.jpg')

    caption = '3D Portrait of Fr\u00E9d\u00E9ric Chopin from ' + \
              'https://hadikarimi.com/portfolio/frederic-chopin'
    st.image(portrait, caption = caption.encode('utf-8').decode('utf-8'), \
             use_column_width = True)
             
    model, summary = get_model('../models/best_maestro_model_2_1_512_0pt4_' + \
                               'lr_5e-04_cn_1pt0.h5')
             
    X_val = load_validation_set('../train_and_val/X_val.npy')

    st.sidebar.title('Controls')
                               
    if (st.sidebar.button('Generate New Seed Index')):
        # Choose randomly and write to file
        seed_index = np.random.randint(len(X_val) - 1)
        with open('seed_index.txt', 'w') as f:
            f.write(str(seed_index))
    else:
        # Read from file
        with open('seed_index.txt', 'r') as f:
            seed_index = int(f.read())

    st.sidebar.write('Seed Index = {}'.format(seed_index))

    no_of_timesteps = st.sidebar.slider('# of timesteps to predict', 1, \
                                        320, 16)

    st.sidebar.write('no_of_timesteps = {}'.format(no_of_timesteps))
    
    threshold = st.sidebar.number_input(\
        'Set probability threshold in range (0, 1)', min_value = 0.0, \
        max_value = 1.0, value = 0.5, key = 'threshold')

    seed_music, generated_music = generate_musical_sequence(model, \
                        X_val[seed_index], no_of_timesteps, threshold)
    
    keys = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    key = st.sidebar.selectbox('Choose a musical key', keys)

    if (key == 'C'):
        transposition = 0
    else:
        transposition = 12 - keys.index(key)  # transpose leftward
    transposed_generated_music = transpose_sequence(generated_music, \
                                        transposition = transposition)
    transposed_seed_music = transpose_sequence(seed_music, transposition = \
                                           transposition)

    bpm = st.sidebar.number_input('Set the bpm (beats per minute) in the '\
                        'range [20, 180]', min_value = 20, max_value = 180,\
                         value = 60, key = 'bpm') 

    for music_type in ['seed', 'generated']:                                                             
        if (st.button('Create MIDI File for the ' + music_type.title() + \
                      ' Music', key = music_type)):
            exec(('convert_to_midi(transposed_{0}_music, bpm = bpm, ' + \
                 'output_file = \'../midi_output/{0}.mid\')').format(music_type))
            st.markdown(get_binary_file_downloader_html('../midi_output/' + \
                    music_type + '.mid', 'MIDI'), unsafe_allow_html = True)


    

