{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data we created in data_read_and_process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../train_and_val/X_train.npy')\n",
    "X_val = np.load('../train_and_val/X_val.npy')\n",
    "y_train = np.load('../train_and_val/y_train.npy')\n",
    "y_val = np.load('../train_and_val/y_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4):\n",
    "    \"\"\"Generate a keras Sequential model of the form as described in Figure 16 of\n",
    "    https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_lstm_nodes, return_sequences = True, input_shape = (16, 89,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(1, n_lstm_layers - 1):\n",
    "        model.add(LSTM(n_lstm_nodes, return_sequences = True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(n_lstm_nodes))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_lstm_nodes // 2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(n_dense_layers - 1):\n",
    "        model.add(Dense(n_lstm_nodes // 2))\n",
    "        model.add(Dropout(0.6))\n",
    "    model.add(Dense(89))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'RMSProp')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss and Metrics\n",
    "\n",
    "\\begin{equation*}\n",
    "bce\\_loss = \\frac{1}{N} (\\sum_{i=1}^{N} y_i log(p(y_i)) + (1 - y_i) log(1 - p(y_i)))\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "maestro\\_loss = 2 * Harshness \\lvert\\frac{d_{true} - d_{pred}}{d_{true} + d{_{pred}}}\\rvert\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\\begin{equation*}\n",
    "total\\_loss = MIN(2 * bce\\_loss, bce\\_loss + maestro\\_loss)\n",
    "\\end{equation*}\n",
    "\n",
    "where N = num_keys_piano, <b>Harshness</b> is a constant to be determined, and <b>d</b> gives the normalized duration. I'll call it the <b>Maestro Loss Function</b> since it pays special attention to the timing of the notes. It is usually composed of a Binary Cross Entropy Term with an additional term proportional to the relative error in duration between $d_{true}$ and $d_{pred}$. However, we limit the total_loss to be less than twice the bce_loss. We also define custom metrics, read the docstrings for their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def maestro_loss_wr(harshness): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last element (which repesents the duration). \n",
    "    The proportionality constant is the 'harshness' of the maestro in regards to\n",
    "    timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-1] * K.log(ypred[:, :-1]) + (1 - ytrue[:, :-1]) * \\\n",
    "                     K.log(1 - ypred[:, :-1]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, -1] elements will be zero\n",
    "            return bce_loss * 2     # This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "\n",
    "def precision_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified precision excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    pred_positives = K.sum(K.round(ypred[:, :-1]))\n",
    "    return true_positives / (pred_positives + K.epsilon())\n",
    "\n",
    "def recall_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified recall excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    poss_positives = K.sum(ytrue[:, :-1])\n",
    "    return true_positives / (poss_positives + K.epsilon())\n",
    "\n",
    "def f1_score_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified f1_score excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    precision = precision_mod(ytrue, ypred)\n",
    "    recall = recall_mod(ytrue, ypred)   \n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def dur_error(ytrue, ypred):\n",
    "    \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "    return 2 * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / (ytrue[:, -1] + ypred[:, -1] + \\\n",
    "                                                         K.epsilon())))\n",
    "\n",
    "def maestro_dur_loss_wr(harshness):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cols_dict(history):\n",
    "    \"\"\"return a mapping of desired column names to the corresponding columns in the\n",
    "    history dictionary (previously history.history where history is the return value\n",
    "    of model.train)\"\"\"\n",
    "    return {'maestro_loss': history['loss'], 'f1_score': history['f1_score_mod'], \\\n",
    " 'precision': history['precision_mod'], 'recall': history['recall_mod'], \\\n",
    " 'dur_error': history['dur_error'], 'dur_loss': history['maestro_dur_loss'], \\\n",
    " 'val_maestro_loss': history['val_loss'], 'val_f1_score': history['val_f1_score_mod'], \\\n",
    " 'val_precision': history['val_precision_mod'], 'val_recall': history['val_recall_mod'], \\\n",
    " 'val_dur_error': history['val_dur_error'], 'val_dur_loss': history['val_maestro_dur_loss']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a similar model to [this paper](https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972). To save time, let us use 2 LSTM layers and 1 Dense layer instead of (4 and 3 as in the paper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4, \\\n",
    "                     batch_size = 512, harshness = 0.1, lr = None, clipnorm = None, clipvalue = None, \\\n",
    "                     epochs = 150):\n",
    "    \"\"\"Train a model using the passed parameters, the data, and using the RMSprop optimizer. Write the\n",
    "    best model as a .h5 and a .csv containing columns for the training and validation custom loss and\n",
    "    metrics. Returns nothing.\"\"\"\n",
    "    model = lstm(n_lstm_layers = n_lstm_layers, n_dense_layers = n_dense_layers, \\\n",
    "                 n_lstm_nodes = n_lstm_nodes, dropout_rate = dropout_rate)\n",
    "\n",
    "    if (lr or clipnorm or clipvalue):\n",
    "        if (lr):          # It's required that the first argument to RMSprop is not None\n",
    "            opt = RMSprop(lr = lr, clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        elif (clipnorm):\n",
    "            opt = RMSprop(clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        else: # clipvalue\n",
    "            opt = RMSprop(clipvalue = clipvalue)\n",
    "    else:\n",
    "        opt = RMSprop()   # TypeError when all are None, so do this instead\n",
    "        \n",
    "    model.compile(loss = maestro_loss_wr(harshness), optimizer = opt, metrics = [f1_score_mod, recall_mod, \\\n",
    "                                                precision_mod, dur_error, maestro_dur_loss_wr(harshness)])\n",
    "    \n",
    "    filename = 'best_maestro_model_{0}_{1}_{2}_{3}'.format(n_lstm_layers, n_dense_layers, n_lstm_nodes, \\\n",
    "                                                          str(dropout_rate).replace('.', 'pt'))\n",
    "    if (lr):\n",
    "        filename += '_lr_{}'.format('%.0e' % Decimal(lr))\n",
    "    if (clipnorm):\n",
    "        filename += '_cn_{}'.format(str(clipnorm).replace('.', 'pt'))     \n",
    "    if (clipvalue):\n",
    "        filename += '_cv_{}'.format(str(clipvalue).replace('.', 'pt'))\n",
    "                                   \n",
    "    mc = ModelCheckpoint('../models/' + filename + '.h5', monitor = 'val_loss', mode = 'min', \\\n",
    "                                                         save_best_only = True, verbose = 1)\n",
    "                                   \n",
    "    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "    \n",
    "    # In most preliminary tests model training has failed at some point when the loss becomes NaN during\n",
    "    # validation\n",
    "    if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "        for key, value in history.history.items():\n",
    "            if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "                value.append(np.nan)\n",
    "                \n",
    "    df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "    df.index.name = 'Epochs'\n",
    "    df.to_csv('../model_data/' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16632, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 139s - loss: 0.2706 - f1_score_mod: 0.0117 - recall_mod: 0.0255 - precision_mod: 0.0858 - dur_error: 0.9370 - maestro_dur_loss: 0.0937 - val_loss: 0.1663 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4517 - val_maestro_dur_loss: 0.0452\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16632 to 0.16358, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 137s - loss: 0.1966 - f1_score_mod: 0.0054 - recall_mod: 0.0028 - precision_mod: 0.2857 - dur_error: 0.6540 - maestro_dur_loss: 0.0654 - val_loss: 0.1636 - val_f1_score_mod: 0.0018 - val_recall_mod: 9.2372e-04 - val_precision_mod: 0.6712 - val_dur_error: 0.4682 - val_maestro_dur_loss: 0.0468\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16358\n",
      "50/50 - 135s - loss: 0.1844 - f1_score_mod: 0.0233 - recall_mod: 0.0121 - precision_mod: 0.4405 - dur_error: 0.6072 - maestro_dur_loss: 0.0607 - val_loss: 0.1650 - val_f1_score_mod: 0.0270 - val_recall_mod: 0.0139 - val_precision_mod: 0.5645 - val_dur_error: 0.5144 - val_maestro_dur_loss: 0.0514\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16358\n",
      "50/50 - 136s - loss: 0.1775 - f1_score_mod: 0.0428 - recall_mod: 0.0225 - precision_mod: 0.5445 - dur_error: 0.5879 - maestro_dur_loss: 0.0588 - val_loss: 0.1640 - val_f1_score_mod: 0.0795 - val_recall_mod: 0.0425 - val_precision_mod: 0.6478 - val_dur_error: 0.5404 - val_maestro_dur_loss: 0.0540\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16358 to 0.16324, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 136s - loss: 0.1727 - f1_score_mod: 0.0651 - recall_mod: 0.0348 - precision_mod: 0.5750 - dur_error: 0.5688 - maestro_dur_loss: 0.0569 - val_loss: 0.1632 - val_f1_score_mod: 0.0991 - val_recall_mod: 0.0540 - val_precision_mod: 0.6254 - val_dur_error: 0.5577 - val_maestro_dur_loss: 0.0558\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16324 to 0.14935, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 135s - loss: 0.1688 - f1_score_mod: 0.0837 - recall_mod: 0.0452 - precision_mod: 0.6153 - dur_error: 0.5575 - maestro_dur_loss: 0.0557 - val_loss: 0.1494 - val_f1_score_mod: 0.0661 - val_recall_mod: 0.0347 - val_precision_mod: 0.7632 - val_dur_error: 0.4313 - val_maestro_dur_loss: 0.0431\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.14935 to 0.14702, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 150s - loss: 0.1649 - f1_score_mod: 0.0995 - recall_mod: 0.0543 - precision_mod: 0.6324 - dur_error: 0.5399 - maestro_dur_loss: 0.0540 - val_loss: 0.1470 - val_f1_score_mod: 0.0785 - val_recall_mod: 0.0415 - val_precision_mod: 0.7742 - val_dur_error: 0.4354 - val_maestro_dur_loss: 0.0435\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14702 to 0.13999, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 142s - loss: 0.1629 - f1_score_mod: 0.1144 - recall_mod: 0.0632 - precision_mod: 0.6386 - dur_error: 0.5333 - maestro_dur_loss: 0.0533 - val_loss: 0.1400 - val_f1_score_mod: 0.0924 - val_recall_mod: 0.0493 - val_precision_mod: 0.7602 - val_dur_error: 0.3724 - val_maestro_dur_loss: 0.0372\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.13999 to 0.13553, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 154s - loss: 0.1599 - f1_score_mod: 0.1266 - recall_mod: 0.0705 - precision_mod: 0.6434 - dur_error: 0.5199 - maestro_dur_loss: 0.0520 - val_loss: 0.1355 - val_f1_score_mod: 0.1231 - val_recall_mod: 0.0676 - val_precision_mod: 0.7023 - val_dur_error: 0.3477 - val_maestro_dur_loss: 0.0348\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13553\n",
      "50/50 - 141s - loss: 0.1572 - f1_score_mod: 0.1367 - recall_mod: 0.0768 - precision_mod: 0.6465 - dur_error: 0.5030 - maestro_dur_loss: 0.0503 - val_loss: 0.1366 - val_f1_score_mod: 0.1279 - val_recall_mod: 0.0703 - val_precision_mod: 0.7252 - val_dur_error: 0.3550 - val_maestro_dur_loss: 0.0355\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13553 to 0.13446, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 135s - loss: 0.1552 - f1_score_mod: 0.1437 - recall_mod: 0.0812 - precision_mod: 0.6487 - dur_error: 0.4936 - maestro_dur_loss: 0.0494 - val_loss: 0.1345 - val_f1_score_mod: 0.1444 - val_recall_mod: 0.0806 - val_precision_mod: 0.7145 - val_dur_error: 0.3553 - val_maestro_dur_loss: 0.0355\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.13446 to 0.12963, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 138s - loss: 0.1533 - f1_score_mod: 0.1500 - recall_mod: 0.0850 - precision_mod: 0.6545 - dur_error: 0.4836 - maestro_dur_loss: 0.0484 - val_loss: 0.1296 - val_f1_score_mod: 0.1628 - val_recall_mod: 0.0928 - val_precision_mod: 0.6765 - val_dur_error: 0.3115 - val_maestro_dur_loss: 0.0312\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12963\n",
      "50/50 - 131s - loss: 0.1513 - f1_score_mod: 0.1599 - recall_mod: 0.0911 - precision_mod: 0.6714 - dur_error: 0.4731 - maestro_dur_loss: 0.0473 - val_loss: 0.1382 - val_f1_score_mod: 0.1658 - val_recall_mod: 0.0943 - val_precision_mod: 0.6992 - val_dur_error: 0.3987 - val_maestro_dur_loss: 0.0399\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12963\n",
      "50/50 - 130s - loss: 0.1505 - f1_score_mod: 0.1690 - recall_mod: 0.0970 - precision_mod: 0.6720 - dur_error: 0.4718 - maestro_dur_loss: 0.0472 - val_loss: 0.1333 - val_f1_score_mod: 0.1605 - val_recall_mod: 0.0903 - val_precision_mod: 0.7373 - val_dur_error: 0.3567 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12963\n",
      "50/50 - 131s - loss: 0.1489 - f1_score_mod: 0.1736 - recall_mod: 0.1001 - precision_mod: 0.6728 - dur_error: 0.4640 - maestro_dur_loss: 0.0464 - val_loss: 0.1334 - val_f1_score_mod: 0.1549 - val_recall_mod: 0.0863 - val_precision_mod: 0.7690 - val_dur_error: 0.3626 - val_maestro_dur_loss: 0.0363\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12963\n",
      "50/50 - 132s - loss: 0.1475 - f1_score_mod: 0.1811 - recall_mod: 0.1050 - precision_mod: 0.6702 - dur_error: 0.4573 - maestro_dur_loss: 0.0457 - val_loss: 0.1347 - val_f1_score_mod: 0.1644 - val_recall_mod: 0.0926 - val_precision_mod: 0.7550 - val_dur_error: 0.3774 - val_maestro_dur_loss: 0.0377\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.12963\n",
      "50/50 - 132s - loss: 0.1462 - f1_score_mod: 0.1841 - recall_mod: 0.1069 - precision_mod: 0.6741 - dur_error: 0.4485 - maestro_dur_loss: 0.0449 - val_loss: 0.1318 - val_f1_score_mod: 0.1652 - val_recall_mod: 0.0931 - val_precision_mod: 0.7472 - val_dur_error: 0.3583 - val_maestro_dur_loss: 0.0358\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12963\n",
      "50/50 - 131s - loss: 0.1453 - f1_score_mod: 0.1918 - recall_mod: 0.1118 - precision_mod: 0.6889 - dur_error: 0.4467 - maestro_dur_loss: 0.0447 - val_loss: 0.1308 - val_f1_score_mod: 0.1868 - val_recall_mod: 0.1072 - val_precision_mod: 0.7367 - val_dur_error: 0.3508 - val_maestro_dur_loss: 0.0351\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.12963 to 0.12947, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1441 - f1_score_mod: 0.1977 - recall_mod: 0.1160 - precision_mod: 0.6799 - dur_error: 0.4417 - maestro_dur_loss: 0.0442 - val_loss: 0.1295 - val_f1_score_mod: 0.2001 - val_recall_mod: 0.1168 - val_precision_mod: 0.7107 - val_dur_error: 0.3465 - val_maestro_dur_loss: 0.0346\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12947\n",
      "50/50 - 132s - loss: 0.1424 - f1_score_mod: 0.2031 - recall_mod: 0.1194 - precision_mod: 0.6919 - dur_error: 0.4320 - maestro_dur_loss: 0.0432 - val_loss: 0.1360 - val_f1_score_mod: 0.1971 - val_recall_mod: 0.1145 - val_precision_mod: 0.7207 - val_dur_error: 0.4093 - val_maestro_dur_loss: 0.0409\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12947\n",
      "50/50 - 133s - loss: 0.1423 - f1_score_mod: 0.2081 - recall_mod: 0.1228 - precision_mod: 0.6928 - dur_error: 0.4359 - maestro_dur_loss: 0.0436 - val_loss: 0.1468 - val_f1_score_mod: 0.2270 - val_recall_mod: 0.1360 - val_precision_mod: 0.6971 - val_dur_error: 0.5192 - val_maestro_dur_loss: 0.0519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.12947 to 0.12365, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 135s - loss: 0.1415 - f1_score_mod: 0.2150 - recall_mod: 0.1277 - precision_mod: 0.6904 - dur_error: 0.4323 - maestro_dur_loss: 0.0432 - val_loss: 0.1237 - val_f1_score_mod: 0.2053 - val_recall_mod: 0.1192 - val_precision_mod: 0.7540 - val_dur_error: 0.3055 - val_maestro_dur_loss: 0.0305\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12365\n",
      "50/50 - 138s - loss: 0.1402 - f1_score_mod: 0.2184 - recall_mod: 0.1298 - precision_mod: 0.6991 - dur_error: 0.4258 - maestro_dur_loss: 0.0426 - val_loss: 0.1440 - val_f1_score_mod: 0.2280 - val_recall_mod: 0.1368 - val_precision_mod: 0.6988 - val_dur_error: 0.5063 - val_maestro_dur_loss: 0.0506\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12365\n",
      "50/50 - 150s - loss: 0.1396 - f1_score_mod: 0.2234 - recall_mod: 0.1337 - precision_mod: 0.6900 - dur_error: 0.4250 - maestro_dur_loss: 0.0425 - val_loss: 0.1346 - val_f1_score_mod: 0.2285 - val_recall_mod: 0.1361 - val_precision_mod: 0.7223 - val_dur_error: 0.4197 - val_maestro_dur_loss: 0.0420\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12365\n",
      "50/50 - 141s - loss: 0.1382 - f1_score_mod: 0.2262 - recall_mod: 0.1353 - precision_mod: 0.6985 - dur_error: 0.4186 - maestro_dur_loss: 0.0419 - val_loss: 0.1286 - val_f1_score_mod: 0.2253 - val_recall_mod: 0.1330 - val_precision_mod: 0.7466 - val_dur_error: 0.3663 - val_maestro_dur_loss: 0.0366\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.12365 to 0.12296, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 137s - loss: 0.1378 - f1_score_mod: 0.2352 - recall_mod: 0.1414 - precision_mod: 0.7092 - dur_error: 0.4189 - maestro_dur_loss: 0.0419 - val_loss: 0.1230 - val_f1_score_mod: 0.2277 - val_recall_mod: 0.1347 - val_precision_mod: 0.7501 - val_dur_error: 0.3164 - val_maestro_dur_loss: 0.0316\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12296\n",
      "50/50 - 139s - loss: 0.1366 - f1_score_mod: 0.2407 - recall_mod: 0.1453 - precision_mod: 0.7094 - dur_error: 0.4126 - maestro_dur_loss: 0.0413 - val_loss: 0.1266 - val_f1_score_mod: 0.2400 - val_recall_mod: 0.1441 - val_precision_mod: 0.7295 - val_dur_error: 0.3509 - val_maestro_dur_loss: 0.0351\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12296\n",
      "50/50 - 140s - loss: 0.1360 - f1_score_mod: 0.2421 - recall_mod: 0.1463 - precision_mod: 0.7081 - dur_error: 0.4097 - maestro_dur_loss: 0.0410 - val_loss: 0.1232 - val_f1_score_mod: 0.2536 - val_recall_mod: 0.1545 - val_precision_mod: 0.7195 - val_dur_error: 0.3213 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.12296 to 0.11844, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 141s - loss: 0.1352 - f1_score_mod: 0.2514 - recall_mod: 0.1532 - precision_mod: 0.7076 - dur_error: 0.4095 - maestro_dur_loss: 0.0409 - val_loss: 0.1184 - val_f1_score_mod: 0.2429 - val_recall_mod: 0.1455 - val_precision_mod: 0.7465 - val_dur_error: 0.2795 - val_maestro_dur_loss: 0.0280\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11844\n",
      "50/50 - 147s - loss: 0.1343 - f1_score_mod: 0.2530 - recall_mod: 0.1545 - precision_mod: 0.7092 - dur_error: 0.4040 - maestro_dur_loss: 0.0404 - val_loss: 0.1199 - val_f1_score_mod: 0.2421 - val_recall_mod: 0.1449 - val_precision_mod: 0.7476 - val_dur_error: 0.2947 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11844\n",
      "50/50 - 143s - loss: 0.1338 - f1_score_mod: 0.2608 - recall_mod: 0.1600 - precision_mod: 0.7128 - dur_error: 0.4051 - maestro_dur_loss: 0.0405 - val_loss: 0.1186 - val_f1_score_mod: 0.2526 - val_recall_mod: 0.1523 - val_precision_mod: 0.7542 - val_dur_error: 0.2886 - val_maestro_dur_loss: 0.0289\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11844\n",
      "50/50 - 141s - loss: 0.1326 - f1_score_mod: 0.2669 - recall_mod: 0.1645 - precision_mod: 0.7150 - dur_error: 0.3986 - maestro_dur_loss: 0.0399 - val_loss: 0.1220 - val_f1_score_mod: 0.2600 - val_recall_mod: 0.1584 - val_precision_mod: 0.7390 - val_dur_error: 0.3243 - val_maestro_dur_loss: 0.0324\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11844\n",
      "50/50 - 131s - loss: 0.1318 - f1_score_mod: 0.2690 - recall_mod: 0.1661 - precision_mod: 0.7159 - dur_error: 0.3963 - maestro_dur_loss: 0.0396 - val_loss: 0.1203 - val_f1_score_mod: 0.2534 - val_recall_mod: 0.1532 - val_precision_mod: 0.7466 - val_dur_error: 0.3077 - val_maestro_dur_loss: 0.0308\n",
      "Epoch 34/150\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11844\n",
      "50/50 - 130s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll refer to the above model as the 'base_model'. Looks like we get a NaN loss while the model was still improving. This seems to be case of exploding gradients. To combat this, we can try lowering the learning rate as suggested [here](https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17649, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 164s - loss: 0.3093 - f1_score_mod: 0.0226 - recall_mod: 0.0388 - precision_mod: 0.0557 - dur_error: 1.0166 - maestro_dur_loss: 0.1017 - val_loss: 0.1765 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5228 - val_maestro_dur_loss: 0.0523\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17649 to 0.16569, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 149s - loss: 0.2098 - f1_score_mod: 0.0019 - recall_mod: 9.6151e-04 - precision_mod: 0.1648 - dur_error: 0.7058 - maestro_dur_loss: 0.0706 - val_loss: 0.1657 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4470 - val_maestro_dur_loss: 0.0447\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16569\n",
      "50/50 - 142s - loss: 0.1944 - f1_score_mod: 0.0028 - recall_mod: 0.0015 - precision_mod: 0.3062 - dur_error: 0.6299 - maestro_dur_loss: 0.0630 - val_loss: 0.1677 - val_f1_score_mod: 3.2324e-04 - val_recall_mod: 1.6194e-04 - val_precision_mod: 0.1545 - val_dur_error: 0.4948 - val_maestro_dur_loss: 0.0495\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16569\n",
      "50/50 - 146s - loss: 0.1850 - f1_score_mod: 0.0095 - recall_mod: 0.0048 - precision_mod: 0.5074 - dur_error: 0.5933 - maestro_dur_loss: 0.0593 - val_loss: 0.1675 - val_f1_score_mod: 0.0060 - val_recall_mod: 0.0030 - val_precision_mod: 0.7120 - val_dur_error: 0.5286 - val_maestro_dur_loss: 0.0529\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16569 to 0.15419, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 141s - loss: 0.1790 - f1_score_mod: 0.0192 - recall_mod: 0.0098 - precision_mod: 0.5106 - dur_error: 0.5717 - maestro_dur_loss: 0.0572 - val_loss: 0.1542 - val_f1_score_mod: 0.0099 - val_recall_mod: 0.0050 - val_precision_mod: 0.6864 - val_dur_error: 0.4101 - val_maestro_dur_loss: 0.0410\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15419\n",
      "50/50 - 134s - loss: 0.1747 - f1_score_mod: 0.0271 - recall_mod: 0.0139 - precision_mod: 0.5582 - dur_error: 0.5568 - maestro_dur_loss: 0.0557 - val_loss: 0.1663 - val_f1_score_mod: 0.0413 - val_recall_mod: 0.0215 - val_precision_mod: 0.6191 - val_dur_error: 0.5403 - val_maestro_dur_loss: 0.0540\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15419 to 0.14981, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1706 - f1_score_mod: 0.0399 - recall_mod: 0.0208 - precision_mod: 0.5813 - dur_error: 0.5386 - maestro_dur_loss: 0.0539 - val_loss: 0.1498 - val_f1_score_mod: 0.0456 - val_recall_mod: 0.0236 - val_precision_mod: 0.7473 - val_dur_error: 0.4110 - val_maestro_dur_loss: 0.0411\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14981\n",
      "50/50 - 131s - loss: 0.1681 - f1_score_mod: 0.0494 - recall_mod: 0.0259 - precision_mod: 0.6019 - dur_error: 0.5317 - maestro_dur_loss: 0.0532 - val_loss: 0.1619 - val_f1_score_mod: 0.0629 - val_recall_mod: 0.0333 - val_precision_mod: 0.6359 - val_dur_error: 0.5326 - val_maestro_dur_loss: 0.0533\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14981 to 0.14610, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 134s - loss: 0.1650 - f1_score_mod: 0.0592 - recall_mod: 0.0312 - precision_mod: 0.6196 - dur_error: 0.5133 - maestro_dur_loss: 0.0513 - val_loss: 0.1461 - val_f1_score_mod: 0.0440 - val_recall_mod: 0.0227 - val_precision_mod: 0.8235 - val_dur_error: 0.3962 - val_maestro_dur_loss: 0.0396\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14610 to 0.14228, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 155s - loss: 0.1627 - f1_score_mod: 0.0700 - recall_mod: 0.0372 - precision_mod: 0.6262 - dur_error: 0.5035 - maestro_dur_loss: 0.0504 - val_loss: 0.1423 - val_f1_score_mod: 0.0679 - val_recall_mod: 0.0357 - val_precision_mod: 0.7467 - val_dur_error: 0.3690 - val_maestro_dur_loss: 0.0369\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14228\n",
      "50/50 - 142s - loss: 0.1600 - f1_score_mod: 0.0782 - recall_mod: 0.0418 - precision_mod: 0.6390 - dur_error: 0.4883 - maestro_dur_loss: 0.0488 - val_loss: 0.1572 - val_f1_score_mod: 0.0886 - val_recall_mod: 0.0473 - val_precision_mod: 0.7252 - val_dur_error: 0.5286 - val_maestro_dur_loss: 0.0529\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.14228 to 0.13507, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 138s - loss: 0.1583 - f1_score_mod: 0.0858 - recall_mod: 0.0461 - precision_mod: 0.6459 - dur_error: 0.4793 - maestro_dur_loss: 0.0479 - val_loss: 0.1351 - val_f1_score_mod: 0.0964 - val_recall_mod: 0.0518 - val_precision_mod: 0.7299 - val_dur_error: 0.3183 - val_maestro_dur_loss: 0.0318\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13507\n",
      "50/50 - 145s - loss: 0.1562 - f1_score_mod: 0.0975 - recall_mod: 0.0528 - precision_mod: 0.6523 - dur_error: 0.4669 - maestro_dur_loss: 0.0467 - val_loss: 0.1407 - val_f1_score_mod: 0.1108 - val_recall_mod: 0.0603 - val_precision_mod: 0.7096 - val_dur_error: 0.3798 - val_maestro_dur_loss: 0.0380\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13507\n",
      "50/50 - 148s - loss: 0.1554 - f1_score_mod: 0.1067 - recall_mod: 0.0582 - precision_mod: 0.6600 - dur_error: 0.4686 - maestro_dur_loss: 0.0469 - val_loss: 0.1453 - val_f1_score_mod: 0.1086 - val_recall_mod: 0.0589 - val_precision_mod: 0.7120 - val_dur_error: 0.4285 - val_maestro_dur_loss: 0.0428\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13507\n",
      "50/50 - 144s - loss: 0.1534 - f1_score_mod: 0.1116 - recall_mod: 0.0612 - precision_mod: 0.6535 - dur_error: 0.4564 - maestro_dur_loss: 0.0456 - val_loss: 0.1470 - val_f1_score_mod: 0.1313 - val_recall_mod: 0.0726 - val_precision_mod: 0.7025 - val_dur_error: 0.4539 - val_maestro_dur_loss: 0.0454\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13507 to 0.13133, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 133s - loss: 0.1523 - f1_score_mod: 0.1185 - recall_mod: 0.0653 - precision_mod: 0.6620 - dur_error: 0.4520 - maestro_dur_loss: 0.0452 - val_loss: 0.1313 - val_f1_score_mod: 0.1342 - val_recall_mod: 0.0745 - val_precision_mod: 0.6938 - val_dur_error: 0.3050 - val_maestro_dur_loss: 0.0305\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13133\n",
      "50/50 - 133s - loss: 0.1513 - f1_score_mod: 0.1263 - recall_mod: 0.0699 - precision_mod: 0.6644 - dur_error: 0.4498 - maestro_dur_loss: 0.0450 - val_loss: 0.1322 - val_f1_score_mod: 0.1324 - val_recall_mod: 0.0730 - val_precision_mod: 0.7307 - val_dur_error: 0.3180 - val_maestro_dur_loss: 0.0318\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13133 to 0.12908, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 133s - loss: 0.1499 - f1_score_mod: 0.1334 - recall_mod: 0.0743 - precision_mod: 0.6739 - dur_error: 0.4417 - maestro_dur_loss: 0.0442 - val_loss: 0.1291 - val_f1_score_mod: 0.1294 - val_recall_mod: 0.0711 - val_precision_mod: 0.7374 - val_dur_error: 0.2923 - val_maestro_dur_loss: 0.0292\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12908\n",
      "50/50 - 152s - loss: 0.1488 - f1_score_mod: 0.1384 - recall_mod: 0.0772 - precision_mod: 0.6781 - dur_error: 0.4388 - maestro_dur_loss: 0.0439 - val_loss: 0.1311 - val_f1_score_mod: 0.1389 - val_recall_mod: 0.0768 - val_precision_mod: 0.7414 - val_dur_error: 0.3208 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.12908 to 0.12904, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 157s - loss: 0.1479 - f1_score_mod: 0.1427 - recall_mod: 0.0801 - precision_mod: 0.6738 - dur_error: 0.4344 - maestro_dur_loss: 0.0434 - val_loss: 0.1290 - val_f1_score_mod: 0.1375 - val_recall_mod: 0.0758 - val_precision_mod: 0.7519 - val_dur_error: 0.3048 - val_maestro_dur_loss: 0.0305\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12904\n",
      "50/50 - 153s - loss: 0.1472 - f1_score_mod: 0.1499 - recall_mod: 0.0844 - precision_mod: 0.6841 - dur_error: 0.4334 - maestro_dur_loss: 0.0433 - val_loss: 0.1338 - val_f1_score_mod: 0.1569 - val_recall_mod: 0.0881 - val_precision_mod: 0.7346 - val_dur_error: 0.3544 - val_maestro_dur_loss: 0.0354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12904\n",
      "50/50 - 151s - loss: 0.1463 - f1_score_mod: 0.1495 - recall_mod: 0.0843 - precision_mod: 0.6745 - dur_error: 0.4248 - maestro_dur_loss: 0.0425 - val_loss: 0.1313 - val_f1_score_mod: 0.1636 - val_recall_mod: 0.0928 - val_precision_mod: 0.7025 - val_dur_error: 0.3307 - val_maestro_dur_loss: 0.0331\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12904\n",
      "50/50 - 152s - loss: 0.1459 - f1_score_mod: 0.1583 - recall_mod: 0.0900 - precision_mod: 0.6792 - dur_error: 0.4273 - maestro_dur_loss: 0.0427 - val_loss: 0.1352 - val_f1_score_mod: 0.1629 - val_recall_mod: 0.0920 - val_precision_mod: 0.7272 - val_dur_error: 0.3771 - val_maestro_dur_loss: 0.0377\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12904 to 0.12561, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 134s - loss: 0.1451 - f1_score_mod: 0.1635 - recall_mod: 0.0932 - precision_mod: 0.6810 - dur_error: 0.4250 - maestro_dur_loss: 0.0425 - val_loss: 0.1256 - val_f1_score_mod: 0.1661 - val_recall_mod: 0.0941 - val_precision_mod: 0.7188 - val_dur_error: 0.2880 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12561 to 0.12547, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 163s - loss: 0.1445 - f1_score_mod: 0.1679 - recall_mod: 0.0963 - precision_mod: 0.6790 - dur_error: 0.4250 - maestro_dur_loss: 0.0425 - val_loss: 0.1255 - val_f1_score_mod: 0.1774 - val_recall_mod: 0.1014 - val_precision_mod: 0.7258 - val_dur_error: 0.2884 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12547\n",
      "50/50 - 154s - loss: 0.1431 - f1_score_mod: 0.1696 - recall_mod: 0.0972 - precision_mod: 0.6826 - dur_error: 0.4133 - maestro_dur_loss: 0.0413 - val_loss: 0.1342 - val_f1_score_mod: 0.1836 - val_recall_mod: 0.1059 - val_precision_mod: 0.7042 - val_dur_error: 0.3775 - val_maestro_dur_loss: 0.0378\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12547\n",
      "50/50 - 144s - loss: 0.1431 - f1_score_mod: 0.1728 - recall_mod: 0.0991 - precision_mod: 0.6883 - dur_error: 0.4172 - maestro_dur_loss: 0.0417 - val_loss: 0.1296 - val_f1_score_mod: 0.1886 - val_recall_mod: 0.1091 - val_precision_mod: 0.7096 - val_dur_error: 0.3362 - val_maestro_dur_loss: 0.0336\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.12547 to 0.12544, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1421 - f1_score_mod: 0.1792 - recall_mod: 0.1034 - precision_mod: 0.6857 - dur_error: 0.4122 - maestro_dur_loss: 0.0412 - val_loss: 0.1254 - val_f1_score_mod: 0.1874 - val_recall_mod: 0.1083 - val_precision_mod: 0.7103 - val_dur_error: 0.2962 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12544\n",
      "50/50 - 146s - loss: 0.1416 - f1_score_mod: 0.1813 - recall_mod: 0.1047 - precision_mod: 0.6867 - dur_error: 0.4100 - maestro_dur_loss: 0.0410 - val_loss: 0.1360 - val_f1_score_mod: 0.1845 - val_recall_mod: 0.1055 - val_precision_mod: 0.7465 - val_dur_error: 0.4086 - val_maestro_dur_loss: 0.0409\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12544\n",
      "50/50 - 147s - loss: 0.1408 - f1_score_mod: 0.1863 - recall_mod: 0.1079 - precision_mod: 0.6918 - dur_error: 0.4067 - maestro_dur_loss: 0.0407 - val_loss: 0.1352 - val_f1_score_mod: 0.1976 - val_recall_mod: 0.1147 - val_precision_mod: 0.7241 - val_dur_error: 0.4014 - val_maestro_dur_loss: 0.0401\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.12544\n",
      "50/50 - 139s - loss: 0.1407 - f1_score_mod: 0.1899 - recall_mod: 0.1102 - precision_mod: 0.6971 - dur_error: 0.4093 - maestro_dur_loss: 0.0409 - val_loss: 0.1275 - val_f1_score_mod: 0.2014 - val_recall_mod: 0.1174 - val_precision_mod: 0.7194 - val_dur_error: 0.3255 - val_maestro_dur_loss: 0.0326\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.12544 to 0.12449, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 145s - loss: 0.1394 - f1_score_mod: 0.1942 - recall_mod: 0.1132 - precision_mod: 0.6922 - dur_error: 0.4007 - maestro_dur_loss: 0.0401 - val_loss: 0.1245 - val_f1_score_mod: 0.1878 - val_recall_mod: 0.1079 - val_precision_mod: 0.7398 - val_dur_error: 0.3001 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.12449 to 0.12425, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 140s - loss: 0.1392 - f1_score_mod: 0.1960 - recall_mod: 0.1144 - precision_mod: 0.6939 - dur_error: 0.4021 - maestro_dur_loss: 0.0402 - val_loss: 0.1243 - val_f1_score_mod: 0.2032 - val_recall_mod: 0.1184 - val_precision_mod: 0.7323 - val_dur_error: 0.3001 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.12425 to 0.12372, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 148s - loss: 0.1384 - f1_score_mod: 0.2031 - recall_mod: 0.1192 - precision_mod: 0.7012 - dur_error: 0.3987 - maestro_dur_loss: 0.0399 - val_loss: 0.1237 - val_f1_score_mod: 0.2034 - val_recall_mod: 0.1181 - val_precision_mod: 0.7494 - val_dur_error: 0.2932 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.12372 to 0.12240, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 154s - loss: 0.1381 - f1_score_mod: 0.2050 - recall_mod: 0.1205 - precision_mod: 0.6987 - dur_error: 0.3991 - maestro_dur_loss: 0.0399 - val_loss: 0.1224 - val_f1_score_mod: 0.1936 - val_recall_mod: 0.1113 - val_precision_mod: 0.7602 - val_dur_error: 0.2875 - val_maestro_dur_loss: 0.0287\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.12240\n",
      "50/50 - 154s - loss: 0.1373 - f1_score_mod: 0.2080 - recall_mod: 0.1225 - precision_mod: 0.6980 - dur_error: 0.3943 - maestro_dur_loss: 0.0394 - val_loss: 0.1226 - val_f1_score_mod: 0.2242 - val_recall_mod: 0.1335 - val_precision_mod: 0.7089 - val_dur_error: 0.2928 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.12240 to 0.12105, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 143s - loss: 0.1369 - f1_score_mod: 0.2164 - recall_mod: 0.1281 - precision_mod: 0.7061 - dur_error: 0.3937 - maestro_dur_loss: 0.0394 - val_loss: 0.1211 - val_f1_score_mod: 0.2223 - val_recall_mod: 0.1317 - val_precision_mod: 0.7249 - val_dur_error: 0.2778 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.12105 to 0.12001, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 149s - loss: 0.1366 - f1_score_mod: 0.2144 - recall_mod: 0.1268 - precision_mod: 0.7066 - dur_error: 0.3941 - maestro_dur_loss: 0.0394 - val_loss: 0.1200 - val_f1_score_mod: 0.2190 - val_recall_mod: 0.1289 - val_precision_mod: 0.7445 - val_dur_error: 0.2737 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.12001\n",
      "50/50 - 149s - loss: 0.1357 - f1_score_mod: 0.2167 - recall_mod: 0.1285 - precision_mod: 0.6996 - dur_error: 0.3876 - maestro_dur_loss: 0.0388 - val_loss: 0.1216 - val_f1_score_mod: 0.2138 - val_recall_mod: 0.1249 - val_precision_mod: 0.7581 - val_dur_error: 0.2897 - val_maestro_dur_loss: 0.0290\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.12001 to 0.11859, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 146s - loss: 0.1354 - f1_score_mod: 0.2231 - recall_mod: 0.1326 - precision_mod: 0.7125 - dur_error: 0.3896 - maestro_dur_loss: 0.0390 - val_loss: 0.1186 - val_f1_score_mod: 0.2238 - val_recall_mod: 0.1321 - val_precision_mod: 0.7485 - val_dur_error: 0.2652 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11859\n",
      "50/50 - 152s - loss: 0.1353 - f1_score_mod: 0.2302 - recall_mod: 0.1377 - precision_mod: 0.7103 - dur_error: 0.3915 - maestro_dur_loss: 0.0391 - val_loss: 0.1208 - val_f1_score_mod: 0.2247 - val_recall_mod: 0.1325 - val_precision_mod: 0.7504 - val_dur_error: 0.2897 - val_maestro_dur_loss: 0.0290\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.11859 to 0.11778, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 172s - loss: 0.1344 - f1_score_mod: 0.2301 - recall_mod: 0.1376 - precision_mod: 0.7120 - dur_error: 0.3848 - maestro_dur_loss: 0.0385 - val_loss: 0.1178 - val_f1_score_mod: 0.2324 - val_recall_mod: 0.1378 - val_precision_mod: 0.7540 - val_dur_error: 0.2621 - val_maestro_dur_loss: 0.0262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11778\n",
      "50/50 - 167s - loss: 0.1336 - f1_score_mod: 0.2340 - recall_mod: 0.1403 - precision_mod: 0.7116 - dur_error: 0.3810 - maestro_dur_loss: 0.0381 - val_loss: 0.1289 - val_f1_score_mod: 0.2357 - val_recall_mod: 0.1405 - val_precision_mod: 0.7453 - val_dur_error: 0.3748 - val_maestro_dur_loss: 0.0375\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11778\n",
      "50/50 - 167s - loss: 0.1337 - f1_score_mod: 0.2383 - recall_mod: 0.1432 - precision_mod: 0.7165 - dur_error: 0.3863 - maestro_dur_loss: 0.0386 - val_loss: 0.1205 - val_f1_score_mod: 0.2269 - val_recall_mod: 0.1335 - val_precision_mod: 0.7720 - val_dur_error: 0.2927 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.11778 to 0.11770, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 167s - loss: 0.1332 - f1_score_mod: 0.2412 - recall_mod: 0.1453 - precision_mod: 0.7166 - dur_error: 0.3845 - maestro_dur_loss: 0.0385 - val_loss: 0.1177 - val_f1_score_mod: 0.2467 - val_recall_mod: 0.1491 - val_precision_mod: 0.7272 - val_dur_error: 0.2671 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11770\n",
      "50/50 - 166s - loss: 0.1321 - f1_score_mod: 0.2425 - recall_mod: 0.1463 - precision_mod: 0.7168 - dur_error: 0.3772 - maestro_dur_loss: 0.0377 - val_loss: 0.1201 - val_f1_score_mod: 0.2359 - val_recall_mod: 0.1397 - val_precision_mod: 0.7708 - val_dur_error: 0.2948 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.11770 to 0.11620, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 166s - loss: 0.1321 - f1_score_mod: 0.2512 - recall_mod: 0.1527 - precision_mod: 0.7168 - dur_error: 0.3802 - maestro_dur_loss: 0.0380 - val_loss: 0.1162 - val_f1_score_mod: 0.2520 - val_recall_mod: 0.1531 - val_precision_mod: 0.7291 - val_dur_error: 0.2566 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11620\n",
      "50/50 - 166s - loss: 0.1311 - f1_score_mod: 0.2558 - recall_mod: 0.1560 - precision_mod: 0.7151 - dur_error: 0.3754 - maestro_dur_loss: 0.0375 - val_loss: 0.1171 - val_f1_score_mod: 0.2568 - val_recall_mod: 0.1557 - val_precision_mod: 0.7457 - val_dur_error: 0.2695 - val_maestro_dur_loss: 0.0270\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11620\n",
      "50/50 - 166s - loss: 0.1311 - f1_score_mod: 0.2570 - recall_mod: 0.1569 - precision_mod: 0.7179 - dur_error: 0.3763 - maestro_dur_loss: 0.0376 - val_loss: 0.1168 - val_f1_score_mod: 0.2525 - val_recall_mod: 0.1522 - val_precision_mod: 0.7545 - val_dur_error: 0.2666 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11620\n",
      "50/50 - 166s - loss: 0.1306 - f1_score_mod: 0.2598 - recall_mod: 0.1587 - precision_mod: 0.7227 - dur_error: 0.3733 - maestro_dur_loss: 0.0373 - val_loss: 0.1182 - val_f1_score_mod: 0.2499 - val_recall_mod: 0.1500 - val_precision_mod: 0.7614 - val_dur_error: 0.2824 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11620\n",
      "50/50 - 166s - loss: 0.1302 - f1_score_mod: 0.2620 - recall_mod: 0.1606 - precision_mod: 0.7205 - dur_error: 0.3737 - maestro_dur_loss: 0.0374 - val_loss: 0.1165 - val_f1_score_mod: 0.2501 - val_recall_mod: 0.1501 - val_precision_mod: 0.7639 - val_dur_error: 0.2675 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.11620 to 0.11390, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 168s - loss: 0.1298 - f1_score_mod: 0.2650 - recall_mod: 0.1628 - precision_mod: 0.7174 - dur_error: 0.3726 - maestro_dur_loss: 0.0373 - val_loss: 0.1139 - val_f1_score_mod: 0.2563 - val_recall_mod: 0.1544 - val_precision_mod: 0.7638 - val_dur_error: 0.2469 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11390\n",
      "50/50 - 171s - loss: 0.1292 - f1_score_mod: 0.2696 - recall_mod: 0.1660 - precision_mod: 0.7255 - dur_error: 0.3710 - maestro_dur_loss: 0.0371 - val_loss: 0.1159 - val_f1_score_mod: 0.2651 - val_recall_mod: 0.1618 - val_precision_mod: 0.7454 - val_dur_error: 0.2681 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11390\n",
      "50/50 - 168s - loss: 0.1289 - f1_score_mod: 0.2746 - recall_mod: 0.1696 - precision_mod: 0.7257 - dur_error: 0.3713 - maestro_dur_loss: 0.0371 - val_loss: 0.1147 - val_f1_score_mod: 0.2689 - val_recall_mod: 0.1651 - val_precision_mod: 0.7357 - val_dur_error: 0.2543 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11390\n",
      "50/50 - 167s - loss: 0.1285 - f1_score_mod: 0.2767 - recall_mod: 0.1715 - precision_mod: 0.7233 - dur_error: 0.3700 - maestro_dur_loss: 0.0370 - val_loss: 0.1171 - val_f1_score_mod: 0.2667 - val_recall_mod: 0.1629 - val_precision_mod: 0.7495 - val_dur_error: 0.2834 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11390\n",
      "50/50 - 168s - loss: 0.1281 - f1_score_mod: 0.2787 - recall_mod: 0.1731 - precision_mod: 0.7215 - dur_error: 0.3683 - maestro_dur_loss: 0.0368 - val_loss: 0.1208 - val_f1_score_mod: 0.2760 - val_recall_mod: 0.1706 - val_precision_mod: 0.7342 - val_dur_error: 0.3184 - val_maestro_dur_loss: 0.0318\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11390\n",
      "50/50 - 166s - loss: 0.1274 - f1_score_mod: 0.2851 - recall_mod: 0.1777 - precision_mod: 0.7268 - dur_error: 0.3670 - maestro_dur_loss: 0.0367 - val_loss: 0.1188 - val_f1_score_mod: 0.2670 - val_recall_mod: 0.1623 - val_precision_mod: 0.7668 - val_dur_error: 0.2956 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.11390 to 0.11386, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 166s - loss: 0.1273 - f1_score_mod: 0.2868 - recall_mod: 0.1788 - precision_mod: 0.7300 - dur_error: 0.3673 - maestro_dur_loss: 0.0367 - val_loss: 0.1139 - val_f1_score_mod: 0.2711 - val_recall_mod: 0.1660 - val_precision_mod: 0.7535 - val_dur_error: 0.2547 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11386\n",
      "50/50 - 166s - loss: 0.1265 - f1_score_mod: 0.2877 - recall_mod: 0.1796 - precision_mod: 0.7280 - dur_error: 0.3613 - maestro_dur_loss: 0.0361 - val_loss: 0.1186 - val_f1_score_mod: 0.2870 - val_recall_mod: 0.1787 - val_precision_mod: 0.7399 - val_dur_error: 0.3025 - val_maestro_dur_loss: 0.0303\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11386\n",
      "50/50 - 166s - loss: 0.1260 - f1_score_mod: 0.2938 - recall_mod: 0.1843 - precision_mod: 0.7309 - dur_error: 0.3619 - maestro_dur_loss: 0.0362 - val_loss: 0.1211 - val_f1_score_mod: 0.2865 - val_recall_mod: 0.1776 - val_precision_mod: 0.7523 - val_dur_error: 0.3321 - val_maestro_dur_loss: 0.0332\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.11386 to 0.11371, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 167s - loss: 0.1259 - f1_score_mod: 0.2980 - recall_mod: 0.1875 - precision_mod: 0.7325 - dur_error: 0.3641 - maestro_dur_loss: 0.0364 - val_loss: 0.1137 - val_f1_score_mod: 0.2952 - val_recall_mod: 0.1855 - val_precision_mod: 0.7337 - val_dur_error: 0.2616 - val_maestro_dur_loss: 0.0262\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.11371\n",
      "50/50 - 167s - loss: 0.1251 - f1_score_mod: 0.3036 - recall_mod: 0.1919 - precision_mod: 0.7338 - dur_error: 0.3597 - maestro_dur_loss: 0.0360 - val_loss: 0.1255 - val_f1_score_mod: 0.2913 - val_recall_mod: 0.1817 - val_precision_mod: 0.7451 - val_dur_error: 0.3809 - val_maestro_dur_loss: 0.0381\n",
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.11371 to 0.11214, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 167s - loss: 0.1247 - f1_score_mod: 0.3025 - recall_mod: 0.1910 - precision_mod: 0.7325 - dur_error: 0.3576 - maestro_dur_loss: 0.0358 - val_loss: 0.1121 - val_f1_score_mod: 0.2795 - val_recall_mod: 0.1718 - val_precision_mod: 0.7634 - val_dur_error: 0.2460 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.11214 to 0.11140, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 168s - loss: 0.1243 - f1_score_mod: 0.3059 - recall_mod: 0.1941 - precision_mod: 0.7279 - dur_error: 0.3571 - maestro_dur_loss: 0.0357 - val_loss: 0.1114 - val_f1_score_mod: 0.3020 - val_recall_mod: 0.1907 - val_precision_mod: 0.7377 - val_dur_error: 0.2426 - val_maestro_dur_loss: 0.0243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.11140\n",
      "50/50 - 167s - loss: 0.1238 - f1_score_mod: 0.3108 - recall_mod: 0.1976 - precision_mod: 0.7311 - dur_error: 0.3552 - maestro_dur_loss: 0.0355 - val_loss: 0.1117 - val_f1_score_mod: 0.3122 - val_recall_mod: 0.1997 - val_precision_mod: 0.7267 - val_dur_error: 0.2450 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.11140 to 0.11126, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 168s - loss: 0.1237 - f1_score_mod: 0.3133 - recall_mod: 0.1994 - precision_mod: 0.7345 - dur_error: 0.3580 - maestro_dur_loss: 0.0358 - val_loss: 0.1113 - val_f1_score_mod: 0.3080 - val_recall_mod: 0.1959 - val_precision_mod: 0.7331 - val_dur_error: 0.2427 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.11126\n",
      "50/50 - 167s - loss: 0.1231 - f1_score_mod: 0.3192 - recall_mod: 0.2041 - precision_mod: 0.7365 - dur_error: 0.3561 - maestro_dur_loss: 0.0356 - val_loss: 0.1135 - val_f1_score_mod: 0.3006 - val_recall_mod: 0.1885 - val_precision_mod: 0.7542 - val_dur_error: 0.2633 - val_maestro_dur_loss: 0.0263\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.11126\n",
      "50/50 - 168s - loss: 0.1226 - f1_score_mod: 0.3224 - recall_mod: 0.2067 - precision_mod: 0.7363 - dur_error: 0.3539 - maestro_dur_loss: 0.0354 - val_loss: 0.1128 - val_f1_score_mod: 0.3142 - val_recall_mod: 0.2012 - val_precision_mod: 0.7296 - val_dur_error: 0.2607 - val_maestro_dur_loss: 0.0261\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.11126\n",
      "50/50 - 167s - loss: 0.1220 - f1_score_mod: 0.3222 - recall_mod: 0.2066 - precision_mod: 0.7389 - dur_error: 0.3510 - maestro_dur_loss: 0.0351 - val_loss: 0.1142 - val_f1_score_mod: 0.3172 - val_recall_mod: 0.2035 - val_precision_mod: 0.7331 - val_dur_error: 0.2738 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.11126 to 0.10984, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 169s - loss: 0.1217 - f1_score_mod: 0.3270 - recall_mod: 0.2108 - precision_mod: 0.7356 - dur_error: 0.3523 - maestro_dur_loss: 0.0352 - val_loss: 0.1098 - val_f1_score_mod: 0.3143 - val_recall_mod: 0.2007 - val_precision_mod: 0.7382 - val_dur_error: 0.2359 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.10984\n",
      "50/50 - 169s - loss: 0.1211 - f1_score_mod: 0.3317 - recall_mod: 0.2145 - precision_mod: 0.7365 - dur_error: 0.3495 - maestro_dur_loss: 0.0349 - val_loss: 0.1126 - val_f1_score_mod: 0.3414 - val_recall_mod: 0.2258 - val_precision_mod: 0.7077 - val_dur_error: 0.2646 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.10984\n",
      "50/50 - 168s - loss: 0.1206 - f1_score_mod: 0.3360 - recall_mod: 0.2183 - precision_mod: 0.7347 - dur_error: 0.3466 - maestro_dur_loss: 0.0347 - val_loss: 0.1101 - val_f1_score_mod: 0.3098 - val_recall_mod: 0.1959 - val_precision_mod: 0.7555 - val_dur_error: 0.2402 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.10984\n",
      "50/50 - 168s - loss: 0.1202 - f1_score_mod: 0.3359 - recall_mod: 0.2178 - precision_mod: 0.7387 - dur_error: 0.3464 - maestro_dur_loss: 0.0346 - val_loss: 0.1099 - val_f1_score_mod: 0.3223 - val_recall_mod: 0.2063 - val_precision_mod: 0.7485 - val_dur_error: 0.2433 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10984\n",
      "50/50 - 167s - loss: 0.1201 - f1_score_mod: 0.3401 - recall_mod: 0.2217 - precision_mod: 0.7351 - dur_error: 0.3478 - maestro_dur_loss: 0.0348 - val_loss: 0.1100 - val_f1_score_mod: 0.3323 - val_recall_mod: 0.2167 - val_precision_mod: 0.7245 - val_dur_error: 0.2427 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10984\n",
      "50/50 - 168s - loss: 0.1195 - f1_score_mod: 0.3421 - recall_mod: 0.2230 - precision_mod: 0.7388 - dur_error: 0.3450 - maestro_dur_loss: 0.0345 - val_loss: 0.1100 - val_f1_score_mod: 0.3320 - val_recall_mod: 0.2152 - val_precision_mod: 0.7370 - val_dur_error: 0.2442 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.10984 to 0.10923, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 172s - loss: 0.1188 - f1_score_mod: 0.3462 - recall_mod: 0.2265 - precision_mod: 0.7386 - dur_error: 0.3422 - maestro_dur_loss: 0.0342 - val_loss: 0.1092 - val_f1_score_mod: 0.3287 - val_recall_mod: 0.2121 - val_precision_mod: 0.7445 - val_dur_error: 0.2397 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10923\n",
      "50/50 - 166s - loss: 0.1184 - f1_score_mod: 0.3521 - recall_mod: 0.2310 - precision_mod: 0.7443 - dur_error: 0.3443 - maestro_dur_loss: 0.0344 - val_loss: 0.1193 - val_f1_score_mod: 0.3329 - val_recall_mod: 0.2160 - val_precision_mod: 0.7381 - val_dur_error: 0.3427 - val_maestro_dur_loss: 0.0343\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10923\n",
      "50/50 - 167s - loss: 0.1179 - f1_score_mod: 0.3545 - recall_mod: 0.2335 - precision_mod: 0.7403 - dur_error: 0.3407 - maestro_dur_loss: 0.0341 - val_loss: 0.1103 - val_f1_score_mod: 0.3408 - val_recall_mod: 0.2223 - val_precision_mod: 0.7414 - val_dur_error: 0.2571 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.10923 to 0.10835, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 166s - loss: 0.1174 - f1_score_mod: 0.3571 - recall_mod: 0.2355 - precision_mod: 0.7438 - dur_error: 0.3408 - maestro_dur_loss: 0.0341 - val_loss: 0.1083 - val_f1_score_mod: 0.3413 - val_recall_mod: 0.2241 - val_precision_mod: 0.7272 - val_dur_error: 0.2325 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.10835 to 0.10802, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 167s - loss: 0.1168 - f1_score_mod: 0.3603 - recall_mod: 0.2386 - precision_mod: 0.7405 - dur_error: 0.3373 - maestro_dur_loss: 0.0337 - val_loss: 0.1080 - val_f1_score_mod: 0.3390 - val_recall_mod: 0.2209 - val_precision_mod: 0.7409 - val_dur_error: 0.2335 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10802\n",
      "50/50 - 167s - loss: 0.1168 - f1_score_mod: 0.3603 - recall_mod: 0.2384 - precision_mod: 0.7414 - dur_error: 0.3392 - maestro_dur_loss: 0.0339 - val_loss: 0.1144 - val_f1_score_mod: 0.3530 - val_recall_mod: 0.2354 - val_precision_mod: 0.7148 - val_dur_error: 0.3001 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10802\n",
      "50/50 - 170s - loss: 0.1164 - f1_score_mod: 0.3657 - recall_mod: 0.2428 - precision_mod: 0.7453 - dur_error: 0.3378 - maestro_dur_loss: 0.0338 - val_loss: 0.1102 - val_f1_score_mod: 0.3546 - val_recall_mod: 0.2366 - val_precision_mod: 0.7170 - val_dur_error: 0.2581 - val_maestro_dur_loss: 0.0258\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10802\n",
      "50/50 - 169s - loss: 0.1158 - f1_score_mod: 0.3680 - recall_mod: 0.2454 - precision_mod: 0.7391 - dur_error: 0.3375 - maestro_dur_loss: 0.0338 - val_loss: 0.1081 - val_f1_score_mod: 0.3432 - val_recall_mod: 0.2229 - val_precision_mod: 0.7541 - val_dur_error: 0.2408 - val_maestro_dur_loss: 0.0241\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.10802 to 0.10719, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 168s - loss: 0.1151 - f1_score_mod: 0.3714 - recall_mod: 0.2481 - precision_mod: 0.7416 - dur_error: 0.3341 - maestro_dur_loss: 0.0334 - val_loss: 0.1072 - val_f1_score_mod: 0.3485 - val_recall_mod: 0.2296 - val_precision_mod: 0.7318 - val_dur_error: 0.2325 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10719\n",
      "50/50 - 168s - loss: 0.1149 - f1_score_mod: 0.3734 - recall_mod: 0.2496 - precision_mod: 0.7448 - dur_error: 0.3333 - maestro_dur_loss: 0.0333 - val_loss: 0.1102 - val_f1_score_mod: 0.3604 - val_recall_mod: 0.2420 - val_precision_mod: 0.7132 - val_dur_error: 0.2629 - val_maestro_dur_loss: 0.0263\n",
      "Epoch 86/150\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10719\n",
      "50/50 - 167s - loss: 0.1143 - f1_score_mod: 0.3789 - recall_mod: 0.2549 - precision_mod: 0.7412 - dur_error: 0.3320 - maestro_dur_loss: 0.0332 - val_loss: 0.1119 - val_f1_score_mod: 0.3597 - val_recall_mod: 0.2421 - val_precision_mod: 0.7078 - val_dur_error: 0.2783 - val_maestro_dur_loss: 0.0278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/150\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10719\n",
      "50/50 - 167s - loss: 0.1139 - f1_score_mod: 0.3830 - recall_mod: 0.2584 - precision_mod: 0.7414 - dur_error: 0.3319 - maestro_dur_loss: 0.0332 - val_loss: 0.1134 - val_f1_score_mod: 0.3629 - val_recall_mod: 0.2442 - val_precision_mod: 0.7130 - val_dur_error: 0.2969 - val_maestro_dur_loss: 0.0297\n",
      "Epoch 88/150\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10719\n",
      "50/50 - 167s - loss: 0.1133 - f1_score_mod: 0.3831 - recall_mod: 0.2585 - precision_mod: 0.7421 - dur_error: 0.3278 - maestro_dur_loss: 0.0328 - val_loss: 0.1165 - val_f1_score_mod: 0.3661 - val_recall_mod: 0.2487 - val_precision_mod: 0.6994 - val_dur_error: 0.3274 - val_maestro_dur_loss: 0.0327\n",
      "Epoch 89/150\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.10719 to 0.10685, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 167s - loss: 0.1128 - f1_score_mod: 0.3850 - recall_mod: 0.2607 - precision_mod: 0.7395 - dur_error: 0.3267 - maestro_dur_loss: 0.0327 - val_loss: 0.1068 - val_f1_score_mod: 0.3608 - val_recall_mod: 0.2405 - val_precision_mod: 0.7300 - val_dur_error: 0.2334 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 90/150\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10685\n",
      "50/50 - 167s - loss: 0.1123 - f1_score_mod: 0.3894 - recall_mod: 0.2641 - precision_mod: 0.7461 - dur_error: 0.3267 - maestro_dur_loss: 0.0327 - val_loss: 0.1124 - val_f1_score_mod: 0.3758 - val_recall_mod: 0.2569 - val_precision_mod: 0.7042 - val_dur_error: 0.2928 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 91/150\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10685\n",
      "50/50 - 168s - loss: 0.1121 - f1_score_mod: 0.3922 - recall_mod: 0.2669 - precision_mod: 0.7422 - dur_error: 0.3262 - maestro_dur_loss: 0.0326 - val_loss: 0.1127 - val_f1_score_mod: 0.3666 - val_recall_mod: 0.2474 - val_precision_mod: 0.7141 - val_dur_error: 0.2940 - val_maestro_dur_loss: 0.0294\n",
      "Epoch 92/150\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10685\n",
      "50/50 - 168s - loss: 0.1119 - f1_score_mod: 0.3942 - recall_mod: 0.2688 - precision_mod: 0.7421 - dur_error: 0.3288 - maestro_dur_loss: 0.0329 - val_loss: 0.1105 - val_f1_score_mod: 0.3743 - val_recall_mod: 0.2543 - val_precision_mod: 0.7128 - val_dur_error: 0.2776 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 93/150\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.10685 to 0.10629, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 167s - loss: 0.1112 - f1_score_mod: 0.3999 - recall_mod: 0.2737 - precision_mod: 0.7450 - dur_error: 0.3255 - maestro_dur_loss: 0.0325 - val_loss: 0.1063 - val_f1_score_mod: 0.3678 - val_recall_mod: 0.2478 - val_precision_mod: 0.7200 - val_dur_error: 0.2335 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 94/150\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.10629\n",
      "50/50 - 167s - loss: 0.1108 - f1_score_mod: 0.4030 - recall_mod: 0.2762 - precision_mod: 0.7484 - dur_error: 0.3254 - maestro_dur_loss: 0.0325 - val_loss: 0.1079 - val_f1_score_mod: 0.3736 - val_recall_mod: 0.2539 - val_precision_mod: 0.7107 - val_dur_error: 0.2430 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 95/150\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10629\n",
      "50/50 - 167s - loss: 0.1102 - f1_score_mod: 0.4076 - recall_mod: 0.2808 - precision_mod: 0.7458 - dur_error: 0.3233 - maestro_dur_loss: 0.0323 - val_loss: 0.1115 - val_f1_score_mod: 0.3850 - val_recall_mod: 0.2659 - val_precision_mod: 0.7033 - val_dur_error: 0.2873 - val_maestro_dur_loss: 0.0287\n",
      "Epoch 96/150\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.10629 to 0.10612, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 169s - loss: 0.1099 - f1_score_mod: 0.4117 - recall_mod: 0.2845 - precision_mod: 0.7483 - dur_error: 0.3231 - maestro_dur_loss: 0.0323 - val_loss: 0.1061 - val_f1_score_mod: 0.3791 - val_recall_mod: 0.2584 - val_precision_mod: 0.7196 - val_dur_error: 0.2335 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 97/150\n",
      "Batch 9: Invalid loss, terminating training\n",
      "Batch 10: Invalid loss, terminating training\n",
      "Batch 11: Invalid loss, terminating training\n",
      "Batch 12: Invalid loss, terminating training\n",
      "Batch 13: Invalid loss, terminating training\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.10612\n",
      "50/50 - 168s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed much better, but was also still improving just before the NaN loss. We can also try gradient clipping (insert link). There are two methods to accomplish this and they are controlled by the optional parameters to RMSProp (clipnorm and clipvalue). clipnorm limits the L2 norm of the gradients after each batch by rescaling them to be lower than the value set by clipnorm. clipvalue enforces a maximum/minimum value (equal to +/- clipvalue) for each gradient. In the linked blog post, reasonable values are given to be 1.0 for clipnorm and 0.5 for clipvalue. \n",
    "\n",
    "We could simply try these but it is better to get a look at the gradients for our base model to inform our decision. For this, we will need to run the base model again (using train_by_batch so I can call the following functions in between batches). Note that training by batch will slightly alter the results when compared to train (insert link). To inform the clipnorm choice, we simply need to calculate the L2-norm of the gradients after each batch. To inform the clipvalue choice, we will need to look at the minimum/maximum gradients (over the batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norm_func(model):\n",
    "    \"\"\"Returns the gradient norm of the model averaged over all gradients of all layers\"\"\"\n",
    "\n",
    "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "    summed_squares = [K.sum(K.square(g)) for g in grads]\n",
    "    norm = K.sqrt(sum(summed_squares))\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    return K.function(inputs, [norm])\n",
    "\n",
    "def get_gradient_stats_func(model):\n",
    "    \"\"\"Returns the gradient statistics of the model\"\"\"\n",
    "\n",
    "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "    means = [K.mean(g) for g in grads]\n",
    "    minimums = [K.min(g) for g in grads]\n",
    "    maximums = [K.max(g) for g in grads]\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    return K.function(inputs, [means, minimums, maximums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 2:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 3:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 4:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 5:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 6:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 7:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 8:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 9:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 10:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 11:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 12:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 13:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 14:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 15:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 16:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 17:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 18:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 19:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 20:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 21:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 22:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 23:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 24:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 25:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 26:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 27:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 28:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 29:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 30:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 31:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 32:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 33:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 34:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 35:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 36:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 37:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 38:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 39:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 40:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 41:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 42:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 43:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 44:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 45:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()   # For some reason, I need to do this to have\n",
    "                                         # access to total_loss. It's strange because \n",
    "                                         # model.run_eagerly = False even before, as it should\n",
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4)\n",
    "model.compile(loss = maestro_loss_wr(0.1), optimizer = RMSprop(), metrics = [f1_score_mod, dur_error, maestro_dur_loss_wr(0.1)])\n",
    "\n",
    "get_gradient_norms = get_gradient_norm_func(model)\n",
    "get_gradient_stats = get_gradient_stats_func(model)\n",
    "norms_by_batch = []\n",
    "stats_by_batch = []\n",
    "batch_size = 512\n",
    "epochs = 45\n",
    "for i in range(epochs):\n",
    "    print('Epoch {}:'.format(i + 1))\n",
    "    for j in range(len(y_train) // batch_size):\n",
    "        model.train_on_batch(X_train[j * batch_size: (j + 1) * batch_size], \\\n",
    "                             y_train[j * batch_size: (j + 1) * batch_size])\n",
    "        \n",
    "        # We just need to pass placeholder arrays of shape (?, window_size, 88),\n",
    "        # (?, n_keys_piano + 1) and (n_keys_piano + 1)\n",
    "        gradient_norms = get_gradient_norms([X_train[:2], y_train[:2], np.ones(2)])\n",
    "        gradient_stats = get_gradient_stats([X_train[:2], y_train[:2], np.ones(2)])\n",
    "        mean = np.mean(gradient_stats[0])\n",
    "        minimum = min(gradient_stats[1])\n",
    "        maximum = max(gradient_stats[2])\n",
    "        print('batch {}'.format(j))\n",
    "        norms_by_batch.append(gradient_norms[0])\n",
    "        stats_by_batch.append([mean, minimum, maximum])\n",
    "        \n",
    "    model.train_on_batch(X_train[(j + 1) * batch_size:], y_train[(j + 1) * batch_size:])\n",
    "    \n",
    "    gradient_norms = get_gradient_norms([X_train[:2], y_train[:2], np.ones(2)])\n",
    "    gradient_stats = get_gradient_stats([X_train[:2], y_train[:2], np.ones(2)])\n",
    "    mean = np.mean(gradient_stats[0])\n",
    "    minimum = min(gradient_stats[1])\n",
    "    maximum = max(gradient_stats[2])\n",
    "    norms_by_batch.append(gradient_norms[0])\n",
    "    stats_by_batch.append([mean, minimum, maximum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>norm</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th>Batch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.238416</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>-0.013196</td>\n",
       "      <td>0.015261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.118737</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>-0.162016</td>\n",
       "      <td>0.025847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.758461</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>-0.044176</td>\n",
       "      <td>0.044242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.640719</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.046878</td>\n",
       "      <td>0.020354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.522039</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.041981</td>\n",
       "      <td>0.008727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">29</th>\n",
       "      <th>1407</th>\n",
       "      <td>0.525405</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.030763</td>\n",
       "      <td>0.027571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>0.527925</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.029569</td>\n",
       "      <td>0.038999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>0.557107</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>-0.029433</td>\n",
       "      <td>0.049748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>0.498037</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.025642</td>\n",
       "      <td>0.029175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>0.509220</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.026250</td>\n",
       "      <td>0.033445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1411 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 norm      mean       min       max\n",
       "Epoch Batch                                        \n",
       "1     1      0.238416  0.000354 -0.013196  0.015261\n",
       "      2      1.118737  0.000137 -0.162016  0.025847\n",
       "      3      0.758461  0.000158 -0.044176  0.044242\n",
       "      4      0.640719  0.000031 -0.046878  0.020354\n",
       "      5      0.522039  0.000014 -0.041981  0.008727\n",
       "...               ...       ...       ...       ...\n",
       "29    1407   0.525405  0.000060 -0.030763  0.027571\n",
       "      1408   0.527925  0.000056 -0.029569  0.038999\n",
       "      1409   0.557107  0.000075 -0.029433  0.049748\n",
       "      1410   0.498037  0.000068 -0.025642  0.029175\n",
       "      1411   0.509220  0.000076 -0.026250  0.033445\n",
       "\n",
       "[1411 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the dataframes\n",
    "df_norms = pd.DataFrame(norms_by_batch)\n",
    "df_stats = pd.DataFrame(stats_by_batch)\n",
    "df_grads = pd.concat([df_norms, df_stats], axis = 1).dropna()\n",
    "df_grads.columns = ['norm', 'mean', 'min', 'max']\n",
    "batches_per_epoch = 49\n",
    "df_grads.index = pd.MultiIndex.from_tuples([((batch // batches_per_epoch) + 1, batch) \\\n",
    "                    for batch in range(1, len(df_grads) + 1)], names = ('Epoch', 'Batch'))\n",
    "df_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data so we don't have to re-run the model\n",
    "df_grads.to_csv('../model_data/gradients_2_1_512_0pt4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x655cb6fd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_grads = pd.read_csv('../model_data/gradients_2_1_512_0pt4.csv', index_col = [0, 1])\n",
    "df_grads.plot(y = 'norm', figsize = (12, 6), logy = True, xlabel = 'Epoch, Batch', \\\n",
    "             ylabel = 'L2-Norm', fontsize = 12)\n",
    "plt.axhline(1, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot (note the logscale on the y-axis), we have added a line corresponding to the suggested clipnorm value of 1.0. We can see that on only two occasions in early training does the L2-norm exceed this value. It is possible that the norm for the final batch would have exploded. To get a better idea, one could re-run with a smaller batch size. However, there is no way to know how much we would have to decrease the batch size to see such an effect. Since lowering the batch size would also increase the run-time, we will halt the exploration here. Our plan is now to train models with clipnorm = 1.0 and 0.5. The latter should have an effect on the late-stage training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x65be62e10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_grads.plot(y = ['mean', 'min', 'max'], xlabel = 'Epoch, Batch', \\\n",
    "             ylabel = 'Gradient', figsize = (12, 6), fontsize = 12, ylim = [-.2, .2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can see that the mean says near zero while the min is largest in early training, and the max is largest in late training (though both get pretty large). However, neither really approach the level of 0.5, although perhaps a clipvalue = 0.5 would delay the NaN failure in the final batch. We'll go with 0.25 first to see what that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19500, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 171s - loss: 0.2857 - f1_score_mod: 0.0100 - recall_mod: 0.0214 - precision_mod: 0.0611 - dur_error: 1.0865 - maestro_dur_loss: 0.1087 - val_loss: 0.1950 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.7128 - val_maestro_dur_loss: 0.0713\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19500 to 0.16948, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 174s - loss: 0.1996 - f1_score_mod: 0.0017 - recall_mod: 8.9960e-04 - precision_mod: 0.1840 - dur_error: 0.6770 - maestro_dur_loss: 0.0677 - val_loss: 0.1695 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4907 - val_maestro_dur_loss: 0.0491\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16948\n",
      "50/50 - 169s - loss: 0.1893 - f1_score_mod: 0.0092 - recall_mod: 0.0047 - precision_mod: 0.4519 - dur_error: 0.6372 - maestro_dur_loss: 0.0637 - val_loss: 0.1820 - val_f1_score_mod: 0.0075 - val_recall_mod: 0.0038 - val_precision_mod: 0.6236 - val_dur_error: 0.6541 - val_maestro_dur_loss: 0.0654\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16948\n",
      "50/50 - 170s - loss: 0.1819 - f1_score_mod: 0.0182 - recall_mod: 0.0093 - precision_mod: 0.5274 - dur_error: 0.6005 - maestro_dur_loss: 0.0601 - val_loss: 0.1764 - val_f1_score_mod: 0.0123 - val_recall_mod: 0.0062 - val_precision_mod: 0.5705 - val_dur_error: 0.6223 - val_maestro_dur_loss: 0.0622\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16948 to 0.15405, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 170s - loss: 0.1757 - f1_score_mod: 0.0273 - recall_mod: 0.0141 - precision_mod: 0.5428 - dur_error: 0.5730 - maestro_dur_loss: 0.0573 - val_loss: 0.1541 - val_f1_score_mod: 0.0335 - val_recall_mod: 0.0173 - val_precision_mod: 0.6448 - val_dur_error: 0.4307 - val_maestro_dur_loss: 0.0431\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15405 to 0.14950, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 173s - loss: 0.1733 - f1_score_mod: 0.0446 - recall_mod: 0.0233 - precision_mod: 0.5878 - dur_error: 0.5684 - maestro_dur_loss: 0.0568 - val_loss: 0.1495 - val_f1_score_mod: 0.0267 - val_recall_mod: 0.0137 - val_precision_mod: 0.7429 - val_dur_error: 0.4026 - val_maestro_dur_loss: 0.0403\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.14950 to 0.14700, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 168s - loss: 0.1715 - f1_score_mod: 0.0565 - recall_mod: 0.0298 - precision_mod: 0.6184 - dur_error: 0.5647 - maestro_dur_loss: 0.0565 - val_loss: 0.1470 - val_f1_score_mod: 0.0584 - val_recall_mod: 0.0305 - val_precision_mod: 0.7557 - val_dur_error: 0.4008 - val_maestro_dur_loss: 0.0401\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14700 to 0.14517, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 171s - loss: 0.1686 - f1_score_mod: 0.0733 - recall_mod: 0.0392 - precision_mod: 0.6257 - dur_error: 0.5548 - maestro_dur_loss: 0.0555 - val_loss: 0.1452 - val_f1_score_mod: 0.0563 - val_recall_mod: 0.0294 - val_precision_mod: 0.7343 - val_dur_error: 0.3886 - val_maestro_dur_loss: 0.0389\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14517 to 0.14437, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 168s - loss: 0.1660 - f1_score_mod: 0.0813 - recall_mod: 0.0436 - precision_mod: 0.6355 - dur_error: 0.5439 - maestro_dur_loss: 0.0544 - val_loss: 0.1444 - val_f1_score_mod: 0.0846 - val_recall_mod: 0.0452 - val_precision_mod: 0.7004 - val_dur_error: 0.3998 - val_maestro_dur_loss: 0.0400\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14437 to 0.14050, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 169s - loss: 0.1626 - f1_score_mod: 0.0932 - recall_mod: 0.0507 - precision_mod: 0.6415 - dur_error: 0.5259 - maestro_dur_loss: 0.0526 - val_loss: 0.1405 - val_f1_score_mod: 0.0846 - val_recall_mod: 0.0450 - val_precision_mod: 0.7503 - val_dur_error: 0.3701 - val_maestro_dur_loss: 0.0370\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.14050 to 0.13660, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 168s - loss: 0.1607 - f1_score_mod: 0.1075 - recall_mod: 0.0588 - precision_mod: 0.6535 - dur_error: 0.5186 - maestro_dur_loss: 0.0519 - val_loss: 0.1366 - val_f1_score_mod: 0.0943 - val_recall_mod: 0.0506 - val_precision_mod: 0.7188 - val_dur_error: 0.3431 - val_maestro_dur_loss: 0.0343\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13660\n",
      "50/50 - 168s - loss: 0.1581 - f1_score_mod: 0.1188 - recall_mod: 0.0655 - precision_mod: 0.6587 - dur_error: 0.5060 - maestro_dur_loss: 0.0506 - val_loss: 0.1510 - val_f1_score_mod: 0.1355 - val_recall_mod: 0.0753 - val_precision_mod: 0.6878 - val_dur_error: 0.4954 - val_maestro_dur_loss: 0.0495\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13660\n",
      "50/50 - 167s - loss: 0.1561 - f1_score_mod: 0.1243 - recall_mod: 0.0690 - precision_mod: 0.6500 - dur_error: 0.4941 - maestro_dur_loss: 0.0494 - val_loss: 0.1368 - val_f1_score_mod: 0.1209 - val_recall_mod: 0.0663 - val_precision_mod: 0.7118 - val_dur_error: 0.3574 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13660\n",
      "50/50 - 168s - loss: 0.1538 - f1_score_mod: 0.1335 - recall_mod: 0.0747 - precision_mod: 0.6609 - dur_error: 0.4814 - maestro_dur_loss: 0.0481 - val_loss: 0.1464 - val_f1_score_mod: 0.1438 - val_recall_mod: 0.0806 - val_precision_mod: 0.6892 - val_dur_error: 0.4609 - val_maestro_dur_loss: 0.0461\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13660\n",
      "50/50 - 169s - loss: 0.1513 - f1_score_mod: 0.1432 - recall_mod: 0.0805 - precision_mod: 0.6647 - dur_error: 0.4663 - maestro_dur_loss: 0.0466 - val_loss: 0.1590 - val_f1_score_mod: 0.1449 - val_recall_mod: 0.0811 - val_precision_mod: 0.6984 - val_dur_error: 0.5969 - val_maestro_dur_loss: 0.0597\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13660\n",
      "50/50 - 169s - loss: 0.1510 - f1_score_mod: 0.1486 - recall_mod: 0.0839 - precision_mod: 0.6674 - dur_error: 0.4693 - maestro_dur_loss: 0.0469 - val_loss: 0.1417 - val_f1_score_mod: 0.1510 - val_recall_mod: 0.0850 - val_precision_mod: 0.6992 - val_dur_error: 0.4298 - val_maestro_dur_loss: 0.0430\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13660\n",
      "50/50 - 169s - loss: 0.1493 - f1_score_mod: 0.1587 - recall_mod: 0.0904 - precision_mod: 0.6697 - dur_error: 0.4600 - maestro_dur_loss: 0.0460 - val_loss: 0.1501 - val_f1_score_mod: 0.1717 - val_recall_mod: 0.0987 - val_precision_mod: 0.6744 - val_dur_error: 0.5162 - val_maestro_dur_loss: 0.0516\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13660\n",
      "50/50 - 167s - loss: 0.1479 - f1_score_mod: 0.1661 - recall_mod: 0.0950 - precision_mod: 0.6754 - dur_error: 0.4545 - maestro_dur_loss: 0.0455 - val_loss: 0.1398 - val_f1_score_mod: 0.1659 - val_recall_mod: 0.0942 - val_precision_mod: 0.7080 - val_dur_error: 0.4232 - val_maestro_dur_loss: 0.0423\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.13660\n",
      "50/50 - 167s - loss: 0.1468 - f1_score_mod: 0.1704 - recall_mod: 0.0978 - precision_mod: 0.6771 - dur_error: 0.4496 - maestro_dur_loss: 0.0450 - val_loss: 0.1408 - val_f1_score_mod: 0.1698 - val_recall_mod: 0.0965 - val_precision_mod: 0.7228 - val_dur_error: 0.4441 - val_maestro_dur_loss: 0.0444\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.13660 to 0.13303, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 169s - loss: 0.1454 - f1_score_mod: 0.1792 - recall_mod: 0.1033 - precision_mod: 0.6887 - dur_error: 0.4407 - maestro_dur_loss: 0.0441 - val_loss: 0.1330 - val_f1_score_mod: 0.1768 - val_recall_mod: 0.1010 - val_precision_mod: 0.7298 - val_dur_error: 0.3650 - val_maestro_dur_loss: 0.0365\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.13303\n",
      "50/50 - 167s - loss: 0.1451 - f1_score_mod: 0.1857 - recall_mod: 0.1076 - precision_mod: 0.6864 - dur_error: 0.4455 - maestro_dur_loss: 0.0446 - val_loss: 0.1363 - val_f1_score_mod: 0.2082 - val_recall_mod: 0.1237 - val_precision_mod: 0.6703 - val_dur_error: 0.4066 - val_maestro_dur_loss: 0.0407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.13303\n",
      "50/50 - 167s - loss: 0.1432 - f1_score_mod: 0.1930 - recall_mod: 0.1126 - precision_mod: 0.6895 - dur_error: 0.4320 - maestro_dur_loss: 0.0432 - val_loss: 0.1447 - val_f1_score_mod: 0.1901 - val_recall_mod: 0.1095 - val_precision_mod: 0.7374 - val_dur_error: 0.4964 - val_maestro_dur_loss: 0.0496\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.13303 to 0.12528, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 168s - loss: 0.1428 - f1_score_mod: 0.1960 - recall_mod: 0.1146 - precision_mod: 0.6901 - dur_error: 0.4336 - maestro_dur_loss: 0.0434 - val_loss: 0.1253 - val_f1_score_mod: 0.1896 - val_recall_mod: 0.1094 - val_precision_mod: 0.7236 - val_dur_error: 0.3100 - val_maestro_dur_loss: 0.0310\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12528\n",
      "50/50 - 166s - loss: 0.1408 - f1_score_mod: 0.2029 - recall_mod: 0.1192 - precision_mod: 0.6926 - dur_error: 0.4222 - maestro_dur_loss: 0.0422 - val_loss: 0.1350 - val_f1_score_mod: 0.1938 - val_recall_mod: 0.1118 - val_precision_mod: 0.7417 - val_dur_error: 0.4059 - val_maestro_dur_loss: 0.0406\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12528 to 0.12218, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 166s - loss: 0.1408 - f1_score_mod: 0.2113 - recall_mod: 0.1249 - precision_mod: 0.6948 - dur_error: 0.4276 - maestro_dur_loss: 0.0428 - val_loss: 0.1222 - val_f1_score_mod: 0.1948 - val_recall_mod: 0.1122 - val_precision_mod: 0.7536 - val_dur_error: 0.2852 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12218\n",
      "50/50 - 171s - loss: 0.1395 - f1_score_mod: 0.2151 - recall_mod: 0.1274 - precision_mod: 0.7014 - dur_error: 0.4201 - maestro_dur_loss: 0.0420 - val_loss: 0.1258 - val_f1_score_mod: 0.2183 - val_recall_mod: 0.1289 - val_precision_mod: 0.7216 - val_dur_error: 0.3244 - val_maestro_dur_loss: 0.0324\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.12218 to 0.12195, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 167s - loss: 0.1386 - f1_score_mod: 0.2201 - recall_mod: 0.1310 - precision_mod: 0.6983 - dur_error: 0.4166 - maestro_dur_loss: 0.0417 - val_loss: 0.1219 - val_f1_score_mod: 0.2149 - val_recall_mod: 0.1266 - val_precision_mod: 0.7220 - val_dur_error: 0.2920 - val_maestro_dur_loss: 0.0292\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12195\n",
      "50/50 - 168s - loss: 0.1373 - f1_score_mod: 0.2257 - recall_mod: 0.1348 - precision_mod: 0.7031 - dur_error: 0.4096 - maestro_dur_loss: 0.0410 - val_loss: 0.1288 - val_f1_score_mod: 0.2371 - val_recall_mod: 0.1423 - val_precision_mod: 0.7207 - val_dur_error: 0.3662 - val_maestro_dur_loss: 0.0366\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12195\n",
      "50/50 - 168s - loss: 0.1368 - f1_score_mod: 0.2354 - recall_mod: 0.1420 - precision_mod: 0.6992 - dur_error: 0.4102 - maestro_dur_loss: 0.0410 - val_loss: 0.1249 - val_f1_score_mod: 0.2358 - val_recall_mod: 0.1415 - val_precision_mod: 0.7181 - val_dur_error: 0.3320 - val_maestro_dur_loss: 0.0332\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12195\n",
      "50/50 - 169s - loss: 0.1359 - f1_score_mod: 0.2380 - recall_mod: 0.1438 - precision_mod: 0.7030 - dur_error: 0.4066 - maestro_dur_loss: 0.0407 - val_loss: 0.1221 - val_f1_score_mod: 0.2483 - val_recall_mod: 0.1505 - val_precision_mod: 0.7170 - val_dur_error: 0.3070 - val_maestro_dur_loss: 0.0307\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.12195 to 0.11917, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 167s - loss: 0.1350 - f1_score_mod: 0.2430 - recall_mod: 0.1473 - precision_mod: 0.7051 - dur_error: 0.4032 - maestro_dur_loss: 0.0403 - val_loss: 0.1192 - val_f1_score_mod: 0.2417 - val_recall_mod: 0.1450 - val_precision_mod: 0.7357 - val_dur_error: 0.2814 - val_maestro_dur_loss: 0.0281\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.11917 to 0.11874, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 168s - loss: 0.1338 - f1_score_mod: 0.2489 - recall_mod: 0.1515 - precision_mod: 0.7080 - dur_error: 0.3979 - maestro_dur_loss: 0.0398 - val_loss: 0.1187 - val_f1_score_mod: 0.2445 - val_recall_mod: 0.1469 - val_precision_mod: 0.7366 - val_dur_error: 0.2766 - val_maestro_dur_loss: 0.0277\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11874\n",
      "50/50 - 167s - loss: 0.1334 - f1_score_mod: 0.2530 - recall_mod: 0.1541 - precision_mod: 0.7119 - dur_error: 0.3990 - maestro_dur_loss: 0.0399 - val_loss: 0.1226 - val_f1_score_mod: 0.2458 - val_recall_mod: 0.1475 - val_precision_mod: 0.7516 - val_dur_error: 0.3233 - val_maestro_dur_loss: 0.0323\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11874\n",
      "50/50 - 166s - loss: 0.1329 - f1_score_mod: 0.2588 - recall_mod: 0.1586 - precision_mod: 0.7111 - dur_error: 0.3969 - maestro_dur_loss: 0.0397 - val_loss: 0.1289 - val_f1_score_mod: 0.2749 - val_recall_mod: 0.1703 - val_precision_mod: 0.7246 - val_dur_error: 0.3898 - val_maestro_dur_loss: 0.0390\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11874\n",
      "50/50 - 144s - loss: 0.1317 - f1_score_mod: 0.2661 - recall_mod: 0.1640 - precision_mod: 0.7136 - dur_error: 0.3924 - maestro_dur_loss: 0.0392 - val_loss: 0.1260 - val_f1_score_mod: 0.2587 - val_recall_mod: 0.1573 - val_precision_mod: 0.7414 - val_dur_error: 0.3640 - val_maestro_dur_loss: 0.0364\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.11874 to 0.11748, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 162s - loss: 0.1312 - f1_score_mod: 0.2710 - recall_mod: 0.1676 - precision_mod: 0.7137 - dur_error: 0.3917 - maestro_dur_loss: 0.0392 - val_loss: 0.1175 - val_f1_score_mod: 0.2738 - val_recall_mod: 0.1704 - val_precision_mod: 0.7025 - val_dur_error: 0.2823 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11748\n",
      "50/50 - 170s - loss: 0.1300 - f1_score_mod: 0.2791 - recall_mod: 0.1735 - precision_mod: 0.7192 - dur_error: 0.3858 - maestro_dur_loss: 0.0386 - val_loss: 0.1185 - val_f1_score_mod: 0.2837 - val_recall_mod: 0.1781 - val_precision_mod: 0.7072 - val_dur_error: 0.2955 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11748\n",
      "50/50 - 169s - loss: 0.1296 - f1_score_mod: 0.2833 - recall_mod: 0.1768 - precision_mod: 0.7178 - dur_error: 0.3880 - maestro_dur_loss: 0.0388 - val_loss: 0.1179 - val_f1_score_mod: 0.2668 - val_recall_mod: 0.1623 - val_precision_mod: 0.7640 - val_dur_error: 0.2947 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.11748 to 0.11387, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 165s - loss: 0.1287 - f1_score_mod: 0.2870 - recall_mod: 0.1796 - precision_mod: 0.7222 - dur_error: 0.3842 - maestro_dur_loss: 0.0384 - val_loss: 0.1139 - val_f1_score_mod: 0.2838 - val_recall_mod: 0.1768 - val_precision_mod: 0.7308 - val_dur_error: 0.2595 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11387\n",
      "50/50 - 178s - loss: 0.1278 - f1_score_mod: 0.2921 - recall_mod: 0.1835 - precision_mod: 0.7197 - dur_error: 0.3805 - maestro_dur_loss: 0.0381 - val_loss: 0.1172 - val_f1_score_mod: 0.2856 - val_recall_mod: 0.1786 - val_precision_mod: 0.7262 - val_dur_error: 0.2958 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.11387 to 0.11296, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "50/50 - 175s - loss: 0.1271 - f1_score_mod: 0.2973 - recall_mod: 0.1875 - precision_mod: 0.7228 - dur_error: 0.3787 - maestro_dur_loss: 0.0379 - val_loss: 0.1130 - val_f1_score_mod: 0.2872 - val_recall_mod: 0.1787 - val_precision_mod: 0.7445 - val_dur_error: 0.2554 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 42/150\n",
      "Batch 6: Invalid loss, terminating training\n",
      "Batch 7: Invalid loss, terminating training\n",
      "Batch 8: Invalid loss, terminating training\n",
      "Batch 9: Invalid loss, terminating training\n",
      "Batch 10: Invalid loss, terminating training\n",
      "Batch 11: Invalid loss, terminating training\n",
      "Batch 12: Invalid loss, terminating training\n",
      "Batch 13: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11296\n",
      "50/50 - 166s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipnorm = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model did slightly better than the base model in both the training and validation set. It also lasted 8 epochs longer before the failure. Let's now try with clipnorm = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17564, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 167s - loss: 0.2738 - f1_score_mod: 0.0095 - recall_mod: 0.0199 - precision_mod: 0.0833 - dur_error: 0.9981 - maestro_dur_loss: 0.0998 - val_loss: 0.1756 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5405 - val_maestro_dur_loss: 0.0540\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17564 to 0.17140, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 166s - loss: 0.1975 - f1_score_mod: 0.0046 - recall_mod: 0.0023 - precision_mod: 0.3018 - dur_error: 0.6713 - maestro_dur_loss: 0.0671 - val_loss: 0.1714 - val_f1_score_mod: 0.0079 - val_recall_mod: 0.0040 - val_precision_mod: 0.7358 - val_dur_error: 0.5485 - val_maestro_dur_loss: 0.0548\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17140 to 0.15831, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 167s - loss: 0.1850 - f1_score_mod: 0.0245 - recall_mod: 0.0126 - precision_mod: 0.5911 - dur_error: 0.6179 - maestro_dur_loss: 0.0618 - val_loss: 0.1583 - val_f1_score_mod: 0.0223 - val_recall_mod: 0.0113 - val_precision_mod: 0.7858 - val_dur_error: 0.4644 - val_maestro_dur_loss: 0.0464\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15831\n",
      "50/50 - 164s - loss: 0.1782 - f1_score_mod: 0.0484 - recall_mod: 0.0254 - precision_mod: 0.5787 - dur_error: 0.5998 - maestro_dur_loss: 0.0600 - val_loss: 0.1684 - val_f1_score_mod: 0.0376 - val_recall_mod: 0.0193 - val_precision_mod: 0.7150 - val_dur_error: 0.5654 - val_maestro_dur_loss: 0.0565\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15831 to 0.15048, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 160s - loss: 0.1750 - f1_score_mod: 0.0577 - recall_mod: 0.0306 - precision_mod: 0.5722 - dur_error: 0.5868 - maestro_dur_loss: 0.0587 - val_loss: 0.1505 - val_f1_score_mod: 0.0433 - val_recall_mod: 0.0224 - val_precision_mod: 0.7748 - val_dur_error: 0.4153 - val_maestro_dur_loss: 0.0415\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15048 to 0.14654, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 142s - loss: 0.1700 - f1_score_mod: 0.0746 - recall_mod: 0.0399 - precision_mod: 0.6113 - dur_error: 0.5621 - maestro_dur_loss: 0.0562 - val_loss: 0.1465 - val_f1_score_mod: 0.0559 - val_recall_mod: 0.0291 - val_precision_mod: 0.7590 - val_dur_error: 0.3843 - val_maestro_dur_loss: 0.0384\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14654\n",
      "50/50 - 136s - loss: 0.1673 - f1_score_mod: 0.0811 - recall_mod: 0.0436 - precision_mod: 0.6190 - dur_error: 0.5502 - maestro_dur_loss: 0.0550 - val_loss: 0.1615 - val_f1_score_mod: 0.0806 - val_recall_mod: 0.0430 - val_precision_mod: 0.6891 - val_dur_error: 0.5660 - val_maestro_dur_loss: 0.0566\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14654\n",
      "50/50 - 138s - loss: 0.1643 - f1_score_mod: 0.0927 - recall_mod: 0.0502 - precision_mod: 0.6364 - dur_error: 0.5354 - maestro_dur_loss: 0.0535 - val_loss: 0.1552 - val_f1_score_mod: 0.1265 - val_recall_mod: 0.0706 - val_precision_mod: 0.6259 - val_dur_error: 0.5123 - val_maestro_dur_loss: 0.0512\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14654 to 0.13910, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1627 - f1_score_mod: 0.1008 - recall_mod: 0.0550 - precision_mod: 0.6401 - dur_error: 0.5318 - maestro_dur_loss: 0.0532 - val_loss: 0.1391 - val_f1_score_mod: 0.1049 - val_recall_mod: 0.0566 - val_precision_mod: 0.7299 - val_dur_error: 0.3699 - val_maestro_dur_loss: 0.0370\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.13910 to 0.13867, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1610 - f1_score_mod: 0.1125 - recall_mod: 0.0619 - precision_mod: 0.6422 - dur_error: 0.5260 - maestro_dur_loss: 0.0526 - val_loss: 0.1387 - val_f1_score_mod: 0.1039 - val_recall_mod: 0.0562 - val_precision_mod: 0.7039 - val_dur_error: 0.3642 - val_maestro_dur_loss: 0.0364\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13867 to 0.13710, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1576 - f1_score_mod: 0.1188 - recall_mod: 0.0657 - precision_mod: 0.6477 - dur_error: 0.5045 - maestro_dur_loss: 0.0505 - val_loss: 0.1371 - val_f1_score_mod: 0.1069 - val_recall_mod: 0.0578 - val_precision_mod: 0.7328 - val_dur_error: 0.3537 - val_maestro_dur_loss: 0.0354\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13710\n",
      "50/50 - 130s - loss: 0.1556 - f1_score_mod: 0.1264 - recall_mod: 0.0702 - precision_mod: 0.6613 - dur_error: 0.4920 - maestro_dur_loss: 0.0492 - val_loss: 0.1375 - val_f1_score_mod: 0.1279 - val_recall_mod: 0.0704 - val_precision_mod: 0.7155 - val_dur_error: 0.3732 - val_maestro_dur_loss: 0.0373\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13710\n",
      "50/50 - 131s - loss: 0.1548 - f1_score_mod: 0.1376 - recall_mod: 0.0771 - precision_mod: 0.6581 - dur_error: 0.4874 - maestro_dur_loss: 0.0487 - val_loss: 0.1392 - val_f1_score_mod: 0.1454 - val_recall_mod: 0.0817 - val_precision_mod: 0.6834 - val_dur_error: 0.3835 - val_maestro_dur_loss: 0.0384\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13710 to 0.13638, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1534 - f1_score_mod: 0.1386 - recall_mod: 0.0777 - precision_mod: 0.6632 - dur_error: 0.4816 - maestro_dur_loss: 0.0482 - val_loss: 0.1364 - val_f1_score_mod: 0.1299 - val_recall_mod: 0.0718 - val_precision_mod: 0.6936 - val_dur_error: 0.3538 - val_maestro_dur_loss: 0.0354\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13638\n",
      "50/50 - 131s - loss: 0.1518 - f1_score_mod: 0.1469 - recall_mod: 0.0828 - precision_mod: 0.6716 - dur_error: 0.4743 - maestro_dur_loss: 0.0474 - val_loss: 0.1402 - val_f1_score_mod: 0.1563 - val_recall_mod: 0.0885 - val_precision_mod: 0.6844 - val_dur_error: 0.4076 - val_maestro_dur_loss: 0.0408\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13638\n",
      "50/50 - 132s - loss: 0.1506 - f1_score_mod: 0.1562 - recall_mod: 0.0886 - precision_mod: 0.6731 - dur_error: 0.4693 - maestro_dur_loss: 0.0469 - val_loss: 0.1378 - val_f1_score_mod: 0.1652 - val_recall_mod: 0.0943 - val_precision_mod: 0.6834 - val_dur_error: 0.3945 - val_maestro_dur_loss: 0.0395\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.13638 to 0.13138, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1488 - f1_score_mod: 0.1570 - recall_mod: 0.0892 - precision_mod: 0.6759 - dur_error: 0.4562 - maestro_dur_loss: 0.0456 - val_loss: 0.1314 - val_f1_score_mod: 0.1592 - val_recall_mod: 0.0896 - val_precision_mod: 0.7386 - val_dur_error: 0.3415 - val_maestro_dur_loss: 0.0342\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13138 to 0.12637, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 132s - loss: 0.1472 - f1_score_mod: 0.1669 - recall_mod: 0.0954 - precision_mod: 0.6797 - dur_error: 0.4497 - maestro_dur_loss: 0.0450 - val_loss: 0.1264 - val_f1_score_mod: 0.1760 - val_recall_mod: 0.1008 - val_precision_mod: 0.7112 - val_dur_error: 0.2960 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12637\n",
      "50/50 - 134s - loss: 0.1462 - f1_score_mod: 0.1755 - recall_mod: 0.1011 - precision_mod: 0.6820 - dur_error: 0.4464 - maestro_dur_loss: 0.0446 - val_loss: 0.1436 - val_f1_score_mod: 0.1648 - val_recall_mod: 0.0931 - val_precision_mod: 0.7408 - val_dur_error: 0.4686 - val_maestro_dur_loss: 0.0469\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12637\n",
      "50/50 - 132s - loss: 0.1457 - f1_score_mod: 0.1785 - recall_mod: 0.1030 - precision_mod: 0.6843 - dur_error: 0.4441 - maestro_dur_loss: 0.0444 - val_loss: 0.1387 - val_f1_score_mod: 0.1988 - val_recall_mod: 0.1168 - val_precision_mod: 0.6852 - val_dur_error: 0.4190 - val_maestro_dur_loss: 0.0419\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.12637 to 0.12584, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 129s - loss: 0.1445 - f1_score_mod: 0.1850 - recall_mod: 0.1074 - precision_mod: 0.6831 - dur_error: 0.4370 - maestro_dur_loss: 0.0437 - val_loss: 0.1258 - val_f1_score_mod: 0.1849 - val_recall_mod: 0.1067 - val_precision_mod: 0.7106 - val_dur_error: 0.3003 - val_maestro_dur_loss: 0.0300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12584\n",
      "50/50 - 129s - loss: 0.1435 - f1_score_mod: 0.1909 - recall_mod: 0.1113 - precision_mod: 0.6827 - dur_error: 0.4347 - maestro_dur_loss: 0.0435 - val_loss: 0.1270 - val_f1_score_mod: 0.1807 - val_recall_mod: 0.1032 - val_precision_mod: 0.7362 - val_dur_error: 0.3163 - val_maestro_dur_loss: 0.0316\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12584\n",
      "50/50 - 130s - loss: 0.1433 - f1_score_mod: 0.1931 - recall_mod: 0.1125 - precision_mod: 0.6899 - dur_error: 0.4364 - maestro_dur_loss: 0.0436 - val_loss: 0.1283 - val_f1_score_mod: 0.1900 - val_recall_mod: 0.1097 - val_precision_mod: 0.7211 - val_dur_error: 0.3334 - val_maestro_dur_loss: 0.0333\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12584\n",
      "50/50 - 129s - loss: 0.1417 - f1_score_mod: 0.2017 - recall_mod: 0.1185 - precision_mod: 0.6919 - dur_error: 0.4264 - maestro_dur_loss: 0.0426 - val_loss: 0.1312 - val_f1_score_mod: 0.1881 - val_recall_mod: 0.1082 - val_precision_mod: 0.7330 - val_dur_error: 0.3627 - val_maestro_dur_loss: 0.0363\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12584\n",
      "50/50 - 130s - loss: 0.1415 - f1_score_mod: 0.1995 - recall_mod: 0.1172 - precision_mod: 0.6864 - dur_error: 0.4199 - maestro_dur_loss: 0.0420 - val_loss: 0.1313 - val_f1_score_mod: 0.1935 - val_recall_mod: 0.1118 - val_precision_mod: 0.7370 - val_dur_error: 0.3672 - val_maestro_dur_loss: 0.0367\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12584\n",
      "50/50 - 129s - loss: 0.1413 - f1_score_mod: 0.2045 - recall_mod: 0.1203 - precision_mod: 0.6948 - dur_error: 0.4258 - maestro_dur_loss: 0.0426 - val_loss: 0.1289 - val_f1_score_mod: 0.2070 - val_recall_mod: 0.1205 - val_precision_mod: 0.7475 - val_dur_error: 0.3532 - val_maestro_dur_loss: 0.0353\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12584\n",
      "50/50 - 129s - loss: 0.1402 - f1_score_mod: 0.2104 - recall_mod: 0.1239 - precision_mod: 0.7066 - dur_error: 0.4208 - maestro_dur_loss: 0.0421 - val_loss: 0.1405 - val_f1_score_mod: 0.2148 - val_recall_mod: 0.1266 - val_precision_mod: 0.7230 - val_dur_error: 0.4605 - val_maestro_dur_loss: 0.0460\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12584\n",
      "50/50 - 129s - loss: 0.1399 - f1_score_mod: 0.2169 - recall_mod: 0.1287 - precision_mod: 0.7014 - dur_error: 0.4215 - maestro_dur_loss: 0.0422 - val_loss: 0.1344 - val_f1_score_mod: 0.2113 - val_recall_mod: 0.1244 - val_precision_mod: 0.7171 - val_dur_error: 0.4061 - val_maestro_dur_loss: 0.0406\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12584\n",
      "50/50 - 129s - loss: 0.1388 - f1_score_mod: 0.2192 - recall_mod: 0.1302 - precision_mod: 0.7014 - dur_error: 0.4167 - maestro_dur_loss: 0.0417 - val_loss: 0.1308 - val_f1_score_mod: 0.2100 - val_recall_mod: 0.1225 - val_precision_mod: 0.7480 - val_dur_error: 0.3792 - val_maestro_dur_loss: 0.0379\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12584 to 0.12088, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1372 - f1_score_mod: 0.2300 - recall_mod: 0.1377 - precision_mod: 0.7062 - dur_error: 0.4090 - maestro_dur_loss: 0.0409 - val_loss: 0.1209 - val_f1_score_mod: 0.2235 - val_recall_mod: 0.1317 - val_precision_mod: 0.7520 - val_dur_error: 0.2832 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.12088 to 0.11874, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1369 - f1_score_mod: 0.2332 - recall_mod: 0.1399 - precision_mod: 0.7092 - dur_error: 0.4085 - maestro_dur_loss: 0.0409 - val_loss: 0.1187 - val_f1_score_mod: 0.2232 - val_recall_mod: 0.1310 - val_precision_mod: 0.7664 - val_dur_error: 0.2723 - val_maestro_dur_loss: 0.0272\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11874\n",
      "50/50 - 131s - loss: 0.1363 - f1_score_mod: 0.2379 - recall_mod: 0.1429 - precision_mod: 0.7172 - dur_error: 0.4067 - maestro_dur_loss: 0.0407 - val_loss: 0.1202 - val_f1_score_mod: 0.2309 - val_recall_mod: 0.1377 - val_precision_mod: 0.7315 - val_dur_error: 0.2838 - val_maestro_dur_loss: 0.0284\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.11874 to 0.11790, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1358 - f1_score_mod: 0.2383 - recall_mod: 0.1437 - precision_mod: 0.7090 - dur_error: 0.4060 - maestro_dur_loss: 0.0406 - val_loss: 0.1179 - val_f1_score_mod: 0.2357 - val_recall_mod: 0.1413 - val_precision_mod: 0.7212 - val_dur_error: 0.2669 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11790\n",
      "50/50 - 130s - loss: 0.1360 - f1_score_mod: 0.2427 - recall_mod: 0.1470 - precision_mod: 0.7063 - dur_error: 0.4020 - maestro_dur_loss: 0.0402 - val_loss: 0.1300 - val_f1_score_mod: 0.2472 - val_recall_mod: 0.1494 - val_precision_mod: 0.7266 - val_dur_error: 0.3904 - val_maestro_dur_loss: 0.0390\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11790\n",
      "50/50 - 133s - loss: 0.1343 - f1_score_mod: 0.2485 - recall_mod: 0.1508 - precision_mod: 0.7139 - dur_error: 0.3984 - maestro_dur_loss: 0.0398 - val_loss: 0.1265 - val_f1_score_mod: 0.2492 - val_recall_mod: 0.1504 - val_precision_mod: 0.7371 - val_dur_error: 0.3585 - val_maestro_dur_loss: 0.0359\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11790\n",
      "50/50 - 130s - loss: 0.1336 - f1_score_mod: 0.2507 - recall_mod: 0.1523 - precision_mod: 0.7182 - dur_error: 0.3975 - maestro_dur_loss: 0.0397 - val_loss: 0.1215 - val_f1_score_mod: 0.2515 - val_recall_mod: 0.1523 - val_precision_mod: 0.7337 - val_dur_error: 0.3141 - val_maestro_dur_loss: 0.0314\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.11790 to 0.11661, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1330 - f1_score_mod: 0.2537 - recall_mod: 0.1549 - precision_mod: 0.7090 - dur_error: 0.3958 - maestro_dur_loss: 0.0396 - val_loss: 0.1166 - val_f1_score_mod: 0.2497 - val_recall_mod: 0.1501 - val_precision_mod: 0.7533 - val_dur_error: 0.2706 - val_maestro_dur_loss: 0.0271\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11661\n",
      "50/50 - 131s - loss: 0.1325 - f1_score_mod: 0.2602 - recall_mod: 0.1595 - precision_mod: 0.7148 - dur_error: 0.3958 - maestro_dur_loss: 0.0396 - val_loss: 0.1184 - val_f1_score_mod: 0.2475 - val_recall_mod: 0.1484 - val_precision_mod: 0.7603 - val_dur_error: 0.2839 - val_maestro_dur_loss: 0.0284\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11661\n",
      "50/50 - 132s - loss: 0.1319 - f1_score_mod: 0.2649 - recall_mod: 0.1629 - precision_mod: 0.7135 - dur_error: 0.3922 - maestro_dur_loss: 0.0392 - val_loss: 0.1194 - val_f1_score_mod: 0.2530 - val_recall_mod: 0.1525 - val_precision_mod: 0.7568 - val_dur_error: 0.2978 - val_maestro_dur_loss: 0.0298\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11661\n",
      "50/50 - 130s - loss: 0.1307 - f1_score_mod: 0.2714 - recall_mod: 0.1677 - precision_mod: 0.7175 - dur_error: 0.3874 - maestro_dur_loss: 0.0387 - val_loss: 0.1196 - val_f1_score_mod: 0.2723 - val_recall_mod: 0.1680 - val_precision_mod: 0.7296 - val_dur_error: 0.3078 - val_maestro_dur_loss: 0.0308\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.11661 to 0.11445, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1302 - f1_score_mod: 0.2750 - recall_mod: 0.1706 - precision_mod: 0.7157 - dur_error: 0.3869 - maestro_dur_loss: 0.0387 - val_loss: 0.1145 - val_f1_score_mod: 0.2775 - val_recall_mod: 0.1711 - val_precision_mod: 0.7421 - val_dur_error: 0.2589 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11445\n",
      "50/50 - 130s - loss: 0.1293 - f1_score_mod: 0.2809 - recall_mod: 0.1749 - precision_mod: 0.7211 - dur_error: 0.3850 - maestro_dur_loss: 0.0385 - val_loss: 0.1205 - val_f1_score_mod: 0.2782 - val_recall_mod: 0.1717 - val_precision_mod: 0.7507 - val_dur_error: 0.3219 - val_maestro_dur_loss: 0.0322\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11445\n",
      "50/50 - 130s - loss: 0.1284 - f1_score_mod: 0.2848 - recall_mod: 0.1777 - precision_mod: 0.7210 - dur_error: 0.3798 - maestro_dur_loss: 0.0380 - val_loss: 0.1259 - val_f1_score_mod: 0.2935 - val_recall_mod: 0.1849 - val_precision_mod: 0.7255 - val_dur_error: 0.3786 - val_maestro_dur_loss: 0.0379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11445\n",
      "50/50 - 130s - loss: 0.1278 - f1_score_mod: 0.2914 - recall_mod: 0.1830 - precision_mod: 0.7222 - dur_error: 0.3779 - maestro_dur_loss: 0.0378 - val_loss: 0.1174 - val_f1_score_mod: 0.2878 - val_recall_mod: 0.1789 - val_precision_mod: 0.7483 - val_dur_error: 0.2973 - val_maestro_dur_loss: 0.0297\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11445\n",
      "50/50 - 130s - loss: 0.1272 - f1_score_mod: 0.2959 - recall_mod: 0.1863 - precision_mod: 0.7239 - dur_error: 0.3762 - maestro_dur_loss: 0.0376 - val_loss: 0.1161 - val_f1_score_mod: 0.2964 - val_recall_mod: 0.1867 - val_precision_mod: 0.7317 - val_dur_error: 0.2851 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11445\n",
      "50/50 - 130s - loss: 0.1265 - f1_score_mod: 0.3010 - recall_mod: 0.1901 - precision_mod: 0.7300 - dur_error: 0.3751 - maestro_dur_loss: 0.0375 - val_loss: 0.1205 - val_f1_score_mod: 0.3001 - val_recall_mod: 0.1884 - val_precision_mod: 0.7484 - val_dur_error: 0.3357 - val_maestro_dur_loss: 0.0336\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11445\n",
      "50/50 - 129s - loss: 0.1261 - f1_score_mod: 0.3019 - recall_mod: 0.1913 - precision_mod: 0.7223 - dur_error: 0.3752 - maestro_dur_loss: 0.0375 - val_loss: 0.1232 - val_f1_score_mod: 0.2999 - val_recall_mod: 0.1885 - val_precision_mod: 0.7472 - val_dur_error: 0.3612 - val_maestro_dur_loss: 0.0361\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11445\n",
      "50/50 - 131s - loss: 0.1250 - f1_score_mod: 0.3113 - recall_mod: 0.1982 - precision_mod: 0.7314 - dur_error: 0.3691 - maestro_dur_loss: 0.0369 - val_loss: 0.1159 - val_f1_score_mod: 0.3036 - val_recall_mod: 0.1909 - val_precision_mod: 0.7571 - val_dur_error: 0.2953 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11445\n",
      "50/50 - 131s - loss: 0.1242 - f1_score_mod: 0.3139 - recall_mod: 0.2004 - precision_mod: 0.7281 - dur_error: 0.3667 - maestro_dur_loss: 0.0367 - val_loss: 0.1175 - val_f1_score_mod: 0.3160 - val_recall_mod: 0.2021 - val_precision_mod: 0.7331 - val_dur_error: 0.3116 - val_maestro_dur_loss: 0.0312\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11445 to 0.11340, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1232 - f1_score_mod: 0.3216 - recall_mod: 0.2061 - precision_mod: 0.7357 - dur_error: 0.3610 - maestro_dur_loss: 0.0361 - val_loss: 0.1134 - val_f1_score_mod: 0.3181 - val_recall_mod: 0.2051 - val_precision_mod: 0.7195 - val_dur_error: 0.2710 - val_maestro_dur_loss: 0.0271\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11340\n",
      "50/50 - 130s - loss: 0.1233 - f1_score_mod: 0.3242 - recall_mod: 0.2088 - precision_mod: 0.7299 - dur_error: 0.3678 - maestro_dur_loss: 0.0368 - val_loss: 0.1140 - val_f1_score_mod: 0.3194 - val_recall_mod: 0.2049 - val_precision_mod: 0.7316 - val_dur_error: 0.2852 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.11340 to 0.10974, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1220 - f1_score_mod: 0.3301 - recall_mod: 0.2136 - precision_mod: 0.7311 - dur_error: 0.3605 - maestro_dur_loss: 0.0361 - val_loss: 0.1097 - val_f1_score_mod: 0.3101 - val_recall_mod: 0.1956 - val_precision_mod: 0.7570 - val_dur_error: 0.2415 - val_maestro_dur_loss: 0.0241\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.10974\n",
      "50/50 - 130s - loss: 0.1213 - f1_score_mod: 0.3300 - recall_mod: 0.2139 - precision_mod: 0.7255 - dur_error: 0.3576 - maestro_dur_loss: 0.0358 - val_loss: 0.1171 - val_f1_score_mod: 0.3355 - val_recall_mod: 0.2204 - val_precision_mod: 0.7089 - val_dur_error: 0.3154 - val_maestro_dur_loss: 0.0315\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.10974\n",
      "50/50 - 131s - loss: 0.1206 - f1_score_mod: 0.3357 - recall_mod: 0.2184 - precision_mod: 0.7303 - dur_error: 0.3550 - maestro_dur_loss: 0.0355 - val_loss: 0.1102 - val_f1_score_mod: 0.3204 - val_recall_mod: 0.2048 - val_precision_mod: 0.7506 - val_dur_error: 0.2525 - val_maestro_dur_loss: 0.0252\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.10974 to 0.10972, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1203 - f1_score_mod: 0.3410 - recall_mod: 0.2224 - precision_mod: 0.7370 - dur_error: 0.3572 - maestro_dur_loss: 0.0357 - val_loss: 0.1097 - val_f1_score_mod: 0.3315 - val_recall_mod: 0.2146 - val_precision_mod: 0.7385 - val_dur_error: 0.2504 - val_maestro_dur_loss: 0.0250\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.10972\n",
      "50/50 - 130s - loss: 0.1187 - f1_score_mod: 0.3459 - recall_mod: 0.2268 - precision_mod: 0.7340 - dur_error: 0.3476 - maestro_dur_loss: 0.0348 - val_loss: 0.1181 - val_f1_score_mod: 0.3415 - val_recall_mod: 0.2253 - val_precision_mod: 0.7141 - val_dur_error: 0.3362 - val_maestro_dur_loss: 0.0336\n",
      "Epoch 57/150\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.10972\n",
      "50/50 - 130s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipnorm = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more significant improvement here, however the model is still improving. Now let's try setting the clipvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24442, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 148s - loss: 0.3199 - f1_score_mod: 0.0109 - recall_mod: 0.0210 - precision_mod: 0.1042 - dur_error: 1.6951 - maestro_dur_loss: 0.1695 - val_loss: 0.2444 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 1.7735 - val_maestro_dur_loss: 0.1774\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24442 to 0.22461, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 163s - loss: 0.2530 - f1_score_mod: 0.0157 - recall_mod: 0.0081 - precision_mod: 0.3608 - dur_error: 1.7009 - maestro_dur_loss: 0.1701 - val_loss: 0.2246 - val_f1_score_mod: 0.0151 - val_recall_mod: 0.0076 - val_precision_mod: 0.7937 - val_dur_error: 1.7011 - val_maestro_dur_loss: 0.1701\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.22461\n",
      "50/50 - 154s - loss: 0.2348 - f1_score_mod: 0.0533 - recall_mod: 0.0285 - precision_mod: 0.5340 - dur_error: 1.6493 - maestro_dur_loss: 0.1649 - val_loss: 0.2406 - val_f1_score_mod: 0.0702 - val_recall_mod: 0.0406 - val_precision_mod: 0.2625 - val_dur_error: 1.7295 - val_maestro_dur_loss: 0.1730\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22461 to 0.21057, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 161s - loss: 0.2243 - f1_score_mod: 0.0843 - recall_mod: 0.0457 - precision_mod: 0.5989 - dur_error: 1.6322 - maestro_dur_loss: 0.1632 - val_loss: 0.2106 - val_f1_score_mod: 0.1317 - val_recall_mod: 0.0743 - val_precision_mod: 0.5876 - val_dur_error: 1.6899 - val_maestro_dur_loss: 0.1690\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21057 to 0.20307, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 155s - loss: 0.2174 - f1_score_mod: 0.1105 - recall_mod: 0.0609 - precision_mod: 0.6333 - dur_error: 1.6283 - maestro_dur_loss: 0.1628 - val_loss: 0.2031 - val_f1_score_mod: 0.1349 - val_recall_mod: 0.0756 - val_precision_mod: 0.6355 - val_dur_error: 1.7067 - val_maestro_dur_loss: 0.1707\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.20307 to 0.20018, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 156s - loss: 0.2113 - f1_score_mod: 0.1339 - recall_mod: 0.0750 - precision_mod: 0.6524 - dur_error: 1.6180 - maestro_dur_loss: 0.1618 - val_loss: 0.2002 - val_f1_score_mod: 0.1590 - val_recall_mod: 0.0904 - val_precision_mod: 0.6689 - val_dur_error: 1.6976 - val_maestro_dur_loss: 0.1698\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20018 to 0.19464, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 151s - loss: 0.2070 - f1_score_mod: 0.1534 - recall_mod: 0.0874 - precision_mod: 0.6571 - dur_error: 1.6290 - maestro_dur_loss: 0.1629 - val_loss: 0.1946 - val_f1_score_mod: 0.1750 - val_recall_mod: 0.1006 - val_precision_mod: 0.6829 - val_dur_error: 1.6696 - val_maestro_dur_loss: 0.1670\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.19464 to 0.19198, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 143s - loss: 0.2026 - f1_score_mod: 0.1722 - recall_mod: 0.0996 - precision_mod: 0.6650 - dur_error: 1.6298 - maestro_dur_loss: 0.1630 - val_loss: 0.1920 - val_f1_score_mod: 0.1954 - val_recall_mod: 0.1145 - val_precision_mod: 0.6729 - val_dur_error: 1.6960 - val_maestro_dur_loss: 0.1696\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.19198 to 0.19001, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 153s - loss: 0.1998 - f1_score_mod: 0.1907 - recall_mod: 0.1119 - precision_mod: 0.6658 - dur_error: 1.6466 - maestro_dur_loss: 0.1647 - val_loss: 0.1900 - val_f1_score_mod: 0.1879 - val_recall_mod: 0.1090 - val_precision_mod: 0.6914 - val_dur_error: 1.6974 - val_maestro_dur_loss: 0.1697\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.19001 to 0.18742, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 152s - loss: 0.2007 - f1_score_mod: 0.1851 - recall_mod: 0.1082 - precision_mod: 0.6540 - dur_error: 1.6449 - maestro_dur_loss: 0.1645 - val_loss: 0.1874 - val_f1_score_mod: 0.1959 - val_recall_mod: 0.1134 - val_precision_mod: 0.7315 - val_dur_error: 1.6863 - val_maestro_dur_loss: 0.1686\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.18742 to 0.18469, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 166s - loss: 0.1956 - f1_score_mod: 0.2069 - recall_mod: 0.1221 - precision_mod: 0.6860 - dur_error: 1.6481 - maestro_dur_loss: 0.1648 - val_loss: 0.1847 - val_f1_score_mod: 0.2150 - val_recall_mod: 0.1267 - val_precision_mod: 0.7181 - val_dur_error: 1.6779 - val_maestro_dur_loss: 0.1678\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.18469\n",
      "50/50 - 147s - loss: 0.1928 - f1_score_mod: 0.2219 - recall_mod: 0.1324 - precision_mod: 0.6934 - dur_error: 1.6599 - maestro_dur_loss: 0.1660 - val_loss: 0.1865 - val_f1_score_mod: 0.2439 - val_recall_mod: 0.1496 - val_precision_mod: 0.6681 - val_dur_error: 1.7009 - val_maestro_dur_loss: 0.1701\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.18469 to 0.18176, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 192s - loss: 0.1900 - f1_score_mod: 0.2353 - recall_mod: 0.1421 - precision_mod: 0.6932 - dur_error: 1.6618 - maestro_dur_loss: 0.1662 - val_loss: 0.1818 - val_f1_score_mod: 0.2452 - val_recall_mod: 0.1487 - val_precision_mod: 0.7109 - val_dur_error: 1.6782 - val_maestro_dur_loss: 0.1678\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.18176 to 0.17999, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt25.h5\n",
      "50/50 - 153s - loss: 0.1880 - f1_score_mod: 0.2467 - recall_mod: 0.1501 - precision_mod: 0.7010 - dur_error: 1.6529 - maestro_dur_loss: 0.1653 - val_loss: 0.1800 - val_f1_score_mod: 0.2637 - val_recall_mod: 0.1623 - val_precision_mod: 0.7148 - val_dur_error: 1.6905 - val_maestro_dur_loss: 0.1690\n",
      "Epoch 15/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-152b18305d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-d0718b19ceac>\u001b[0m in \u001b[0;36mtrain_lstm_model\u001b[0;34m(n_lstm_layers, n_dense_layers, n_lstm_nodes, dropout_rate, batch_size, harshness, lr, clipnorm, clipvalue, epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \\\n\u001b[0;32m---> 36\u001b[0;31m                     validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# In most preliminary tests model training has failed at some point when the loss becomes NaN during\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipvalue = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aborted early because this model is not learning the durations. Wow this is by far the worst. This is due to this model improving it's bce_loss much faster than its dur_loss. Recall that we limited total_loss to 2 * bce_loss, so there quickly became no effect of the dur_loss on the total_loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out visualize_performance.ipynb (Figures 1 and 2) for a comparison of the model training between those run so far.\n",
    "Next, let's combine the our best clipnorm value (0.5) with an lr of 5e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16899, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 133s - loss: 0.2989 - f1_score_mod: 0.0175 - recall_mod: 0.0298 - precision_mod: 0.0602 - dur_error: 1.0064 - maestro_dur_loss: 0.1006 - val_loss: 0.1690 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4563 - val_maestro_dur_loss: 0.0456\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.16899\n",
      "50/50 - 129s - loss: 0.2062 - f1_score_mod: 0.0020 - recall_mod: 0.0010 - precision_mod: 0.1293 - dur_error: 0.6894 - maestro_dur_loss: 0.0689 - val_loss: 0.1739 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5371 - val_maestro_dur_loss: 0.0537\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16899 to 0.16506, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1924 - f1_score_mod: 0.0037 - recall_mod: 0.0019 - precision_mod: 0.3494 - dur_error: 0.6280 - maestro_dur_loss: 0.0628 - val_loss: 0.1651 - val_f1_score_mod: 4.4370e-04 - val_recall_mod: 2.2201e-04 - val_precision_mod: 0.3182 - val_dur_error: 0.4832 - val_maestro_dur_loss: 0.0483\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16506 to 0.15668, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1834 - f1_score_mod: 0.0117 - recall_mod: 0.0060 - precision_mod: 0.4571 - dur_error: 0.5850 - maestro_dur_loss: 0.0585 - val_loss: 0.1567 - val_f1_score_mod: 0.0097 - val_recall_mod: 0.0049 - val_precision_mod: 0.7718 - val_dur_error: 0.4253 - val_maestro_dur_loss: 0.0425\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15668 to 0.15437, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1779 - f1_score_mod: 0.0229 - recall_mod: 0.0117 - precision_mod: 0.5400 - dur_error: 0.5683 - maestro_dur_loss: 0.0568 - val_loss: 0.1544 - val_f1_score_mod: 0.0168 - val_recall_mod: 0.0085 - val_precision_mod: 0.7634 - val_dur_error: 0.4261 - val_maestro_dur_loss: 0.0426\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15437 to 0.15280, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1733 - f1_score_mod: 0.0357 - recall_mod: 0.0185 - precision_mod: 0.5835 - dur_error: 0.5533 - maestro_dur_loss: 0.0553 - val_loss: 0.1528 - val_f1_score_mod: 0.0264 - val_recall_mod: 0.0135 - val_precision_mod: 0.8100 - val_dur_error: 0.4309 - val_maestro_dur_loss: 0.0431\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15280 to 0.14931, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1698 - f1_score_mod: 0.0459 - recall_mod: 0.0239 - precision_mod: 0.6040 - dur_error: 0.5401 - maestro_dur_loss: 0.0540 - val_loss: 0.1493 - val_f1_score_mod: 0.0412 - val_recall_mod: 0.0213 - val_precision_mod: 0.7971 - val_dur_error: 0.4183 - val_maestro_dur_loss: 0.0418\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14931 to 0.14281, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1666 - f1_score_mod: 0.0607 - recall_mod: 0.0320 - precision_mod: 0.6426 - dur_error: 0.5252 - maestro_dur_loss: 0.0525 - val_loss: 0.1428 - val_f1_score_mod: 0.0686 - val_recall_mod: 0.0361 - val_precision_mod: 0.7441 - val_dur_error: 0.3651 - val_maestro_dur_loss: 0.0365\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14281\n",
      "50/50 - 131s - loss: 0.1639 - f1_score_mod: 0.0729 - recall_mod: 0.0389 - precision_mod: 0.6129 - dur_error: 0.5119 - maestro_dur_loss: 0.0512 - val_loss: 0.1435 - val_f1_score_mod: 0.0705 - val_recall_mod: 0.0371 - val_precision_mod: 0.7534 - val_dur_error: 0.3851 - val_maestro_dur_loss: 0.0385\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14281 to 0.14056, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1615 - f1_score_mod: 0.0846 - recall_mod: 0.0454 - precision_mod: 0.6551 - dur_error: 0.5008 - maestro_dur_loss: 0.0501 - val_loss: 0.1406 - val_f1_score_mod: 0.0779 - val_recall_mod: 0.0412 - val_precision_mod: 0.7459 - val_dur_error: 0.3602 - val_maestro_dur_loss: 0.0360\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.14056 to 0.13548, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1590 - f1_score_mod: 0.0949 - recall_mod: 0.0514 - precision_mod: 0.6531 - dur_error: 0.4873 - maestro_dur_loss: 0.0487 - val_loss: 0.1355 - val_f1_score_mod: 0.1068 - val_recall_mod: 0.0579 - val_precision_mod: 0.7060 - val_dur_error: 0.3248 - val_maestro_dur_loss: 0.0325\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13548\n",
      "50/50 - 130s - loss: 0.1572 - f1_score_mod: 0.1046 - recall_mod: 0.0569 - precision_mod: 0.6669 - dur_error: 0.4785 - maestro_dur_loss: 0.0479 - val_loss: 0.1377 - val_f1_score_mod: 0.0945 - val_recall_mod: 0.0506 - val_precision_mod: 0.7417 - val_dur_error: 0.3478 - val_maestro_dur_loss: 0.0348\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.13548 to 0.13366, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 132s - loss: 0.1553 - f1_score_mod: 0.1110 - recall_mod: 0.0607 - precision_mod: 0.6639 - dur_error: 0.4676 - maestro_dur_loss: 0.0468 - val_loss: 0.1337 - val_f1_score_mod: 0.0952 - val_recall_mod: 0.0509 - val_precision_mod: 0.7562 - val_dur_error: 0.3120 - val_maestro_dur_loss: 0.0312\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13366\n",
      "50/50 - 130s - loss: 0.1534 - f1_score_mod: 0.1173 - recall_mod: 0.0645 - precision_mod: 0.6722 - dur_error: 0.4594 - maestro_dur_loss: 0.0459 - val_loss: 0.1367 - val_f1_score_mod: 0.1075 - val_recall_mod: 0.0579 - val_precision_mod: 0.7681 - val_dur_error: 0.3466 - val_maestro_dur_loss: 0.0347\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13366\n",
      "50/50 - 130s - loss: 0.1526 - f1_score_mod: 0.1263 - recall_mod: 0.0699 - precision_mod: 0.6725 - dur_error: 0.4570 - maestro_dur_loss: 0.0457 - val_loss: 0.1347 - val_f1_score_mod: 0.1342 - val_recall_mod: 0.0741 - val_precision_mod: 0.7267 - val_dur_error: 0.3431 - val_maestro_dur_loss: 0.0343\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13366 to 0.13003, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1509 - f1_score_mod: 0.1320 - recall_mod: 0.0733 - precision_mod: 0.6742 - dur_error: 0.4467 - maestro_dur_loss: 0.0447 - val_loss: 0.1300 - val_f1_score_mod: 0.1463 - val_recall_mod: 0.0817 - val_precision_mod: 0.7117 - val_dur_error: 0.3060 - val_maestro_dur_loss: 0.0306\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.13003 to 0.12930, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1505 - f1_score_mod: 0.1394 - recall_mod: 0.0781 - precision_mod: 0.6705 - dur_error: 0.4475 - maestro_dur_loss: 0.0447 - val_loss: 0.1293 - val_f1_score_mod: 0.1338 - val_recall_mod: 0.0737 - val_precision_mod: 0.7436 - val_dur_error: 0.3012 - val_maestro_dur_loss: 0.0301\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.12930 to 0.12929, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1486 - f1_score_mod: 0.1412 - recall_mod: 0.0790 - precision_mod: 0.6710 - dur_error: 0.4360 - maestro_dur_loss: 0.0436 - val_loss: 0.1293 - val_f1_score_mod: 0.1489 - val_recall_mod: 0.0834 - val_precision_mod: 0.7138 - val_dur_error: 0.3044 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12929\n",
      "50/50 - 130s - loss: 0.1484 - f1_score_mod: 0.1473 - recall_mod: 0.0828 - precision_mod: 0.6800 - dur_error: 0.4405 - maestro_dur_loss: 0.0440 - val_loss: 0.1541 - val_f1_score_mod: 0.1540 - val_recall_mod: 0.0866 - val_precision_mod: 0.7087 - val_dur_error: 0.5547 - val_maestro_dur_loss: 0.0555\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12929\n",
      "50/50 - 130s - loss: 0.1472 - f1_score_mod: 0.1555 - recall_mod: 0.0880 - precision_mod: 0.6786 - dur_error: 0.4340 - maestro_dur_loss: 0.0434 - val_loss: 0.1303 - val_f1_score_mod: 0.1425 - val_recall_mod: 0.0789 - val_precision_mod: 0.7521 - val_dur_error: 0.3198 - val_maestro_dur_loss: 0.0320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12929\n",
      "50/50 - 130s - loss: 0.1462 - f1_score_mod: 0.1601 - recall_mod: 0.0910 - precision_mod: 0.6814 - dur_error: 0.4279 - maestro_dur_loss: 0.0428 - val_loss: 0.1342 - val_f1_score_mod: 0.1648 - val_recall_mod: 0.0936 - val_precision_mod: 0.7086 - val_dur_error: 0.3681 - val_maestro_dur_loss: 0.0368\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.12929 to 0.12630, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1454 - f1_score_mod: 0.1611 - recall_mod: 0.0916 - precision_mod: 0.6828 - dur_error: 0.4247 - maestro_dur_loss: 0.0425 - val_loss: 0.1263 - val_f1_score_mod: 0.1685 - val_recall_mod: 0.0957 - val_precision_mod: 0.7209 - val_dur_error: 0.2892 - val_maestro_dur_loss: 0.0289\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12630\n",
      "50/50 - 130s - loss: 0.1443 - f1_score_mod: 0.1691 - recall_mod: 0.0967 - precision_mod: 0.6855 - dur_error: 0.4202 - maestro_dur_loss: 0.0420 - val_loss: 0.1263 - val_f1_score_mod: 0.1734 - val_recall_mod: 0.0985 - val_precision_mod: 0.7397 - val_dur_error: 0.2956 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12630 to 0.12515, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1440 - f1_score_mod: 0.1706 - recall_mod: 0.0975 - precision_mod: 0.6925 - dur_error: 0.4206 - maestro_dur_loss: 0.0421 - val_loss: 0.1252 - val_f1_score_mod: 0.1762 - val_recall_mod: 0.1006 - val_precision_mod: 0.7285 - val_dur_error: 0.2909 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12515\n",
      "50/50 - 130s - loss: 0.1430 - f1_score_mod: 0.1781 - recall_mod: 0.1026 - precision_mod: 0.6850 - dur_error: 0.4145 - maestro_dur_loss: 0.0414 - val_loss: 0.1268 - val_f1_score_mod: 0.1771 - val_recall_mod: 0.1009 - val_precision_mod: 0.7343 - val_dur_error: 0.3128 - val_maestro_dur_loss: 0.0313\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.12515 to 0.12462, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1421 - f1_score_mod: 0.1814 - recall_mod: 0.1047 - precision_mod: 0.6914 - dur_error: 0.4119 - maestro_dur_loss: 0.0412 - val_loss: 0.1246 - val_f1_score_mod: 0.1891 - val_recall_mod: 0.1094 - val_precision_mod: 0.7132 - val_dur_error: 0.2877 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12462\n",
      "50/50 - 130s - loss: 0.1417 - f1_score_mod: 0.1849 - recall_mod: 0.1069 - precision_mod: 0.6944 - dur_error: 0.4099 - maestro_dur_loss: 0.0410 - val_loss: 0.1311 - val_f1_score_mod: 0.1961 - val_recall_mod: 0.1139 - val_precision_mod: 0.7135 - val_dur_error: 0.3566 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12462\n",
      "50/50 - 130s - loss: 0.1411 - f1_score_mod: 0.1923 - recall_mod: 0.1119 - precision_mod: 0.6945 - dur_error: 0.4086 - maestro_dur_loss: 0.0409 - val_loss: 0.1269 - val_f1_score_mod: 0.1930 - val_recall_mod: 0.1115 - val_precision_mod: 0.7308 - val_dur_error: 0.3207 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12462\n",
      "50/50 - 131s - loss: 0.1405 - f1_score_mod: 0.1915 - recall_mod: 0.1114 - precision_mod: 0.6936 - dur_error: 0.4070 - maestro_dur_loss: 0.0407 - val_loss: 0.1299 - val_f1_score_mod: 0.2092 - val_recall_mod: 0.1237 - val_precision_mod: 0.6880 - val_dur_error: 0.3441 - val_maestro_dur_loss: 0.0344\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12462\n",
      "50/50 - 130s - loss: 0.1399 - f1_score_mod: 0.1951 - recall_mod: 0.1139 - precision_mod: 0.6955 - dur_error: 0.4054 - maestro_dur_loss: 0.0405 - val_loss: 0.1268 - val_f1_score_mod: 0.2065 - val_recall_mod: 0.1212 - val_precision_mod: 0.7081 - val_dur_error: 0.3243 - val_maestro_dur_loss: 0.0324\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.12462 to 0.12455, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1392 - f1_score_mod: 0.2022 - recall_mod: 0.1184 - precision_mod: 0.6997 - dur_error: 0.4012 - maestro_dur_loss: 0.0401 - val_loss: 0.1245 - val_f1_score_mod: 0.2174 - val_recall_mod: 0.1289 - val_precision_mod: 0.7044 - val_dur_error: 0.3013 - val_maestro_dur_loss: 0.0301\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.12455 to 0.12409, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1387 - f1_score_mod: 0.2074 - recall_mod: 0.1224 - precision_mod: 0.6935 - dur_error: 0.4015 - maestro_dur_loss: 0.0401 - val_loss: 0.1241 - val_f1_score_mod: 0.2016 - val_recall_mod: 0.1168 - val_precision_mod: 0.7497 - val_dur_error: 0.3025 - val_maestro_dur_loss: 0.0302\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.12409\n",
      "50/50 - 130s - loss: 0.1378 - f1_score_mod: 0.2087 - recall_mod: 0.1228 - precision_mod: 0.7041 - dur_error: 0.3973 - maestro_dur_loss: 0.0397 - val_loss: 0.1291 - val_f1_score_mod: 0.2238 - val_recall_mod: 0.1332 - val_precision_mod: 0.7119 - val_dur_error: 0.3548 - val_maestro_dur_loss: 0.0355\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.12409\n",
      "50/50 - 130s - loss: 0.1377 - f1_score_mod: 0.2131 - recall_mod: 0.1261 - precision_mod: 0.6995 - dur_error: 0.3975 - maestro_dur_loss: 0.0397 - val_loss: 0.1262 - val_f1_score_mod: 0.2042 - val_recall_mod: 0.1187 - val_precision_mod: 0.7442 - val_dur_error: 0.3250 - val_maestro_dur_loss: 0.0325\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.12409 to 0.11919, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1372 - f1_score_mod: 0.2185 - recall_mod: 0.1297 - precision_mod: 0.7034 - dur_error: 0.3949 - maestro_dur_loss: 0.0395 - val_loss: 0.1192 - val_f1_score_mod: 0.2142 - val_recall_mod: 0.1256 - val_precision_mod: 0.7405 - val_dur_error: 0.2633 - val_maestro_dur_loss: 0.0263\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11919\n",
      "50/50 - 131s - loss: 0.1365 - f1_score_mod: 0.2221 - recall_mod: 0.1320 - precision_mod: 0.7086 - dur_error: 0.3929 - maestro_dur_loss: 0.0393 - val_loss: 0.1256 - val_f1_score_mod: 0.2431 - val_recall_mod: 0.1483 - val_precision_mod: 0.6890 - val_dur_error: 0.3272 - val_maestro_dur_loss: 0.0327\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11919\n",
      "50/50 - 130s - loss: 0.1361 - f1_score_mod: 0.2236 - recall_mod: 0.1332 - precision_mod: 0.7051 - dur_error: 0.3925 - maestro_dur_loss: 0.0393 - val_loss: 0.1246 - val_f1_score_mod: 0.2170 - val_recall_mod: 0.1274 - val_precision_mod: 0.7443 - val_dur_error: 0.3213 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11919\n",
      "50/50 - 130s - loss: 0.1351 - f1_score_mod: 0.2276 - recall_mod: 0.1360 - precision_mod: 0.7077 - dur_error: 0.3879 - maestro_dur_loss: 0.0388 - val_loss: 0.1243 - val_f1_score_mod: 0.2401 - val_recall_mod: 0.1450 - val_precision_mod: 0.7078 - val_dur_error: 0.3211 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11919\n",
      "50/50 - 131s - loss: 0.1350 - f1_score_mod: 0.2320 - recall_mod: 0.1390 - precision_mod: 0.7081 - dur_error: 0.3894 - maestro_dur_loss: 0.0389 - val_loss: 0.1358 - val_f1_score_mod: 0.2440 - val_recall_mod: 0.1482 - val_precision_mod: 0.7013 - val_dur_error: 0.4378 - val_maestro_dur_loss: 0.0438\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11919\n",
      "50/50 - 131s - loss: 0.1346 - f1_score_mod: 0.2368 - recall_mod: 0.1428 - precision_mod: 0.7056 - dur_error: 0.3880 - maestro_dur_loss: 0.0388 - val_loss: 0.1197 - val_f1_score_mod: 0.2395 - val_recall_mod: 0.1436 - val_precision_mod: 0.7317 - val_dur_error: 0.2757 - val_maestro_dur_loss: 0.0276\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11919\n",
      "50/50 - 130s - loss: 0.1340 - f1_score_mod: 0.2383 - recall_mod: 0.1436 - precision_mod: 0.7104 - dur_error: 0.3841 - maestro_dur_loss: 0.0384 - val_loss: 0.1203 - val_f1_score_mod: 0.2381 - val_recall_mod: 0.1428 - val_precision_mod: 0.7266 - val_dur_error: 0.2877 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11919\n",
      "50/50 - 130s - loss: 0.1336 - f1_score_mod: 0.2422 - recall_mod: 0.1462 - precision_mod: 0.7149 - dur_error: 0.3836 - maestro_dur_loss: 0.0384 - val_loss: 0.1251 - val_f1_score_mod: 0.2506 - val_recall_mod: 0.1530 - val_precision_mod: 0.7047 - val_dur_error: 0.3342 - val_maestro_dur_loss: 0.0334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11919\n",
      "50/50 - 130s - loss: 0.1330 - f1_score_mod: 0.2463 - recall_mod: 0.1490 - precision_mod: 0.7182 - dur_error: 0.3831 - maestro_dur_loss: 0.0383 - val_loss: 0.1255 - val_f1_score_mod: 0.2458 - val_recall_mod: 0.1481 - val_precision_mod: 0.7327 - val_dur_error: 0.3431 - val_maestro_dur_loss: 0.0343\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.11919 to 0.11852, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 130s - loss: 0.1325 - f1_score_mod: 0.2478 - recall_mod: 0.1500 - precision_mod: 0.7170 - dur_error: 0.3811 - maestro_dur_loss: 0.0381 - val_loss: 0.1185 - val_f1_score_mod: 0.2414 - val_recall_mod: 0.1441 - val_precision_mod: 0.7521 - val_dur_error: 0.2799 - val_maestro_dur_loss: 0.0280\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11852\n",
      "50/50 - 130s - loss: 0.1321 - f1_score_mod: 0.2489 - recall_mod: 0.1510 - precision_mod: 0.7154 - dur_error: 0.3804 - maestro_dur_loss: 0.0380 - val_loss: 0.1225 - val_f1_score_mod: 0.2562 - val_recall_mod: 0.1564 - val_precision_mod: 0.7175 - val_dur_error: 0.3161 - val_maestro_dur_loss: 0.0316\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.11852 to 0.11636, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1318 - f1_score_mod: 0.2549 - recall_mod: 0.1553 - precision_mod: 0.7167 - dur_error: 0.3794 - maestro_dur_loss: 0.0379 - val_loss: 0.1164 - val_f1_score_mod: 0.2618 - val_recall_mod: 0.1603 - val_precision_mod: 0.7239 - val_dur_error: 0.2600 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11636\n",
      "50/50 - 130s - loss: 0.1310 - f1_score_mod: 0.2585 - recall_mod: 0.1580 - precision_mod: 0.7189 - dur_error: 0.3753 - maestro_dur_loss: 0.0375 - val_loss: 0.1248 - val_f1_score_mod: 0.2705 - val_recall_mod: 0.1668 - val_precision_mod: 0.7234 - val_dur_error: 0.3480 - val_maestro_dur_loss: 0.0348\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11636\n",
      "50/50 - 131s - loss: 0.1308 - f1_score_mod: 0.2616 - recall_mod: 0.1601 - precision_mod: 0.7235 - dur_error: 0.3769 - maestro_dur_loss: 0.0377 - val_loss: 0.1189 - val_f1_score_mod: 0.2606 - val_recall_mod: 0.1590 - val_precision_mod: 0.7333 - val_dur_error: 0.2885 - val_maestro_dur_loss: 0.0289\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11636\n",
      "50/50 - 130s - loss: 0.1303 - f1_score_mod: 0.2631 - recall_mod: 0.1615 - precision_mod: 0.7172 - dur_error: 0.3739 - maestro_dur_loss: 0.0374 - val_loss: 0.1233 - val_f1_score_mod: 0.2790 - val_recall_mod: 0.1745 - val_precision_mod: 0.7028 - val_dur_error: 0.3347 - val_maestro_dur_loss: 0.0335\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11636 to 0.11601, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1296 - f1_score_mod: 0.2727 - recall_mod: 0.1684 - precision_mod: 0.7226 - dur_error: 0.3736 - maestro_dur_loss: 0.0374 - val_loss: 0.1160 - val_f1_score_mod: 0.2670 - val_recall_mod: 0.1637 - val_precision_mod: 0.7383 - val_dur_error: 0.2646 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11601\n",
      "50/50 - 130s - loss: 0.1290 - f1_score_mod: 0.2699 - recall_mod: 0.1664 - precision_mod: 0.7196 - dur_error: 0.3689 - maestro_dur_loss: 0.0369 - val_loss: 0.1208 - val_f1_score_mod: 0.2710 - val_recall_mod: 0.1672 - val_precision_mod: 0.7308 - val_dur_error: 0.3158 - val_maestro_dur_loss: 0.0316\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11601\n",
      "50/50 - 130s - loss: 0.1290 - f1_score_mod: 0.2741 - recall_mod: 0.1695 - precision_mod: 0.7240 - dur_error: 0.3720 - maestro_dur_loss: 0.0372 - val_loss: 0.1218 - val_f1_score_mod: 0.2807 - val_recall_mod: 0.1751 - val_precision_mod: 0.7205 - val_dur_error: 0.3250 - val_maestro_dur_loss: 0.0325\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.11601 to 0.11452, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1278 - f1_score_mod: 0.2818 - recall_mod: 0.1751 - precision_mod: 0.7269 - dur_error: 0.3648 - maestro_dur_loss: 0.0365 - val_loss: 0.1145 - val_f1_score_mod: 0.2678 - val_recall_mod: 0.1637 - val_precision_mod: 0.7468 - val_dur_error: 0.2596 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.11452 to 0.11448, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 132s - loss: 0.1279 - f1_score_mod: 0.2824 - recall_mod: 0.1756 - precision_mod: 0.7286 - dur_error: 0.3665 - maestro_dur_loss: 0.0367 - val_loss: 0.1145 - val_f1_score_mod: 0.2819 - val_recall_mod: 0.1765 - val_precision_mod: 0.7121 - val_dur_error: 0.2547 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11448\n",
      "50/50 - 129s - loss: 0.1274 - f1_score_mod: 0.2870 - recall_mod: 0.1792 - precision_mod: 0.7260 - dur_error: 0.3650 - maestro_dur_loss: 0.0365 - val_loss: 0.1200 - val_f1_score_mod: 0.2793 - val_recall_mod: 0.1728 - val_precision_mod: 0.7433 - val_dur_error: 0.3186 - val_maestro_dur_loss: 0.0319\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11448\n",
      "50/50 - 131s - loss: 0.1269 - f1_score_mod: 0.2889 - recall_mod: 0.1804 - precision_mod: 0.7311 - dur_error: 0.3636 - maestro_dur_loss: 0.0364 - val_loss: 0.1214 - val_f1_score_mod: 0.2868 - val_recall_mod: 0.1800 - val_precision_mod: 0.7200 - val_dur_error: 0.3310 - val_maestro_dur_loss: 0.0331\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11448\n",
      "50/50 - 130s - loss: 0.1264 - f1_score_mod: 0.2932 - recall_mod: 0.1840 - precision_mod: 0.7285 - dur_error: 0.3626 - maestro_dur_loss: 0.0363 - val_loss: 0.1218 - val_f1_score_mod: 0.2895 - val_recall_mod: 0.1810 - val_precision_mod: 0.7300 - val_dur_error: 0.3349 - val_maestro_dur_loss: 0.0335\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.11448 to 0.11289, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1259 - f1_score_mod: 0.2967 - recall_mod: 0.1867 - precision_mod: 0.7270 - dur_error: 0.3606 - maestro_dur_loss: 0.0361 - val_loss: 0.1129 - val_f1_score_mod: 0.2822 - val_recall_mod: 0.1744 - val_precision_mod: 0.7520 - val_dur_error: 0.2505 - val_maestro_dur_loss: 0.0251\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11289\n",
      "50/50 - 132s - loss: 0.1255 - f1_score_mod: 0.2979 - recall_mod: 0.1872 - precision_mod: 0.7335 - dur_error: 0.3620 - maestro_dur_loss: 0.0362 - val_loss: 0.1256 - val_f1_score_mod: 0.3106 - val_recall_mod: 0.1995 - val_precision_mod: 0.7101 - val_dur_error: 0.3791 - val_maestro_dur_loss: 0.0379\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11289\n",
      "50/50 - 131s - loss: 0.1252 - f1_score_mod: 0.3039 - recall_mod: 0.1920 - precision_mod: 0.7347 - dur_error: 0.3609 - maestro_dur_loss: 0.0361 - val_loss: 0.1163 - val_f1_score_mod: 0.2913 - val_recall_mod: 0.1815 - val_precision_mod: 0.7504 - val_dur_error: 0.2841 - val_maestro_dur_loss: 0.0284\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.11289\n",
      "50/50 - 131s - loss: 0.1244 - f1_score_mod: 0.3047 - recall_mod: 0.1928 - precision_mod: 0.7342 - dur_error: 0.3557 - maestro_dur_loss: 0.0356 - val_loss: 0.1131 - val_f1_score_mod: 0.2998 - val_recall_mod: 0.1891 - val_precision_mod: 0.7314 - val_dur_error: 0.2595 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.11289 to 0.11153, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 132s - loss: 0.1240 - f1_score_mod: 0.3094 - recall_mod: 0.1965 - precision_mod: 0.7322 - dur_error: 0.3555 - maestro_dur_loss: 0.0356 - val_loss: 0.1115 - val_f1_score_mod: 0.3039 - val_recall_mod: 0.1923 - val_precision_mod: 0.7335 - val_dur_error: 0.2456 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.11153\n",
      "50/50 - 131s - loss: 0.1236 - f1_score_mod: 0.3127 - recall_mod: 0.1988 - precision_mod: 0.7370 - dur_error: 0.3550 - maestro_dur_loss: 0.0355 - val_loss: 0.1126 - val_f1_score_mod: 0.3032 - val_recall_mod: 0.1911 - val_precision_mod: 0.7429 - val_dur_error: 0.2527 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 64/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00064: val_loss did not improve from 0.11153\n",
      "50/50 - 131s - loss: 0.1231 - f1_score_mod: 0.3171 - recall_mod: 0.2029 - precision_mod: 0.7289 - dur_error: 0.3530 - maestro_dur_loss: 0.0353 - val_loss: 0.1131 - val_f1_score_mod: 0.3042 - val_recall_mod: 0.1925 - val_precision_mod: 0.7369 - val_dur_error: 0.2636 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.11153 to 0.11099, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1229 - f1_score_mod: 0.3200 - recall_mod: 0.2050 - precision_mod: 0.7336 - dur_error: 0.3547 - maestro_dur_loss: 0.0355 - val_loss: 0.1110 - val_f1_score_mod: 0.3055 - val_recall_mod: 0.1936 - val_precision_mod: 0.7372 - val_dur_error: 0.2418 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.11099\n",
      "50/50 - 131s - loss: 0.1226 - f1_score_mod: 0.3227 - recall_mod: 0.2069 - precision_mod: 0.7369 - dur_error: 0.3545 - maestro_dur_loss: 0.0355 - val_loss: 0.1232 - val_f1_score_mod: 0.3163 - val_recall_mod: 0.2039 - val_precision_mod: 0.7127 - val_dur_error: 0.3663 - val_maestro_dur_loss: 0.0366\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.11099\n",
      "50/50 - 131s - loss: 0.1218 - f1_score_mod: 0.3282 - recall_mod: 0.2113 - precision_mod: 0.7383 - dur_error: 0.3518 - maestro_dur_loss: 0.0352 - val_loss: 0.1135 - val_f1_score_mod: 0.3144 - val_recall_mod: 0.2014 - val_precision_mod: 0.7271 - val_dur_error: 0.2733 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.11099\n",
      "50/50 - 131s - loss: 0.1216 - f1_score_mod: 0.3296 - recall_mod: 0.2126 - precision_mod: 0.7377 - dur_error: 0.3501 - maestro_dur_loss: 0.0350 - val_loss: 0.1163 - val_f1_score_mod: 0.3223 - val_recall_mod: 0.2073 - val_precision_mod: 0.7332 - val_dur_error: 0.3046 - val_maestro_dur_loss: 0.0305\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.11099\n",
      "50/50 - 131s - loss: 0.1209 - f1_score_mod: 0.3287 - recall_mod: 0.2121 - precision_mod: 0.7347 - dur_error: 0.3488 - maestro_dur_loss: 0.0349 - val_loss: 0.1145 - val_f1_score_mod: 0.3184 - val_recall_mod: 0.2045 - val_precision_mod: 0.7266 - val_dur_error: 0.2830 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.11099\n",
      "50/50 - 131s - loss: 0.1210 - f1_score_mod: 0.3369 - recall_mod: 0.2188 - precision_mod: 0.7389 - dur_error: 0.3516 - maestro_dur_loss: 0.0352 - val_loss: 0.1132 - val_f1_score_mod: 0.3165 - val_recall_mod: 0.2007 - val_precision_mod: 0.7567 - val_dur_error: 0.2753 - val_maestro_dur_loss: 0.0275\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.11099\n",
      "50/50 - 131s - loss: 0.1199 - f1_score_mod: 0.3383 - recall_mod: 0.2199 - precision_mod: 0.7376 - dur_error: 0.3445 - maestro_dur_loss: 0.0344 - val_loss: 0.1217 - val_f1_score_mod: 0.3346 - val_recall_mod: 0.2187 - val_precision_mod: 0.7204 - val_dur_error: 0.3621 - val_maestro_dur_loss: 0.0362\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.11099 to 0.10952, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1196 - f1_score_mod: 0.3430 - recall_mod: 0.2237 - precision_mod: 0.7389 - dur_error: 0.3442 - maestro_dur_loss: 0.0344 - val_loss: 0.1095 - val_f1_score_mod: 0.3332 - val_recall_mod: 0.2170 - val_precision_mod: 0.7275 - val_dur_error: 0.2403 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.10952 to 0.10938, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1192 - f1_score_mod: 0.3449 - recall_mod: 0.2255 - precision_mod: 0.7367 - dur_error: 0.3439 - maestro_dur_loss: 0.0344 - val_loss: 0.1094 - val_f1_score_mod: 0.3265 - val_recall_mod: 0.2108 - val_precision_mod: 0.7320 - val_dur_error: 0.2372 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10938\n",
      "50/50 - 131s - loss: 0.1192 - f1_score_mod: 0.3441 - recall_mod: 0.2247 - precision_mod: 0.7395 - dur_error: 0.3475 - maestro_dur_loss: 0.0348 - val_loss: 0.1134 - val_f1_score_mod: 0.3417 - val_recall_mod: 0.2245 - val_precision_mod: 0.7209 - val_dur_error: 0.2824 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10938\n",
      "50/50 - 131s - loss: 0.1184 - f1_score_mod: 0.3525 - recall_mod: 0.2318 - precision_mod: 0.7393 - dur_error: 0.3420 - maestro_dur_loss: 0.0342 - val_loss: 0.1113 - val_f1_score_mod: 0.3417 - val_recall_mod: 0.2258 - val_precision_mod: 0.7122 - val_dur_error: 0.2631 - val_maestro_dur_loss: 0.0263\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.10938 to 0.10902, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1177 - f1_score_mod: 0.3512 - recall_mod: 0.2306 - precision_mod: 0.7418 - dur_error: 0.3385 - maestro_dur_loss: 0.0338 - val_loss: 0.1090 - val_f1_score_mod: 0.3319 - val_recall_mod: 0.2155 - val_precision_mod: 0.7313 - val_dur_error: 0.2370 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10902\n",
      "50/50 - 135s - loss: 0.1177 - f1_score_mod: 0.3581 - recall_mod: 0.2365 - precision_mod: 0.7407 - dur_error: 0.3433 - maestro_dur_loss: 0.0343 - val_loss: 0.1122 - val_f1_score_mod: 0.3448 - val_recall_mod: 0.2273 - val_precision_mod: 0.7222 - val_dur_error: 0.2749 - val_maestro_dur_loss: 0.0275\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10902\n",
      "50/50 - 131s - loss: 0.1164 - f1_score_mod: 0.3639 - recall_mod: 0.2409 - precision_mod: 0.7474 - dur_error: 0.3358 - maestro_dur_loss: 0.0336 - val_loss: 0.1178 - val_f1_score_mod: 0.3423 - val_recall_mod: 0.2243 - val_precision_mod: 0.7330 - val_dur_error: 0.3363 - val_maestro_dur_loss: 0.0336\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.10902\n",
      "50/50 - 131s - loss: 0.1168 - f1_score_mod: 0.3650 - recall_mod: 0.2424 - precision_mod: 0.7427 - dur_error: 0.3409 - maestro_dur_loss: 0.0341 - val_loss: 0.1093 - val_f1_score_mod: 0.3451 - val_recall_mod: 0.2263 - val_precision_mod: 0.7366 - val_dur_error: 0.2483 - val_maestro_dur_loss: 0.0248\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.10902 to 0.10809, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 132s - loss: 0.1159 - f1_score_mod: 0.3671 - recall_mod: 0.2446 - precision_mod: 0.7394 - dur_error: 0.3354 - maestro_dur_loss: 0.0335 - val_loss: 0.1081 - val_f1_score_mod: 0.3486 - val_recall_mod: 0.2301 - val_precision_mod: 0.7255 - val_dur_error: 0.2362 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.10809 to 0.10797, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 132s - loss: 0.1159 - f1_score_mod: 0.3713 - recall_mod: 0.2478 - precision_mod: 0.7449 - dur_error: 0.3381 - maestro_dur_loss: 0.0338 - val_loss: 0.1080 - val_f1_score_mod: 0.3476 - val_recall_mod: 0.2289 - val_precision_mod: 0.7321 - val_dur_error: 0.2395 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10797\n",
      "50/50 - 131s - loss: 0.1154 - f1_score_mod: 0.3720 - recall_mod: 0.2489 - precision_mod: 0.7408 - dur_error: 0.3357 - maestro_dur_loss: 0.0336 - val_loss: 0.1083 - val_f1_score_mod: 0.3499 - val_recall_mod: 0.2303 - val_precision_mod: 0.7387 - val_dur_error: 0.2429 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.10797 to 0.10790, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1145 - f1_score_mod: 0.3777 - recall_mod: 0.2534 - precision_mod: 0.7452 - dur_error: 0.3336 - maestro_dur_loss: 0.0334 - val_loss: 0.1079 - val_f1_score_mod: 0.3555 - val_recall_mod: 0.2366 - val_precision_mod: 0.7230 - val_dur_error: 0.2367 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10790\n",
      "50/50 - 131s - loss: 0.1142 - f1_score_mod: 0.3790 - recall_mod: 0.2548 - precision_mod: 0.7422 - dur_error: 0.3313 - maestro_dur_loss: 0.0331 - val_loss: 0.1103 - val_f1_score_mod: 0.3740 - val_recall_mod: 0.2587 - val_precision_mod: 0.6799 - val_dur_error: 0.2560 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10790\n",
      "50/50 - 135s - loss: 0.1137 - f1_score_mod: 0.3829 - recall_mod: 0.2584 - precision_mod: 0.7415 - dur_error: 0.3314 - maestro_dur_loss: 0.0331 - val_loss: 0.1114 - val_f1_score_mod: 0.3658 - val_recall_mod: 0.2464 - val_precision_mod: 0.7175 - val_dur_error: 0.2804 - val_maestro_dur_loss: 0.0280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/150\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10790\n",
      "50/50 - 130s - loss: 0.1133 - f1_score_mod: 0.3890 - recall_mod: 0.2633 - precision_mod: 0.7463 - dur_error: 0.3311 - maestro_dur_loss: 0.0331 - val_loss: 0.1110 - val_f1_score_mod: 0.3646 - val_recall_mod: 0.2452 - val_precision_mod: 0.7155 - val_dur_error: 0.2741 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 87/150\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10790\n",
      "50/50 - 130s - loss: 0.1128 - f1_score_mod: 0.3886 - recall_mod: 0.2637 - precision_mod: 0.7431 - dur_error: 0.3291 - maestro_dur_loss: 0.0329 - val_loss: 0.1166 - val_f1_score_mod: 0.3647 - val_recall_mod: 0.2461 - val_precision_mod: 0.7102 - val_dur_error: 0.3297 - val_maestro_dur_loss: 0.0330\n",
      "Epoch 88/150\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.10790 to 0.10781, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1123 - f1_score_mod: 0.3929 - recall_mod: 0.2675 - precision_mod: 0.7430 - dur_error: 0.3280 - maestro_dur_loss: 0.0328 - val_loss: 0.1078 - val_f1_score_mod: 0.3604 - val_recall_mod: 0.2397 - val_precision_mod: 0.7337 - val_dur_error: 0.2409 - val_maestro_dur_loss: 0.0241\n",
      "Epoch 89/150\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10781\n",
      "50/50 - 131s - loss: 0.1118 - f1_score_mod: 0.3961 - recall_mod: 0.2704 - precision_mod: 0.7439 - dur_error: 0.3273 - maestro_dur_loss: 0.0327 - val_loss: 0.1082 - val_f1_score_mod: 0.3787 - val_recall_mod: 0.2621 - val_precision_mod: 0.6857 - val_dur_error: 0.2419 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 90/150\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.10781 to 0.10721, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1113 - f1_score_mod: 0.3996 - recall_mod: 0.2741 - precision_mod: 0.7399 - dur_error: 0.3241 - maestro_dur_loss: 0.0324 - val_loss: 0.1072 - val_f1_score_mod: 0.3711 - val_recall_mod: 0.2518 - val_precision_mod: 0.7127 - val_dur_error: 0.2390 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 91/150\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.10721 to 0.10662, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt5.h5\n",
      "50/50 - 131s - loss: 0.1111 - f1_score_mod: 0.4017 - recall_mod: 0.2754 - precision_mod: 0.7452 - dur_error: 0.3259 - maestro_dur_loss: 0.0326 - val_loss: 0.1066 - val_f1_score_mod: 0.3709 - val_recall_mod: 0.2507 - val_precision_mod: 0.7172 - val_dur_error: 0.2342 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 92/150\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10662\n",
      "50/50 - 131s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is did not quite reach the best model yet, and that may be due to the random initialization of weights by keras. However, one can be pretty confident that the clipnorm effect is overshadowed by the lower learning rate from Figures 3 and 4 (see visualize_performance.ipynb). Therefore, we will run another with clipnorm = 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17173, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 141s - loss: 0.2930 - f1_score_mod: 0.0197 - recall_mod: 0.0332 - precision_mod: 0.0624 - dur_error: 0.9637 - maestro_dur_loss: 0.0964 - val_loss: 0.1717 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4630 - val_maestro_dur_loss: 0.0463\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17173 to 0.16473, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.2041 - f1_score_mod: 0.0019 - recall_mod: 9.8314e-04 - precision_mod: 0.1340 - dur_error: 0.6732 - maestro_dur_loss: 0.0673 - val_loss: 0.1647 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4387 - val_maestro_dur_loss: 0.0439\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16473\n",
      "50/50 - 132s - loss: 0.1904 - f1_score_mod: 0.0042 - recall_mod: 0.0021 - precision_mod: 0.3371 - dur_error: 0.6120 - maestro_dur_loss: 0.0612 - val_loss: 0.1719 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5260 - val_maestro_dur_loss: 0.0526\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16473 to 0.15424, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 131s - loss: 0.1828 - f1_score_mod: 0.0106 - recall_mod: 0.0054 - precision_mod: 0.4420 - dur_error: 0.5873 - maestro_dur_loss: 0.0587 - val_loss: 0.1542 - val_f1_score_mod: 0.0020 - val_recall_mod: 0.0010 - val_precision_mod: 0.4886 - val_dur_error: 0.4093 - val_maestro_dur_loss: 0.0409\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15424\n",
      "50/50 - 131s - loss: 0.1767 - f1_score_mod: 0.0230 - recall_mod: 0.0118 - precision_mod: 0.4784 - dur_error: 0.5634 - maestro_dur_loss: 0.0563 - val_loss: 0.1649 - val_f1_score_mod: 0.0327 - val_recall_mod: 0.0169 - val_precision_mod: 0.6042 - val_dur_error: 0.5360 - val_maestro_dur_loss: 0.0536\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15424 to 0.14882, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 132s - loss: 0.1724 - f1_score_mod: 0.0380 - recall_mod: 0.0198 - precision_mod: 0.5610 - dur_error: 0.5470 - maestro_dur_loss: 0.0547 - val_loss: 0.1488 - val_f1_score_mod: 0.0286 - val_recall_mod: 0.0146 - val_precision_mod: 0.7801 - val_dur_error: 0.3974 - val_maestro_dur_loss: 0.0397\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14882\n",
      "50/50 - 133s - loss: 0.1692 - f1_score_mod: 0.0493 - recall_mod: 0.0259 - precision_mod: 0.5808 - dur_error: 0.5341 - maestro_dur_loss: 0.0534 - val_loss: 0.1540 - val_f1_score_mod: 0.0452 - val_recall_mod: 0.0234 - val_precision_mod: 0.7574 - val_dur_error: 0.4616 - val_maestro_dur_loss: 0.0462\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14882 to 0.14293, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1664 - f1_score_mod: 0.0580 - recall_mod: 0.0306 - precision_mod: 0.6000 - dur_error: 0.5207 - maestro_dur_loss: 0.0521 - val_loss: 0.1429 - val_f1_score_mod: 0.0562 - val_recall_mod: 0.0293 - val_precision_mod: 0.7462 - val_dur_error: 0.3604 - val_maestro_dur_loss: 0.0360\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14293 to 0.14075, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1638 - f1_score_mod: 0.0677 - recall_mod: 0.0359 - precision_mod: 0.6288 - dur_error: 0.5066 - maestro_dur_loss: 0.0507 - val_loss: 0.1408 - val_f1_score_mod: 0.0654 - val_recall_mod: 0.0343 - val_precision_mod: 0.7516 - val_dur_error: 0.3494 - val_maestro_dur_loss: 0.0349\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14075\n",
      "50/50 - 133s - loss: 0.1613 - f1_score_mod: 0.0764 - recall_mod: 0.0408 - precision_mod: 0.6303 - dur_error: 0.4955 - maestro_dur_loss: 0.0495 - val_loss: 0.1417 - val_f1_score_mod: 0.0695 - val_recall_mod: 0.0365 - val_precision_mod: 0.7827 - val_dur_error: 0.3649 - val_maestro_dur_loss: 0.0365\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14075\n",
      "50/50 - 133s - loss: 0.1595 - f1_score_mod: 0.0879 - recall_mod: 0.0474 - precision_mod: 0.6430 - dur_error: 0.4851 - maestro_dur_loss: 0.0485 - val_loss: 0.1485 - val_f1_score_mod: 0.1070 - val_recall_mod: 0.0582 - val_precision_mod: 0.6931 - val_dur_error: 0.4376 - val_maestro_dur_loss: 0.0438\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.14075 to 0.13858, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1574 - f1_score_mod: 0.0953 - recall_mod: 0.0517 - precision_mod: 0.6377 - dur_error: 0.4734 - maestro_dur_loss: 0.0473 - val_loss: 0.1386 - val_f1_score_mod: 0.0847 - val_recall_mod: 0.0449 - val_precision_mod: 0.7702 - val_dur_error: 0.3501 - val_maestro_dur_loss: 0.0350\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13858\n",
      "50/50 - 132s - loss: 0.1567 - f1_score_mod: 0.1003 - recall_mod: 0.0546 - precision_mod: 0.6395 - dur_error: 0.4767 - maestro_dur_loss: 0.0477 - val_loss: 0.1477 - val_f1_score_mod: 0.0926 - val_recall_mod: 0.0495 - val_precision_mod: 0.7356 - val_dur_error: 0.4379 - val_maestro_dur_loss: 0.0438\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13858 to 0.13737, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1549 - f1_score_mod: 0.1084 - recall_mod: 0.0594 - precision_mod: 0.6577 - dur_error: 0.4650 - maestro_dur_loss: 0.0465 - val_loss: 0.1374 - val_f1_score_mod: 0.1130 - val_recall_mod: 0.0614 - val_precision_mod: 0.7213 - val_dur_error: 0.3522 - val_maestro_dur_loss: 0.0352\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13737\n",
      "50/50 - 131s - loss: 0.1526 - f1_score_mod: 0.1154 - recall_mod: 0.0636 - precision_mod: 0.6447 - dur_error: 0.4508 - maestro_dur_loss: 0.0451 - val_loss: 0.1495 - val_f1_score_mod: 0.1334 - val_recall_mod: 0.0740 - val_precision_mod: 0.6975 - val_dur_error: 0.4780 - val_maestro_dur_loss: 0.0478\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13737 to 0.13478, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 131s - loss: 0.1519 - f1_score_mod: 0.1250 - recall_mod: 0.0693 - precision_mod: 0.6534 - dur_error: 0.4483 - maestro_dur_loss: 0.0448 - val_loss: 0.1348 - val_f1_score_mod: 0.1276 - val_recall_mod: 0.0700 - val_precision_mod: 0.7338 - val_dur_error: 0.3407 - val_maestro_dur_loss: 0.0341\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13478\n",
      "50/50 - 132s - loss: 0.1504 - f1_score_mod: 0.1298 - recall_mod: 0.0722 - precision_mod: 0.6601 - dur_error: 0.4405 - maestro_dur_loss: 0.0440 - val_loss: 0.1440 - val_f1_score_mod: 0.1432 - val_recall_mod: 0.0801 - val_precision_mod: 0.7019 - val_dur_error: 0.4348 - val_maestro_dur_loss: 0.0435\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13478\n",
      "50/50 - 132s - loss: 0.1498 - f1_score_mod: 0.1347 - recall_mod: 0.0751 - precision_mod: 0.6630 - dur_error: 0.4423 - maestro_dur_loss: 0.0442 - val_loss: 0.1379 - val_f1_score_mod: 0.1579 - val_recall_mod: 0.0893 - val_precision_mod: 0.6970 - val_dur_error: 0.3811 - val_maestro_dur_loss: 0.0381\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.13478 to 0.13239, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 132s - loss: 0.1483 - f1_score_mod: 0.1386 - recall_mod: 0.0777 - precision_mod: 0.6628 - dur_error: 0.4331 - maestro_dur_loss: 0.0433 - val_loss: 0.1324 - val_f1_score_mod: 0.1405 - val_recall_mod: 0.0779 - val_precision_mod: 0.7315 - val_dur_error: 0.3321 - val_maestro_dur_loss: 0.0332\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.13239\n",
      "50/50 - 132s - loss: 0.1478 - f1_score_mod: 0.1424 - recall_mod: 0.0798 - precision_mod: 0.6749 - dur_error: 0.4325 - maestro_dur_loss: 0.0433 - val_loss: 0.1457 - val_f1_score_mod: 0.1624 - val_recall_mod: 0.0920 - val_precision_mod: 0.7074 - val_dur_error: 0.4672 - val_maestro_dur_loss: 0.0467\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.13239\n",
      "50/50 - 132s - loss: 0.1469 - f1_score_mod: 0.1516 - recall_mod: 0.0855 - precision_mod: 0.6793 - dur_error: 0.4300 - maestro_dur_loss: 0.0430 - val_loss: 0.1399 - val_f1_score_mod: 0.1869 - val_recall_mod: 0.1100 - val_precision_mod: 0.6335 - val_dur_error: 0.4062 - val_maestro_dur_loss: 0.0406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.13239\n",
      "50/50 - 133s - loss: 0.1458 - f1_score_mod: 0.1537 - recall_mod: 0.0871 - precision_mod: 0.6704 - dur_error: 0.4222 - maestro_dur_loss: 0.0422 - val_loss: 0.1394 - val_f1_score_mod: 0.1745 - val_recall_mod: 0.0999 - val_precision_mod: 0.6994 - val_dur_error: 0.4169 - val_maestro_dur_loss: 0.0417\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.13239 to 0.12735, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1452 - f1_score_mod: 0.1586 - recall_mod: 0.0902 - precision_mod: 0.6773 - dur_error: 0.4207 - maestro_dur_loss: 0.0421 - val_loss: 0.1273 - val_f1_score_mod: 0.1674 - val_recall_mod: 0.0949 - val_precision_mod: 0.7225 - val_dur_error: 0.3014 - val_maestro_dur_loss: 0.0301\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12735 to 0.12699, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 134s - loss: 0.1442 - f1_score_mod: 0.1656 - recall_mod: 0.0945 - precision_mod: 0.6833 - dur_error: 0.4160 - maestro_dur_loss: 0.0416 - val_loss: 0.1270 - val_f1_score_mod: 0.1443 - val_recall_mod: 0.0798 - val_precision_mod: 0.7794 - val_dur_error: 0.2964 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12699\n",
      "50/50 - 133s - loss: 0.1436 - f1_score_mod: 0.1675 - recall_mod: 0.0957 - precision_mod: 0.6816 - dur_error: 0.4151 - maestro_dur_loss: 0.0415 - val_loss: 0.1316 - val_f1_score_mod: 0.1969 - val_recall_mod: 0.1157 - val_precision_mod: 0.6769 - val_dur_error: 0.3461 - val_maestro_dur_loss: 0.0346\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12699\n",
      "50/50 - 134s - loss: 0.1436 - f1_score_mod: 0.1726 - recall_mod: 0.0991 - precision_mod: 0.6837 - dur_error: 0.4182 - maestro_dur_loss: 0.0418 - val_loss: 0.1331 - val_f1_score_mod: 0.1810 - val_recall_mod: 0.1040 - val_precision_mod: 0.7101 - val_dur_error: 0.3662 - val_maestro_dur_loss: 0.0366\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.12699 to 0.12464, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 134s - loss: 0.1424 - f1_score_mod: 0.1757 - recall_mod: 0.1010 - precision_mod: 0.6842 - dur_error: 0.4114 - maestro_dur_loss: 0.0411 - val_loss: 0.1246 - val_f1_score_mod: 0.1814 - val_recall_mod: 0.1040 - val_precision_mod: 0.7213 - val_dur_error: 0.2838 - val_maestro_dur_loss: 0.0284\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.12464 to 0.12430, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 134s - loss: 0.1419 - f1_score_mod: 0.1825 - recall_mod: 0.1055 - precision_mod: 0.6870 - dur_error: 0.4118 - maestro_dur_loss: 0.0412 - val_loss: 0.1243 - val_f1_score_mod: 0.1851 - val_recall_mod: 0.1064 - val_precision_mod: 0.7289 - val_dur_error: 0.2864 - val_maestro_dur_loss: 0.0286\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12430\n",
      "50/50 - 133s - loss: 0.1412 - f1_score_mod: 0.1858 - recall_mod: 0.1077 - precision_mod: 0.6910 - dur_error: 0.4077 - maestro_dur_loss: 0.0408 - val_loss: 0.1244 - val_f1_score_mod: 0.1887 - val_recall_mod: 0.1084 - val_precision_mod: 0.7425 - val_dur_error: 0.2911 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12430\n",
      "50/50 - 134s - loss: 0.1404 - f1_score_mod: 0.1896 - recall_mod: 0.1103 - precision_mod: 0.6896 - dur_error: 0.4036 - maestro_dur_loss: 0.0404 - val_loss: 0.1323 - val_f1_score_mod: 0.1990 - val_recall_mod: 0.1161 - val_precision_mod: 0.7101 - val_dur_error: 0.3699 - val_maestro_dur_loss: 0.0370\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.12430\n",
      "50/50 - 134s - loss: 0.1405 - f1_score_mod: 0.1955 - recall_mod: 0.1141 - precision_mod: 0.6946 - dur_error: 0.4092 - maestro_dur_loss: 0.0409 - val_loss: 0.1385 - val_f1_score_mod: 0.2158 - val_recall_mod: 0.1282 - val_precision_mod: 0.6936 - val_dur_error: 0.4366 - val_maestro_dur_loss: 0.0437\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.12430\n",
      "50/50 - 134s - loss: 0.1393 - f1_score_mod: 0.1986 - recall_mod: 0.1161 - precision_mod: 0.6976 - dur_error: 0.4005 - maestro_dur_loss: 0.0400 - val_loss: 0.1252 - val_f1_score_mod: 0.1873 - val_recall_mod: 0.1075 - val_precision_mod: 0.7403 - val_dur_error: 0.3005 - val_maestro_dur_loss: 0.0301\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.12430 to 0.12082, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 134s - loss: 0.1391 - f1_score_mod: 0.2043 - recall_mod: 0.1200 - precision_mod: 0.6984 - dur_error: 0.4039 - maestro_dur_loss: 0.0404 - val_loss: 0.1208 - val_f1_score_mod: 0.2016 - val_recall_mod: 0.1173 - val_precision_mod: 0.7284 - val_dur_error: 0.2686 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.12082 to 0.12061, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 134s - loss: 0.1382 - f1_score_mod: 0.2064 - recall_mod: 0.1215 - precision_mod: 0.6979 - dur_error: 0.3967 - maestro_dur_loss: 0.0397 - val_loss: 0.1206 - val_f1_score_mod: 0.2156 - val_recall_mod: 0.1272 - val_precision_mod: 0.7176 - val_dur_error: 0.2694 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.12061\n",
      "50/50 - 134s - loss: 0.1374 - f1_score_mod: 0.2118 - recall_mod: 0.1250 - precision_mod: 0.7015 - dur_error: 0.3940 - maestro_dur_loss: 0.0394 - val_loss: 0.1240 - val_f1_score_mod: 0.2065 - val_recall_mod: 0.1201 - val_precision_mod: 0.7471 - val_dur_error: 0.3049 - val_maestro_dur_loss: 0.0305\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.12061\n",
      "50/50 - 134s - loss: 0.1372 - f1_score_mod: 0.2174 - recall_mod: 0.1289 - precision_mod: 0.6995 - dur_error: 0.3958 - maestro_dur_loss: 0.0396 - val_loss: 0.1216 - val_f1_score_mod: 0.2114 - val_recall_mod: 0.1237 - val_precision_mod: 0.7411 - val_dur_error: 0.2859 - val_maestro_dur_loss: 0.0286\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.12061\n",
      "50/50 - 134s - loss: 0.1368 - f1_score_mod: 0.2170 - recall_mod: 0.1287 - precision_mod: 0.7007 - dur_error: 0.3943 - maestro_dur_loss: 0.0394 - val_loss: 0.1223 - val_f1_score_mod: 0.2196 - val_recall_mod: 0.1297 - val_precision_mod: 0.7307 - val_dur_error: 0.2911 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.12061 to 0.11863, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 136s - loss: 0.1360 - f1_score_mod: 0.2235 - recall_mod: 0.1332 - precision_mod: 0.7030 - dur_error: 0.3908 - maestro_dur_loss: 0.0391 - val_loss: 0.1186 - val_f1_score_mod: 0.2234 - val_recall_mod: 0.1322 - val_precision_mod: 0.7355 - val_dur_error: 0.2604 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11863\n",
      "50/50 - 135s - loss: 0.1353 - f1_score_mod: 0.2269 - recall_mod: 0.1357 - precision_mod: 0.7015 - dur_error: 0.3874 - maestro_dur_loss: 0.0387 - val_loss: 0.1255 - val_f1_score_mod: 0.2255 - val_recall_mod: 0.1337 - val_precision_mod: 0.7365 - val_dur_error: 0.3334 - val_maestro_dur_loss: 0.0333\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11863\n",
      "50/50 - 134s - loss: 0.1349 - f1_score_mod: 0.2295 - recall_mod: 0.1370 - precision_mod: 0.7138 - dur_error: 0.3867 - maestro_dur_loss: 0.0387 - val_loss: 0.1190 - val_f1_score_mod: 0.2264 - val_recall_mod: 0.1341 - val_precision_mod: 0.7420 - val_dur_error: 0.2702 - val_maestro_dur_loss: 0.0270\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11863\n",
      "50/50 - 134s - loss: 0.1347 - f1_score_mod: 0.2314 - recall_mod: 0.1387 - precision_mod: 0.7073 - dur_error: 0.3879 - maestro_dur_loss: 0.0388 - val_loss: 0.1222 - val_f1_score_mod: 0.2367 - val_recall_mod: 0.1415 - val_precision_mod: 0.7334 - val_dur_error: 0.3074 - val_maestro_dur_loss: 0.0307\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.11863 to 0.11834, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 135s - loss: 0.1339 - f1_score_mod: 0.2415 - recall_mod: 0.1458 - precision_mod: 0.7091 - dur_error: 0.3835 - maestro_dur_loss: 0.0383 - val_loss: 0.1183 - val_f1_score_mod: 0.2318 - val_recall_mod: 0.1380 - val_precision_mod: 0.7387 - val_dur_error: 0.2681 - val_maestro_dur_loss: 0.0268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11834\n",
      "50/50 - 135s - loss: 0.1336 - f1_score_mod: 0.2436 - recall_mod: 0.1472 - precision_mod: 0.7160 - dur_error: 0.3858 - maestro_dur_loss: 0.0386 - val_loss: 0.1187 - val_f1_score_mod: 0.2415 - val_recall_mod: 0.1447 - val_precision_mod: 0.7431 - val_dur_error: 0.2744 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11834\n",
      "50/50 - 135s - loss: 0.1329 - f1_score_mod: 0.2429 - recall_mod: 0.1467 - precision_mod: 0.7109 - dur_error: 0.3800 - maestro_dur_loss: 0.0380 - val_loss: 0.1189 - val_f1_score_mod: 0.2501 - val_recall_mod: 0.1523 - val_precision_mod: 0.7129 - val_dur_error: 0.2768 - val_maestro_dur_loss: 0.0277\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11834\n",
      "50/50 - 133s - loss: 0.1326 - f1_score_mod: 0.2504 - recall_mod: 0.1519 - precision_mod: 0.7183 - dur_error: 0.3800 - maestro_dur_loss: 0.0380 - val_loss: 0.1204 - val_f1_score_mod: 0.2427 - val_recall_mod: 0.1456 - val_precision_mod: 0.7420 - val_dur_error: 0.2938 - val_maestro_dur_loss: 0.0294\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11834\n",
      "50/50 - 132s - loss: 0.1324 - f1_score_mod: 0.2531 - recall_mod: 0.1544 - precision_mod: 0.7111 - dur_error: 0.3811 - maestro_dur_loss: 0.0381 - val_loss: 0.1239 - val_f1_score_mod: 0.2489 - val_recall_mod: 0.1501 - val_precision_mod: 0.7403 - val_dur_error: 0.3338 - val_maestro_dur_loss: 0.0334\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.11834 to 0.11661, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 132s - loss: 0.1316 - f1_score_mod: 0.2551 - recall_mod: 0.1554 - precision_mod: 0.7194 - dur_error: 0.3766 - maestro_dur_loss: 0.0377 - val_loss: 0.1166 - val_f1_score_mod: 0.2460 - val_recall_mod: 0.1476 - val_precision_mod: 0.7522 - val_dur_error: 0.2613 - val_maestro_dur_loss: 0.0261\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11661\n",
      "50/50 - 133s - loss: 0.1311 - f1_score_mod: 0.2598 - recall_mod: 0.1587 - precision_mod: 0.7219 - dur_error: 0.3759 - maestro_dur_loss: 0.0376 - val_loss: 0.1191 - val_f1_score_mod: 0.2612 - val_recall_mod: 0.1597 - val_precision_mod: 0.7282 - val_dur_error: 0.2881 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.11661 to 0.11623, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1306 - f1_score_mod: 0.2628 - recall_mod: 0.1611 - precision_mod: 0.7192 - dur_error: 0.3743 - maestro_dur_loss: 0.0374 - val_loss: 0.1162 - val_f1_score_mod: 0.2598 - val_recall_mod: 0.1582 - val_precision_mod: 0.7393 - val_dur_error: 0.2599 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11623 to 0.11537, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 131s - loss: 0.1298 - f1_score_mod: 0.2652 - recall_mod: 0.1630 - precision_mod: 0.7209 - dur_error: 0.3710 - maestro_dur_loss: 0.0371 - val_loss: 0.1154 - val_f1_score_mod: 0.2607 - val_recall_mod: 0.1592 - val_precision_mod: 0.7328 - val_dur_error: 0.2564 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11537\n",
      "50/50 - 131s - loss: 0.1294 - f1_score_mod: 0.2687 - recall_mod: 0.1653 - precision_mod: 0.7225 - dur_error: 0.3697 - maestro_dur_loss: 0.0370 - val_loss: 0.1214 - val_f1_score_mod: 0.2776 - val_recall_mod: 0.1730 - val_precision_mod: 0.7135 - val_dur_error: 0.3151 - val_maestro_dur_loss: 0.0315\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.11537 to 0.11475, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 132s - loss: 0.1294 - f1_score_mod: 0.2734 - recall_mod: 0.1690 - precision_mod: 0.7225 - dur_error: 0.3725 - maestro_dur_loss: 0.0373 - val_loss: 0.1147 - val_f1_score_mod: 0.2762 - val_recall_mod: 0.1712 - val_precision_mod: 0.7263 - val_dur_error: 0.2536 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11475\n",
      "50/50 - 133s - loss: 0.1285 - f1_score_mod: 0.2746 - recall_mod: 0.1700 - precision_mod: 0.7198 - dur_error: 0.3682 - maestro_dur_loss: 0.0368 - val_loss: 0.1182 - val_f1_score_mod: 0.2799 - val_recall_mod: 0.1735 - val_precision_mod: 0.7322 - val_dur_error: 0.2884 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11475\n",
      "50/50 - 133s - loss: 0.1279 - f1_score_mod: 0.2809 - recall_mod: 0.1747 - precision_mod: 0.7245 - dur_error: 0.3660 - maestro_dur_loss: 0.0366 - val_loss: 0.1199 - val_f1_score_mod: 0.2732 - val_recall_mod: 0.1681 - val_precision_mod: 0.7429 - val_dur_error: 0.3036 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11475\n",
      "50/50 - 132s - loss: 0.1279 - f1_score_mod: 0.2850 - recall_mod: 0.1776 - precision_mod: 0.7261 - dur_error: 0.3689 - maestro_dur_loss: 0.0369 - val_loss: 0.1148 - val_f1_score_mod: 0.2755 - val_recall_mod: 0.1695 - val_precision_mod: 0.7518 - val_dur_error: 0.2602 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11475\n",
      "50/50 - 132s - loss: 0.1275 - f1_score_mod: 0.2901 - recall_mod: 0.1817 - precision_mod: 0.7269 - dur_error: 0.3663 - maestro_dur_loss: 0.0366 - val_loss: 0.1197 - val_f1_score_mod: 0.2849 - val_recall_mod: 0.1781 - val_precision_mod: 0.7253 - val_dur_error: 0.3102 - val_maestro_dur_loss: 0.0310\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11475\n",
      "50/50 - 132s - loss: 0.1265 - f1_score_mod: 0.2935 - recall_mod: 0.1843 - precision_mod: 0.7288 - dur_error: 0.3626 - maestro_dur_loss: 0.0363 - val_loss: 0.1223 - val_f1_score_mod: 0.2940 - val_recall_mod: 0.1857 - val_precision_mod: 0.7182 - val_dur_error: 0.3393 - val_maestro_dur_loss: 0.0339\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11475\n",
      "50/50 - 132s - loss: 0.1262 - f1_score_mod: 0.2946 - recall_mod: 0.1851 - precision_mod: 0.7287 - dur_error: 0.3613 - maestro_dur_loss: 0.0361 - val_loss: 0.1163 - val_f1_score_mod: 0.2898 - val_recall_mod: 0.1815 - val_precision_mod: 0.7312 - val_dur_error: 0.2819 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11475\n",
      "50/50 - 132s - loss: 0.1259 - f1_score_mod: 0.2993 - recall_mod: 0.1887 - precision_mod: 0.7295 - dur_error: 0.3611 - maestro_dur_loss: 0.0361 - val_loss: 0.1155 - val_f1_score_mod: 0.2917 - val_recall_mod: 0.1829 - val_precision_mod: 0.7323 - val_dur_error: 0.2733 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11475\n",
      "50/50 - 133s - loss: 0.1250 - f1_score_mod: 0.3021 - recall_mod: 0.1907 - precision_mod: 0.7332 - dur_error: 0.3585 - maestro_dur_loss: 0.0359 - val_loss: 0.1152 - val_f1_score_mod: 0.3060 - val_recall_mod: 0.1959 - val_precision_mod: 0.7100 - val_dur_error: 0.2703 - val_maestro_dur_loss: 0.0270\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.11475\n",
      "50/50 - 133s - loss: 0.1247 - f1_score_mod: 0.3057 - recall_mod: 0.1939 - precision_mod: 0.7282 - dur_error: 0.3576 - maestro_dur_loss: 0.0358 - val_loss: 0.1257 - val_f1_score_mod: 0.3077 - val_recall_mod: 0.1973 - val_precision_mod: 0.7135 - val_dur_error: 0.3780 - val_maestro_dur_loss: 0.0378\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.11475\n",
      "50/50 - 133s - loss: 0.1244 - f1_score_mod: 0.3106 - recall_mod: 0.1976 - precision_mod: 0.7301 - dur_error: 0.3582 - maestro_dur_loss: 0.0358 - val_loss: 0.1185 - val_f1_score_mod: 0.3030 - val_recall_mod: 0.1919 - val_precision_mod: 0.7352 - val_dur_error: 0.3121 - val_maestro_dur_loss: 0.0312\n",
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.11475\n",
      "50/50 - 134s - loss: 0.1237 - f1_score_mod: 0.3137 - recall_mod: 0.2000 - precision_mod: 0.7328 - dur_error: 0.3552 - maestro_dur_loss: 0.0355 - val_loss: 0.1184 - val_f1_score_mod: 0.3076 - val_recall_mod: 0.1959 - val_precision_mod: 0.7258 - val_dur_error: 0.3109 - val_maestro_dur_loss: 0.0311\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.11475 to 0.11387, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1237 - f1_score_mod: 0.3181 - recall_mod: 0.2034 - precision_mod: 0.7345 - dur_error: 0.3558 - maestro_dur_loss: 0.0356 - val_loss: 0.1139 - val_f1_score_mod: 0.3122 - val_recall_mod: 0.1991 - val_precision_mod: 0.7336 - val_dur_error: 0.2707 - val_maestro_dur_loss: 0.0271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.11387 to 0.11239, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1230 - f1_score_mod: 0.3215 - recall_mod: 0.2064 - precision_mod: 0.7331 - dur_error: 0.3552 - maestro_dur_loss: 0.0355 - val_loss: 0.1124 - val_f1_score_mod: 0.3097 - val_recall_mod: 0.1964 - val_precision_mod: 0.7419 - val_dur_error: 0.2546 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.11239\n",
      "50/50 - 135s - loss: 0.1223 - f1_score_mod: 0.3246 - recall_mod: 0.2088 - precision_mod: 0.7359 - dur_error: 0.3511 - maestro_dur_loss: 0.0351 - val_loss: 0.1189 - val_f1_score_mod: 0.3293 - val_recall_mod: 0.2145 - val_precision_mod: 0.7198 - val_dur_error: 0.3235 - val_maestro_dur_loss: 0.0323\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.11239\n",
      "50/50 - 133s - loss: 0.1224 - f1_score_mod: 0.3284 - recall_mod: 0.2113 - precision_mod: 0.7404 - dur_error: 0.3535 - maestro_dur_loss: 0.0354 - val_loss: 0.1131 - val_f1_score_mod: 0.3148 - val_recall_mod: 0.2022 - val_precision_mod: 0.7247 - val_dur_error: 0.2598 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.11239\n",
      "50/50 - 133s - loss: 0.1220 - f1_score_mod: 0.3282 - recall_mod: 0.2122 - precision_mod: 0.7288 - dur_error: 0.3541 - maestro_dur_loss: 0.0354 - val_loss: 0.1126 - val_f1_score_mod: 0.3148 - val_recall_mod: 0.2006 - val_precision_mod: 0.7459 - val_dur_error: 0.2641 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.11239 to 0.11016, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1213 - f1_score_mod: 0.3335 - recall_mod: 0.2154 - precision_mod: 0.7420 - dur_error: 0.3503 - maestro_dur_loss: 0.0350 - val_loss: 0.1102 - val_f1_score_mod: 0.3267 - val_recall_mod: 0.2105 - val_precision_mod: 0.7397 - val_dur_error: 0.2413 - val_maestro_dur_loss: 0.0241\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.11016\n",
      "50/50 - 133s - loss: 0.1205 - f1_score_mod: 0.3368 - recall_mod: 0.2184 - precision_mod: 0.7387 - dur_error: 0.3461 - maestro_dur_loss: 0.0346 - val_loss: 0.1220 - val_f1_score_mod: 0.3333 - val_recall_mod: 0.2185 - val_precision_mod: 0.7123 - val_dur_error: 0.3580 - val_maestro_dur_loss: 0.0358\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.11016\n",
      "50/50 - 133s - loss: 0.1207 - f1_score_mod: 0.3379 - recall_mod: 0.2197 - precision_mod: 0.7353 - dur_error: 0.3502 - maestro_dur_loss: 0.0350 - val_loss: 0.1111 - val_f1_score_mod: 0.3293 - val_recall_mod: 0.2137 - val_precision_mod: 0.7311 - val_dur_error: 0.2493 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.11016 to 0.10981, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.1199 - f1_score_mod: 0.3430 - recall_mod: 0.2237 - precision_mod: 0.7387 - dur_error: 0.3460 - maestro_dur_loss: 0.0346 - val_loss: 0.1098 - val_f1_score_mod: 0.3380 - val_recall_mod: 0.2227 - val_precision_mod: 0.7093 - val_dur_error: 0.2401 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.10981 to 0.10957, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 135s - loss: 0.1191 - f1_score_mod: 0.3461 - recall_mod: 0.2260 - precision_mod: 0.7408 - dur_error: 0.3421 - maestro_dur_loss: 0.0342 - val_loss: 0.1096 - val_f1_score_mod: 0.3282 - val_recall_mod: 0.2113 - val_precision_mod: 0.7494 - val_dur_error: 0.2432 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10957\n",
      "50/50 - 133s - loss: 0.1191 - f1_score_mod: 0.3472 - recall_mod: 0.2275 - precision_mod: 0.7353 - dur_error: 0.3445 - maestro_dur_loss: 0.0345 - val_loss: 0.1151 - val_f1_score_mod: 0.3306 - val_recall_mod: 0.2145 - val_precision_mod: 0.7351 - val_dur_error: 0.3001 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10957\n",
      "50/50 - 154s - loss: 0.1183 - f1_score_mod: 0.3532 - recall_mod: 0.2326 - precision_mod: 0.7363 - dur_error: 0.3407 - maestro_dur_loss: 0.0341 - val_loss: 0.1105 - val_f1_score_mod: 0.3461 - val_recall_mod: 0.2298 - val_precision_mod: 0.7088 - val_dur_error: 0.2513 - val_maestro_dur_loss: 0.0251\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.10957\n",
      "50/50 - 168s - loss: 0.1182 - f1_score_mod: 0.3562 - recall_mod: 0.2345 - precision_mod: 0.7442 - dur_error: 0.3430 - maestro_dur_loss: 0.0343 - val_loss: 0.1189 - val_f1_score_mod: 0.3404 - val_recall_mod: 0.2238 - val_precision_mod: 0.7206 - val_dur_error: 0.3395 - val_maestro_dur_loss: 0.0339\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10957\n",
      "50/50 - 171s - loss: 0.1179 - f1_score_mod: 0.3546 - recall_mod: 0.2337 - precision_mod: 0.7395 - dur_error: 0.3433 - maestro_dur_loss: 0.0343 - val_loss: 0.1127 - val_f1_score_mod: 0.3515 - val_recall_mod: 0.2347 - val_precision_mod: 0.7048 - val_dur_error: 0.2782 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.10957 to 0.10944, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 156s - loss: 0.1171 - f1_score_mod: 0.3622 - recall_mod: 0.2396 - precision_mod: 0.7450 - dur_error: 0.3390 - maestro_dur_loss: 0.0339 - val_loss: 0.1094 - val_f1_score_mod: 0.3508 - val_recall_mod: 0.2329 - val_precision_mod: 0.7196 - val_dur_error: 0.2461 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.10944 to 0.10842, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 159s - loss: 0.1168 - f1_score_mod: 0.3649 - recall_mod: 0.2426 - precision_mod: 0.7404 - dur_error: 0.3397 - maestro_dur_loss: 0.0340 - val_loss: 0.1084 - val_f1_score_mod: 0.3472 - val_recall_mod: 0.2289 - val_precision_mod: 0.7283 - val_dur_error: 0.2419 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10842\n",
      "50/50 - 163s - loss: 0.1162 - f1_score_mod: 0.3668 - recall_mod: 0.2442 - precision_mod: 0.7406 - dur_error: 0.3374 - maestro_dur_loss: 0.0337 - val_loss: 0.1087 - val_f1_score_mod: 0.3520 - val_recall_mod: 0.2333 - val_precision_mod: 0.7248 - val_dur_error: 0.2428 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10842\n",
      "50/50 - 161s - loss: 0.1156 - f1_score_mod: 0.3702 - recall_mod: 0.2464 - precision_mod: 0.7474 - dur_error: 0.3352 - maestro_dur_loss: 0.0335 - val_loss: 0.1093 - val_f1_score_mod: 0.3658 - val_recall_mod: 0.2504 - val_precision_mod: 0.6857 - val_dur_error: 0.2472 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10842\n",
      "50/50 - 168s - loss: 0.1150 - f1_score_mod: 0.3768 - recall_mod: 0.2528 - precision_mod: 0.7435 - dur_error: 0.3320 - maestro_dur_loss: 0.0332 - val_loss: 0.1119 - val_f1_score_mod: 0.3479 - val_recall_mod: 0.2285 - val_precision_mod: 0.7380 - val_dur_error: 0.2772 - val_maestro_dur_loss: 0.0277\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10842\n",
      "50/50 - 164s - loss: 0.1152 - f1_score_mod: 0.3726 - recall_mod: 0.2494 - precision_mod: 0.7409 - dur_error: 0.3355 - maestro_dur_loss: 0.0335 - val_loss: 0.1087 - val_f1_score_mod: 0.3591 - val_recall_mod: 0.2412 - val_precision_mod: 0.7136 - val_dur_error: 0.2435 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.10842 to 0.10785, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 165s - loss: 0.1146 - f1_score_mod: 0.3778 - recall_mod: 0.2539 - precision_mod: 0.7417 - dur_error: 0.3336 - maestro_dur_loss: 0.0334 - val_loss: 0.1078 - val_f1_score_mod: 0.3615 - val_recall_mod: 0.2420 - val_precision_mod: 0.7204 - val_dur_error: 0.2391 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10785\n",
      "50/50 - 143s - loss: 0.1140 - f1_score_mod: 0.3814 - recall_mod: 0.2569 - precision_mod: 0.7427 - dur_error: 0.3322 - maestro_dur_loss: 0.0332 - val_loss: 0.1082 - val_f1_score_mod: 0.3548 - val_recall_mod: 0.2349 - val_precision_mod: 0.7346 - val_dur_error: 0.2441 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10785\n",
      "50/50 - 141s - loss: 0.1134 - f1_score_mod: 0.3850 - recall_mod: 0.2599 - precision_mod: 0.7457 - dur_error: 0.3284 - maestro_dur_loss: 0.0328 - val_loss: 0.1158 - val_f1_score_mod: 0.3680 - val_recall_mod: 0.2503 - val_precision_mod: 0.7025 - val_dur_error: 0.3203 - val_maestro_dur_loss: 0.0320\n",
      "Epoch 87/150\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.10785 to 0.10774, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 157s - loss: 0.1132 - f1_score_mod: 0.3887 - recall_mod: 0.2634 - precision_mod: 0.7445 - dur_error: 0.3299 - maestro_dur_loss: 0.0330 - val_loss: 0.1077 - val_f1_score_mod: 0.3616 - val_recall_mod: 0.2406 - val_precision_mod: 0.7351 - val_dur_error: 0.2399 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 88/150\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10774\n",
      "50/50 - 144s - loss: 0.1127 - f1_score_mod: 0.3917 - recall_mod: 0.2660 - precision_mod: 0.7445 - dur_error: 0.3298 - maestro_dur_loss: 0.0330 - val_loss: 0.1095 - val_f1_score_mod: 0.3746 - val_recall_mod: 0.2560 - val_precision_mod: 0.7054 - val_dur_error: 0.2598 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 89/150\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.10774 to 0.10629, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 146s - loss: 0.1124 - f1_score_mod: 0.3953 - recall_mod: 0.2693 - precision_mod: 0.7452 - dur_error: 0.3296 - maestro_dur_loss: 0.0330 - val_loss: 0.1063 - val_f1_score_mod: 0.3688 - val_recall_mod: 0.2468 - val_precision_mod: 0.7357 - val_dur_error: 0.2284 - val_maestro_dur_loss: 0.0228\n",
      "Epoch 90/150\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10629\n",
      "50/50 - 172s - loss: 0.1117 - f1_score_mod: 0.3972 - recall_mod: 0.2710 - precision_mod: 0.7469 - dur_error: 0.3263 - maestro_dur_loss: 0.0326 - val_loss: 0.1107 - val_f1_score_mod: 0.3759 - val_recall_mod: 0.2554 - val_precision_mod: 0.7205 - val_dur_error: 0.2771 - val_maestro_dur_loss: 0.0277\n",
      "Epoch 91/150\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10629\n",
      "50/50 - 157s - loss: 0.1115 - f1_score_mod: 0.4022 - recall_mod: 0.2756 - precision_mod: 0.7468 - dur_error: 0.3265 - maestro_dur_loss: 0.0326 - val_loss: 0.1144 - val_f1_score_mod: 0.3801 - val_recall_mod: 0.2612 - val_precision_mod: 0.7054 - val_dur_error: 0.3105 - val_maestro_dur_loss: 0.0311\n",
      "Epoch 92/150\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10629\n",
      "50/50 - 151s - loss: 0.1109 - f1_score_mod: 0.4053 - recall_mod: 0.2790 - precision_mod: 0.7439 - dur_error: 0.3270 - maestro_dur_loss: 0.0327 - val_loss: 0.1100 - val_f1_score_mod: 0.3761 - val_recall_mod: 0.2575 - val_precision_mod: 0.7049 - val_dur_error: 0.2705 - val_maestro_dur_loss: 0.0271\n",
      "Epoch 93/150\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.10629\n",
      "50/50 - 153s - loss: 0.1105 - f1_score_mod: 0.4050 - recall_mod: 0.2781 - precision_mod: 0.7475 - dur_error: 0.3246 - maestro_dur_loss: 0.0325 - val_loss: 0.1079 - val_f1_score_mod: 0.3827 - val_recall_mod: 0.2634 - val_precision_mod: 0.7098 - val_dur_error: 0.2490 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 94/150\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.10629 to 0.10569, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 148s - loss: 0.1100 - f1_score_mod: 0.4128 - recall_mod: 0.2853 - precision_mod: 0.7483 - dur_error: 0.3245 - maestro_dur_loss: 0.0325 - val_loss: 0.1057 - val_f1_score_mod: 0.3899 - val_recall_mod: 0.2690 - val_precision_mod: 0.7139 - val_dur_error: 0.2296 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 95/150\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10569\n",
      "50/50 - 158s - loss: 0.1095 - f1_score_mod: 0.4134 - recall_mod: 0.2864 - precision_mod: 0.7452 - dur_error: 0.3220 - maestro_dur_loss: 0.0322 - val_loss: 0.1063 - val_f1_score_mod: 0.3899 - val_recall_mod: 0.2697 - val_precision_mod: 0.7106 - val_dur_error: 0.2338 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 96/150\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.10569\n",
      "50/50 - 151s - loss: 0.1092 - f1_score_mod: 0.4162 - recall_mod: 0.2885 - precision_mod: 0.7491 - dur_error: 0.3230 - maestro_dur_loss: 0.0323 - val_loss: 0.1140 - val_f1_score_mod: 0.3914 - val_recall_mod: 0.2724 - val_precision_mod: 0.7010 - val_dur_error: 0.3140 - val_maestro_dur_loss: 0.0314\n",
      "Epoch 97/150\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.10569\n",
      "50/50 - 143s - loss: 0.1086 - f1_score_mod: 0.4196 - recall_mod: 0.2921 - precision_mod: 0.7467 - dur_error: 0.3202 - maestro_dur_loss: 0.0320 - val_loss: 0.1100 - val_f1_score_mod: 0.3912 - val_recall_mod: 0.2707 - val_precision_mod: 0.7113 - val_dur_error: 0.2780 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 98/150\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.10569\n",
      "50/50 - 134s - loss: 0.1079 - f1_score_mod: 0.4244 - recall_mod: 0.2962 - precision_mod: 0.7515 - dur_error: 0.3186 - maestro_dur_loss: 0.0319 - val_loss: 0.1142 - val_f1_score_mod: 0.3954 - val_recall_mod: 0.2765 - val_precision_mod: 0.7003 - val_dur_error: 0.3188 - val_maestro_dur_loss: 0.0319\n",
      "Epoch 99/150\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.10569\n",
      "50/50 - 153s - loss: 0.1075 - f1_score_mod: 0.4259 - recall_mod: 0.2986 - precision_mod: 0.7437 - dur_error: 0.3157 - maestro_dur_loss: 0.0316 - val_loss: 0.1064 - val_f1_score_mod: 0.3861 - val_recall_mod: 0.2633 - val_precision_mod: 0.7315 - val_dur_error: 0.2422 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 100/150\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.10569\n",
      "50/50 - 161s - loss: 0.1068 - f1_score_mod: 0.4296 - recall_mod: 0.3012 - precision_mod: 0.7512 - dur_error: 0.3137 - maestro_dur_loss: 0.0314 - val_loss: 0.1062 - val_f1_score_mod: 0.3974 - val_recall_mod: 0.2782 - val_precision_mod: 0.7002 - val_dur_error: 0.2377 - val_maestro_dur_loss: 0.0238\n",
      "Epoch 101/150\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.10569\n",
      "50/50 - 144s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model yet, barely. But it was also still improving when it failed. I have a hunch that decreasing the dropout_rate may help to combat the exploding gradients, let's try with 0.3 instead of 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18630, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.2927 - f1_score_mod: 0.0137 - recall_mod: 0.0257 - precision_mod: 0.0589 - dur_error: 1.0222 - maestro_dur_loss: 0.1022 - val_loss: 0.1863 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6280 - val_maestro_dur_loss: 0.0628\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.18630\n",
      "50/50 - 129s - loss: 0.2015 - f1_score_mod: 4.4372e-04 - recall_mod: 2.2348e-04 - precision_mod: 0.0989 - dur_error: 0.6733 - maestro_dur_loss: 0.0673 - val_loss: 0.1869 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6530 - val_maestro_dur_loss: 0.0653\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.18630 to 0.17602, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1872 - f1_score_mod: 0.0024 - recall_mod: 0.0012 - precision_mod: 0.3048 - dur_error: 0.6025 - maestro_dur_loss: 0.0603 - val_loss: 0.1760 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5993 - val_maestro_dur_loss: 0.0599\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17602 to 0.16009, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1783 - f1_score_mod: 0.0121 - recall_mod: 0.0062 - precision_mod: 0.4891 - dur_error: 0.5645 - maestro_dur_loss: 0.0565 - val_loss: 0.1601 - val_f1_score_mod: 0.0029 - val_recall_mod: 0.0014 - val_precision_mod: 0.8356 - val_dur_error: 0.4419 - val_maestro_dur_loss: 0.0442\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16009\n",
      "50/50 - 128s - loss: 0.1726 - f1_score_mod: 0.0261 - recall_mod: 0.0134 - precision_mod: 0.5513 - dur_error: 0.5407 - maestro_dur_loss: 0.0541 - val_loss: 0.1711 - val_f1_score_mod: 0.0294 - val_recall_mod: 0.0150 - val_precision_mod: 0.7119 - val_dur_error: 0.6060 - val_maestro_dur_loss: 0.0606\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16009 to 0.15840, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1679 - f1_score_mod: 0.0438 - recall_mod: 0.0228 - precision_mod: 0.5931 - dur_error: 0.5225 - maestro_dur_loss: 0.0522 - val_loss: 0.1584 - val_f1_score_mod: 0.0574 - val_recall_mod: 0.0300 - val_precision_mod: 0.6929 - val_dur_error: 0.5011 - val_maestro_dur_loss: 0.0501\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15840 to 0.15250, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1642 - f1_score_mod: 0.0613 - recall_mod: 0.0324 - precision_mod: 0.6108 - dur_error: 0.5084 - maestro_dur_loss: 0.0508 - val_loss: 0.1525 - val_f1_score_mod: 0.0830 - val_recall_mod: 0.0445 - val_precision_mod: 0.6330 - val_dur_error: 0.4558 - val_maestro_dur_loss: 0.0456\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15250\n",
      "50/50 - 128s - loss: 0.1617 - f1_score_mod: 0.0775 - recall_mod: 0.0414 - precision_mod: 0.6390 - dur_error: 0.5007 - maestro_dur_loss: 0.0501 - val_loss: 0.1588 - val_f1_score_mod: 0.0962 - val_recall_mod: 0.0519 - val_precision_mod: 0.6811 - val_dur_error: 0.5291 - val_maestro_dur_loss: 0.0529\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.15250 to 0.14272, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 131s - loss: 0.1585 - f1_score_mod: 0.0929 - recall_mod: 0.0502 - precision_mod: 0.6497 - dur_error: 0.4834 - maestro_dur_loss: 0.0483 - val_loss: 0.1427 - val_f1_score_mod: 0.1075 - val_recall_mod: 0.0583 - val_precision_mod: 0.7096 - val_dur_error: 0.3930 - val_maestro_dur_loss: 0.0393\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14272\n",
      "50/50 - 129s - loss: 0.1563 - f1_score_mod: 0.1036 - recall_mod: 0.0565 - precision_mod: 0.6576 - dur_error: 0.4751 - maestro_dur_loss: 0.0475 - val_loss: 0.1465 - val_f1_score_mod: 0.1044 - val_recall_mod: 0.0564 - val_precision_mod: 0.7307 - val_dur_error: 0.4328 - val_maestro_dur_loss: 0.0433\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.14272 to 0.13379, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1536 - f1_score_mod: 0.1179 - recall_mod: 0.0650 - precision_mod: 0.6591 - dur_error: 0.4598 - maestro_dur_loss: 0.0460 - val_loss: 0.1338 - val_f1_score_mod: 0.1169 - val_recall_mod: 0.0637 - val_precision_mod: 0.7257 - val_dur_error: 0.3212 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13379\n",
      "50/50 - 129s - loss: 0.1516 - f1_score_mod: 0.1269 - recall_mod: 0.0705 - precision_mod: 0.6619 - dur_error: 0.4501 - maestro_dur_loss: 0.0450 - val_loss: 0.1338 - val_f1_score_mod: 0.1187 - val_recall_mod: 0.0649 - val_precision_mod: 0.7211 - val_dur_error: 0.3213 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.13379 to 0.13170, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1495 - f1_score_mod: 0.1387 - recall_mod: 0.0777 - precision_mod: 0.6692 - dur_error: 0.4368 - maestro_dur_loss: 0.0437 - val_loss: 0.1317 - val_f1_score_mod: 0.1409 - val_recall_mod: 0.0784 - val_precision_mod: 0.7108 - val_dur_error: 0.3127 - val_maestro_dur_loss: 0.0313\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13170\n",
      "50/50 - 128s - loss: 0.1481 - f1_score_mod: 0.1442 - recall_mod: 0.0810 - precision_mod: 0.6710 - dur_error: 0.4320 - maestro_dur_loss: 0.0432 - val_loss: 0.1389 - val_f1_score_mod: 0.1390 - val_recall_mod: 0.0769 - val_precision_mod: 0.7502 - val_dur_error: 0.3907 - val_maestro_dur_loss: 0.0391\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13170\n",
      "50/50 - 129s - loss: 0.1467 - f1_score_mod: 0.1536 - recall_mod: 0.0869 - precision_mod: 0.6719 - dur_error: 0.4262 - maestro_dur_loss: 0.0426 - val_loss: 0.1429 - val_f1_score_mod: 0.1627 - val_recall_mod: 0.0923 - val_precision_mod: 0.6986 - val_dur_error: 0.4340 - val_maestro_dur_loss: 0.0434\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13170\n",
      "50/50 - 130s - loss: 0.1454 - f1_score_mod: 0.1620 - recall_mod: 0.0925 - precision_mod: 0.6715 - dur_error: 0.4202 - maestro_dur_loss: 0.0420 - val_loss: 0.1327 - val_f1_score_mod: 0.1838 - val_recall_mod: 0.1071 - val_precision_mod: 0.6572 - val_dur_error: 0.3308 - val_maestro_dur_loss: 0.0331\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.13170 to 0.12903, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1439 - f1_score_mod: 0.1705 - recall_mod: 0.0978 - precision_mod: 0.6775 - dur_error: 0.4121 - maestro_dur_loss: 0.0412 - val_loss: 0.1290 - val_f1_score_mod: 0.1501 - val_recall_mod: 0.0835 - val_precision_mod: 0.7669 - val_dur_error: 0.3103 - val_maestro_dur_loss: 0.0310\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.12903 to 0.12758, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1436 - f1_score_mod: 0.1771 - recall_mod: 0.1022 - precision_mod: 0.6786 - dur_error: 0.4142 - maestro_dur_loss: 0.0414 - val_loss: 0.1276 - val_f1_score_mod: 0.1632 - val_recall_mod: 0.0920 - val_precision_mod: 0.7389 - val_dur_error: 0.3037 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12758\n",
      "50/50 - 129s - loss: 0.1419 - f1_score_mod: 0.1820 - recall_mod: 0.1051 - precision_mod: 0.6907 - dur_error: 0.4062 - maestro_dur_loss: 0.0406 - val_loss: 0.1292 - val_f1_score_mod: 0.1742 - val_recall_mod: 0.0990 - val_precision_mod: 0.7412 - val_dur_error: 0.3273 - val_maestro_dur_loss: 0.0327\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12758\n",
      "50/50 - 129s - loss: 0.1407 - f1_score_mod: 0.1899 - recall_mod: 0.1104 - precision_mod: 0.6899 - dur_error: 0.4003 - maestro_dur_loss: 0.0400 - val_loss: 0.1314 - val_f1_score_mod: 0.1729 - val_recall_mod: 0.0981 - val_precision_mod: 0.7509 - val_dur_error: 0.3505 - val_maestro_dur_loss: 0.0351\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.12758 to 0.12492, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1406 - f1_score_mod: 0.1961 - recall_mod: 0.1145 - precision_mod: 0.6918 - dur_error: 0.4043 - maestro_dur_loss: 0.0404 - val_loss: 0.1249 - val_f1_score_mod: 0.1850 - val_recall_mod: 0.1066 - val_precision_mod: 0.7119 - val_dur_error: 0.2905 - val_maestro_dur_loss: 0.0291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.12492 to 0.12435, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1394 - f1_score_mod: 0.2011 - recall_mod: 0.1179 - precision_mod: 0.6928 - dur_error: 0.3982 - maestro_dur_loss: 0.0398 - val_loss: 0.1244 - val_f1_score_mod: 0.1815 - val_recall_mod: 0.1033 - val_precision_mod: 0.7589 - val_dur_error: 0.2925 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.12435 to 0.12389, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1384 - f1_score_mod: 0.2062 - recall_mod: 0.1212 - precision_mod: 0.6996 - dur_error: 0.3943 - maestro_dur_loss: 0.0394 - val_loss: 0.1239 - val_f1_score_mod: 0.2050 - val_recall_mod: 0.1202 - val_precision_mod: 0.7077 - val_dur_error: 0.2882 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12389\n",
      "50/50 - 129s - loss: 0.1376 - f1_score_mod: 0.2121 - recall_mod: 0.1255 - precision_mod: 0.6964 - dur_error: 0.3908 - maestro_dur_loss: 0.0391 - val_loss: 0.1394 - val_f1_score_mod: 0.2202 - val_recall_mod: 0.1313 - val_precision_mod: 0.6994 - val_dur_error: 0.4450 - val_maestro_dur_loss: 0.0445\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12389 to 0.12308, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1370 - f1_score_mod: 0.2198 - recall_mod: 0.1307 - precision_mod: 0.7000 - dur_error: 0.3900 - maestro_dur_loss: 0.0390 - val_loss: 0.1231 - val_f1_score_mod: 0.2063 - val_recall_mod: 0.1204 - val_precision_mod: 0.7387 - val_dur_error: 0.2937 - val_maestro_dur_loss: 0.0294\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.12308 to 0.12201, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1358 - f1_score_mod: 0.2219 - recall_mod: 0.1320 - precision_mod: 0.7076 - dur_error: 0.3835 - maestro_dur_loss: 0.0383 - val_loss: 0.1220 - val_f1_score_mod: 0.2159 - val_recall_mod: 0.1272 - val_precision_mod: 0.7279 - val_dur_error: 0.2828 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12201\n",
      "50/50 - 129s - loss: 0.1351 - f1_score_mod: 0.2282 - recall_mod: 0.1364 - precision_mod: 0.7126 - dur_error: 0.3818 - maestro_dur_loss: 0.0382 - val_loss: 0.1221 - val_f1_score_mod: 0.2178 - val_recall_mod: 0.1284 - val_precision_mod: 0.7304 - val_dur_error: 0.2909 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12201\n",
      "50/50 - 128s - loss: 0.1342 - f1_score_mod: 0.2336 - recall_mod: 0.1399 - precision_mod: 0.7147 - dur_error: 0.3778 - maestro_dur_loss: 0.0378 - val_loss: 0.1275 - val_f1_score_mod: 0.2308 - val_recall_mod: 0.1382 - val_precision_mod: 0.7187 - val_dur_error: 0.3468 - val_maestro_dur_loss: 0.0347\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12201\n",
      "50/50 - 128s - loss: 0.1339 - f1_score_mod: 0.2395 - recall_mod: 0.1442 - precision_mod: 0.7120 - dur_error: 0.3801 - maestro_dur_loss: 0.0380 - val_loss: 0.1229 - val_f1_score_mod: 0.2315 - val_recall_mod: 0.1384 - val_precision_mod: 0.7234 - val_dur_error: 0.3042 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12201 to 0.11981, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1329 - f1_score_mod: 0.2406 - recall_mod: 0.1449 - precision_mod: 0.7164 - dur_error: 0.3743 - maestro_dur_loss: 0.0374 - val_loss: 0.1198 - val_f1_score_mod: 0.2418 - val_recall_mod: 0.1459 - val_precision_mod: 0.7205 - val_dur_error: 0.2758 - val_maestro_dur_loss: 0.0276\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11981\n",
      "50/50 - 129s - loss: 0.1327 - f1_score_mod: 0.2478 - recall_mod: 0.1504 - precision_mod: 0.7098 - dur_error: 0.3759 - maestro_dur_loss: 0.0376 - val_loss: 0.1207 - val_f1_score_mod: 0.2416 - val_recall_mod: 0.1458 - val_precision_mod: 0.7140 - val_dur_error: 0.2860 - val_maestro_dur_loss: 0.0286\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11981\n",
      "50/50 - 129s - loss: 0.1312 - f1_score_mod: 0.2533 - recall_mod: 0.1544 - precision_mod: 0.7121 - dur_error: 0.3684 - maestro_dur_loss: 0.0368 - val_loss: 0.1235 - val_f1_score_mod: 0.2401 - val_recall_mod: 0.1436 - val_precision_mod: 0.7413 - val_dur_error: 0.3225 - val_maestro_dur_loss: 0.0322\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11981\n",
      "50/50 - 129s - loss: 0.1311 - f1_score_mod: 0.2587 - recall_mod: 0.1580 - precision_mod: 0.7229 - dur_error: 0.3706 - maestro_dur_loss: 0.0371 - val_loss: 0.1213 - val_f1_score_mod: 0.2554 - val_recall_mod: 0.1561 - val_precision_mod: 0.7119 - val_dur_error: 0.3013 - val_maestro_dur_loss: 0.0301\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11981 to 0.11900, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1304 - f1_score_mod: 0.2609 - recall_mod: 0.1596 - precision_mod: 0.7217 - dur_error: 0.3679 - maestro_dur_loss: 0.0368 - val_loss: 0.1190 - val_f1_score_mod: 0.2543 - val_recall_mod: 0.1546 - val_precision_mod: 0.7241 - val_dur_error: 0.2810 - val_maestro_dur_loss: 0.0281\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.11900 to 0.11668, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1298 - f1_score_mod: 0.2695 - recall_mod: 0.1657 - precision_mod: 0.7297 - dur_error: 0.3673 - maestro_dur_loss: 0.0367 - val_loss: 0.1167 - val_f1_score_mod: 0.2618 - val_recall_mod: 0.1607 - val_precision_mod: 0.7281 - val_dur_error: 0.2614 - val_maestro_dur_loss: 0.0261\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11668\n",
      "50/50 - 131s - loss: 0.1291 - f1_score_mod: 0.2679 - recall_mod: 0.1650 - precision_mod: 0.7219 - dur_error: 0.3632 - maestro_dur_loss: 0.0363 - val_loss: 0.1232 - val_f1_score_mod: 0.2677 - val_recall_mod: 0.1647 - val_precision_mod: 0.7288 - val_dur_error: 0.3302 - val_maestro_dur_loss: 0.0330\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11668\n",
      "50/50 - 128s - loss: 0.1286 - f1_score_mod: 0.2756 - recall_mod: 0.1705 - precision_mod: 0.7264 - dur_error: 0.3641 - maestro_dur_loss: 0.0364 - val_loss: 0.1192 - val_f1_score_mod: 0.2483 - val_recall_mod: 0.1493 - val_precision_mod: 0.7521 - val_dur_error: 0.2901 - val_maestro_dur_loss: 0.0290\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.11668 to 0.11496, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1280 - f1_score_mod: 0.2820 - recall_mod: 0.1753 - precision_mod: 0.7255 - dur_error: 0.3629 - maestro_dur_loss: 0.0363 - val_loss: 0.1150 - val_f1_score_mod: 0.2721 - val_recall_mod: 0.1681 - val_precision_mod: 0.7294 - val_dur_error: 0.2539 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11496\n",
      "50/50 - 129s - loss: 0.1270 - f1_score_mod: 0.2851 - recall_mod: 0.1776 - precision_mod: 0.7290 - dur_error: 0.3570 - maestro_dur_loss: 0.0357 - val_loss: 0.1163 - val_f1_score_mod: 0.2836 - val_recall_mod: 0.1786 - val_precision_mod: 0.6997 - val_dur_error: 0.2648 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11496\n",
      "50/50 - 129s - loss: 0.1268 - f1_score_mod: 0.2907 - recall_mod: 0.1819 - precision_mod: 0.7302 - dur_error: 0.3580 - maestro_dur_loss: 0.0358 - val_loss: 0.1190 - val_f1_score_mod: 0.2681 - val_recall_mod: 0.1643 - val_precision_mod: 0.7446 - val_dur_error: 0.2939 - val_maestro_dur_loss: 0.0294\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11496\n",
      "50/50 - 131s - loss: 0.1260 - f1_score_mod: 0.2943 - recall_mod: 0.1846 - precision_mod: 0.7318 - dur_error: 0.3554 - maestro_dur_loss: 0.0355 - val_loss: 0.1157 - val_f1_score_mod: 0.2816 - val_recall_mod: 0.1750 - val_precision_mod: 0.7323 - val_dur_error: 0.2668 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11496\n",
      "50/50 - 130s - loss: 0.1259 - f1_score_mod: 0.2966 - recall_mod: 0.1861 - precision_mod: 0.7351 - dur_error: 0.3587 - maestro_dur_loss: 0.0359 - val_loss: 0.1212 - val_f1_score_mod: 0.2787 - val_recall_mod: 0.1726 - val_precision_mod: 0.7345 - val_dur_error: 0.3253 - val_maestro_dur_loss: 0.0325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11496\n",
      "50/50 - 130s - loss: 0.1248 - f1_score_mod: 0.3028 - recall_mod: 0.1910 - precision_mod: 0.7367 - dur_error: 0.3504 - maestro_dur_loss: 0.0350 - val_loss: 0.1173 - val_f1_score_mod: 0.2851 - val_recall_mod: 0.1781 - val_precision_mod: 0.7278 - val_dur_error: 0.2895 - val_maestro_dur_loss: 0.0289\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11496\n",
      "50/50 - 135s - loss: 0.1247 - f1_score_mod: 0.3070 - recall_mod: 0.1943 - precision_mod: 0.7346 - dur_error: 0.3540 - maestro_dur_loss: 0.0354 - val_loss: 0.1170 - val_f1_score_mod: 0.2871 - val_recall_mod: 0.1794 - val_precision_mod: 0.7316 - val_dur_error: 0.2861 - val_maestro_dur_loss: 0.0286\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11496\n",
      "50/50 - 130s - loss: 0.1239 - f1_score_mod: 0.3092 - recall_mod: 0.1958 - precision_mod: 0.7410 - dur_error: 0.3493 - maestro_dur_loss: 0.0349 - val_loss: 0.1207 - val_f1_score_mod: 0.2951 - val_recall_mod: 0.1860 - val_precision_mod: 0.7242 - val_dur_error: 0.3262 - val_maestro_dur_loss: 0.0326\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11496\n",
      "50/50 - 129s - loss: 0.1231 - f1_score_mod: 0.3142 - recall_mod: 0.1998 - precision_mod: 0.7379 - dur_error: 0.3470 - maestro_dur_loss: 0.0347 - val_loss: 0.1181 - val_f1_score_mod: 0.3085 - val_recall_mod: 0.1981 - val_precision_mod: 0.7147 - val_dur_error: 0.3020 - val_maestro_dur_loss: 0.0302\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11496\n",
      "50/50 - 129s - loss: 0.1224 - f1_score_mod: 0.3190 - recall_mod: 0.2037 - precision_mod: 0.7430 - dur_error: 0.3450 - maestro_dur_loss: 0.0345 - val_loss: 0.1195 - val_f1_score_mod: 0.2996 - val_recall_mod: 0.1881 - val_precision_mod: 0.7498 - val_dur_error: 0.3241 - val_maestro_dur_loss: 0.0324\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11496\n",
      "50/50 - 128s - loss: 0.1224 - f1_score_mod: 0.3216 - recall_mod: 0.2055 - precision_mod: 0.7459 - dur_error: 0.3478 - maestro_dur_loss: 0.0348 - val_loss: 0.1226 - val_f1_score_mod: 0.3136 - val_recall_mod: 0.2018 - val_precision_mod: 0.7111 - val_dur_error: 0.3513 - val_maestro_dur_loss: 0.0351\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.11496 to 0.11278, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1220 - f1_score_mod: 0.3239 - recall_mod: 0.2077 - precision_mod: 0.7385 - dur_error: 0.3470 - maestro_dur_loss: 0.0347 - val_loss: 0.1128 - val_f1_score_mod: 0.3156 - val_recall_mod: 0.2030 - val_precision_mod: 0.7197 - val_dur_error: 0.2597 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11278\n",
      "50/50 - 128s - loss: 0.1214 - f1_score_mod: 0.3316 - recall_mod: 0.2139 - precision_mod: 0.7419 - dur_error: 0.3465 - maestro_dur_loss: 0.0346 - val_loss: 0.1131 - val_f1_score_mod: 0.3107 - val_recall_mod: 0.1974 - val_precision_mod: 0.7406 - val_dur_error: 0.2634 - val_maestro_dur_loss: 0.0263\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11278\n",
      "50/50 - 130s - loss: 0.1202 - f1_score_mod: 0.3354 - recall_mod: 0.2167 - precision_mod: 0.7463 - dur_error: 0.3403 - maestro_dur_loss: 0.0340 - val_loss: 0.1130 - val_f1_score_mod: 0.3088 - val_recall_mod: 0.1953 - val_precision_mod: 0.7497 - val_dur_error: 0.2645 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.11278 to 0.11235, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1198 - f1_score_mod: 0.3404 - recall_mod: 0.2208 - precision_mod: 0.7477 - dur_error: 0.3399 - maestro_dur_loss: 0.0340 - val_loss: 0.1123 - val_f1_score_mod: 0.3183 - val_recall_mod: 0.2041 - val_precision_mod: 0.7325 - val_dur_error: 0.2612 - val_maestro_dur_loss: 0.0261\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.11235 to 0.11204, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1193 - f1_score_mod: 0.3458 - recall_mod: 0.2251 - precision_mod: 0.7500 - dur_error: 0.3391 - maestro_dur_loss: 0.0339 - val_loss: 0.1120 - val_f1_score_mod: 0.3218 - val_recall_mod: 0.2066 - val_precision_mod: 0.7396 - val_dur_error: 0.2591 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.11204 to 0.11129, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1193 - f1_score_mod: 0.3461 - recall_mod: 0.2260 - precision_mod: 0.7433 - dur_error: 0.3407 - maestro_dur_loss: 0.0341 - val_loss: 0.1113 - val_f1_score_mod: 0.3123 - val_recall_mod: 0.1977 - val_precision_mod: 0.7543 - val_dur_error: 0.2536 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.11129 to 0.11086, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1183 - f1_score_mod: 0.3535 - recall_mod: 0.2316 - precision_mod: 0.7490 - dur_error: 0.3372 - maestro_dur_loss: 0.0337 - val_loss: 0.1109 - val_f1_score_mod: 0.3278 - val_recall_mod: 0.2109 - val_precision_mod: 0.7437 - val_dur_error: 0.2532 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.11086 to 0.11049, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1178 - f1_score_mod: 0.3579 - recall_mod: 0.2353 - precision_mod: 0.7510 - dur_error: 0.3369 - maestro_dur_loss: 0.0337 - val_loss: 0.1105 - val_f1_score_mod: 0.3294 - val_recall_mod: 0.2138 - val_precision_mod: 0.7300 - val_dur_error: 0.2490 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.11049 to 0.11014, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1173 - f1_score_mod: 0.3627 - recall_mod: 0.2393 - precision_mod: 0.7540 - dur_error: 0.3355 - maestro_dur_loss: 0.0335 - val_loss: 0.1101 - val_f1_score_mod: 0.3281 - val_recall_mod: 0.2110 - val_precision_mod: 0.7491 - val_dur_error: 0.2496 - val_maestro_dur_loss: 0.0250\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11014\n",
      "50/50 - 130s - loss: 0.1166 - f1_score_mod: 0.3660 - recall_mod: 0.2424 - precision_mod: 0.7510 - dur_error: 0.3321 - maestro_dur_loss: 0.0332 - val_loss: 0.1124 - val_f1_score_mod: 0.3428 - val_recall_mod: 0.2261 - val_precision_mod: 0.7187 - val_dur_error: 0.2728 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11014\n",
      "50/50 - 128s - loss: 0.1163 - f1_score_mod: 0.3685 - recall_mod: 0.2446 - precision_mod: 0.7503 - dur_error: 0.3330 - maestro_dur_loss: 0.0333 - val_loss: 0.1180 - val_f1_score_mod: 0.3396 - val_recall_mod: 0.2237 - val_precision_mod: 0.7164 - val_dur_error: 0.3285 - val_maestro_dur_loss: 0.0328\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.11014 to 0.10928, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1158 - f1_score_mod: 0.3729 - recall_mod: 0.2481 - precision_mod: 0.7538 - dur_error: 0.3319 - maestro_dur_loss: 0.0332 - val_loss: 0.1093 - val_f1_score_mod: 0.3457 - val_recall_mod: 0.2277 - val_precision_mod: 0.7259 - val_dur_error: 0.2466 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.10928 to 0.10924, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1152 - f1_score_mod: 0.3774 - recall_mod: 0.2519 - precision_mod: 0.7562 - dur_error: 0.3303 - maestro_dur_loss: 0.0330 - val_loss: 0.1092 - val_f1_score_mod: 0.3568 - val_recall_mod: 0.2383 - val_precision_mod: 0.7154 - val_dur_error: 0.2490 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.10924\n",
      "50/50 - 129s - loss: 0.1146 - f1_score_mod: 0.3821 - recall_mod: 0.2567 - precision_mod: 0.7507 - dur_error: 0.3284 - maestro_dur_loss: 0.0328 - val_loss: 0.1172 - val_f1_score_mod: 0.3504 - val_recall_mod: 0.2311 - val_precision_mod: 0.7312 - val_dur_error: 0.3304 - val_maestro_dur_loss: 0.0330\n",
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.10924 to 0.10787, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1142 - f1_score_mod: 0.3818 - recall_mod: 0.2559 - precision_mod: 0.7551 - dur_error: 0.3277 - maestro_dur_loss: 0.0328 - val_loss: 0.1079 - val_f1_score_mod: 0.3534 - val_recall_mod: 0.2341 - val_precision_mod: 0.7304 - val_dur_error: 0.2403 - val_maestro_dur_loss: 0.0240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.10787\n",
      "50/50 - 129s - loss: 0.1133 - f1_score_mod: 0.3870 - recall_mod: 0.2603 - precision_mod: 0.7557 - dur_error: 0.3236 - maestro_dur_loss: 0.0324 - val_loss: 0.1095 - val_f1_score_mod: 0.3572 - val_recall_mod: 0.2394 - val_precision_mod: 0.7143 - val_dur_error: 0.2503 - val_maestro_dur_loss: 0.0250\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10787\n",
      "50/50 - 130s - loss: 0.1132 - f1_score_mod: 0.3899 - recall_mod: 0.2631 - precision_mod: 0.7550 - dur_error: 0.3267 - maestro_dur_loss: 0.0327 - val_loss: 0.1133 - val_f1_score_mod: 0.3596 - val_recall_mod: 0.2403 - val_precision_mod: 0.7218 - val_dur_error: 0.2970 - val_maestro_dur_loss: 0.0297\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.10787\n",
      "50/50 - 128s - loss: 0.1124 - f1_score_mod: 0.3945 - recall_mod: 0.2674 - precision_mod: 0.7547 - dur_error: 0.3226 - maestro_dur_loss: 0.0323 - val_loss: 0.1136 - val_f1_score_mod: 0.3658 - val_recall_mod: 0.2469 - val_precision_mod: 0.7135 - val_dur_error: 0.3000 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.10787\n",
      "50/50 - 128s - loss: 0.1118 - f1_score_mod: 0.3984 - recall_mod: 0.2707 - precision_mod: 0.7569 - dur_error: 0.3204 - maestro_dur_loss: 0.0320 - val_loss: 0.1180 - val_f1_score_mod: 0.3696 - val_recall_mod: 0.2506 - val_precision_mod: 0.7151 - val_dur_error: 0.3481 - val_maestro_dur_loss: 0.0348\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.10787\n",
      "50/50 - 129s - loss: 0.1114 - f1_score_mod: 0.4020 - recall_mod: 0.2735 - precision_mod: 0.7608 - dur_error: 0.3217 - maestro_dur_loss: 0.0322 - val_loss: 0.1157 - val_f1_score_mod: 0.3670 - val_recall_mod: 0.2487 - val_precision_mod: 0.7070 - val_dur_error: 0.3209 - val_maestro_dur_loss: 0.0321\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.10787\n",
      "50/50 - 129s - loss: 0.1108 - f1_score_mod: 0.4048 - recall_mod: 0.2767 - precision_mod: 0.7561 - dur_error: 0.3190 - maestro_dur_loss: 0.0319 - val_loss: 0.1154 - val_f1_score_mod: 0.3736 - val_recall_mod: 0.2542 - val_precision_mod: 0.7093 - val_dur_error: 0.3234 - val_maestro_dur_loss: 0.0323\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.10787 to 0.10699, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1104 - f1_score_mod: 0.4075 - recall_mod: 0.2793 - precision_mod: 0.7554 - dur_error: 0.3192 - maestro_dur_loss: 0.0319 - val_loss: 0.1070 - val_f1_score_mod: 0.3733 - val_recall_mod: 0.2533 - val_precision_mod: 0.7153 - val_dur_error: 0.2353 - val_maestro_dur_loss: 0.0235\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.10699\n",
      "50/50 - 129s - loss: 0.1098 - f1_score_mod: 0.4103 - recall_mod: 0.2822 - precision_mod: 0.7551 - dur_error: 0.3153 - maestro_dur_loss: 0.0315 - val_loss: 0.1079 - val_f1_score_mod: 0.3661 - val_recall_mod: 0.2450 - val_precision_mod: 0.7334 - val_dur_error: 0.2501 - val_maestro_dur_loss: 0.0250\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.10699\n",
      "50/50 - 131s - loss: 0.1096 - f1_score_mod: 0.4129 - recall_mod: 0.2842 - precision_mod: 0.7586 - dur_error: 0.3195 - maestro_dur_loss: 0.0320 - val_loss: 0.1191 - val_f1_score_mod: 0.3870 - val_recall_mod: 0.2685 - val_precision_mod: 0.6978 - val_dur_error: 0.3629 - val_maestro_dur_loss: 0.0363\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.10699 to 0.10642, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1087 - f1_score_mod: 0.4215 - recall_mod: 0.2922 - precision_mod: 0.7576 - dur_error: 0.3146 - maestro_dur_loss: 0.0315 - val_loss: 0.1064 - val_f1_score_mod: 0.3790 - val_recall_mod: 0.2583 - val_precision_mod: 0.7193 - val_dur_error: 0.2387 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10642\n",
      "50/50 - 130s - loss: 0.1085 - f1_score_mod: 0.4218 - recall_mod: 0.2927 - precision_mod: 0.7572 - dur_error: 0.3175 - maestro_dur_loss: 0.0318 - val_loss: 0.1089 - val_f1_score_mod: 0.3811 - val_recall_mod: 0.2607 - val_precision_mod: 0.7140 - val_dur_error: 0.2645 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.10642 to 0.10608, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1075 - f1_score_mod: 0.4256 - recall_mod: 0.2965 - precision_mod: 0.7567 - dur_error: 0.3099 - maestro_dur_loss: 0.0310 - val_loss: 0.1061 - val_f1_score_mod: 0.3888 - val_recall_mod: 0.2701 - val_precision_mod: 0.6983 - val_dur_error: 0.2373 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.10608\n",
      "50/50 - 129s - loss: 0.1070 - f1_score_mod: 0.4314 - recall_mod: 0.3015 - precision_mod: 0.7609 - dur_error: 0.3121 - maestro_dur_loss: 0.0312 - val_loss: 0.1071 - val_f1_score_mod: 0.3792 - val_recall_mod: 0.2584 - val_precision_mod: 0.7198 - val_dur_error: 0.2475 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10608\n",
      "50/50 - 129s - loss: 0.1067 - f1_score_mod: 0.4338 - recall_mod: 0.3041 - precision_mod: 0.7589 - dur_error: 0.3117 - maestro_dur_loss: 0.0312 - val_loss: 0.1066 - val_f1_score_mod: 0.3882 - val_recall_mod: 0.2664 - val_precision_mod: 0.7219 - val_dur_error: 0.2460 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10608\n",
      "50/50 - 128s - loss: 0.1063 - f1_score_mod: 0.4365 - recall_mod: 0.3070 - precision_mod: 0.7571 - dur_error: 0.3114 - maestro_dur_loss: 0.0311 - val_loss: 0.1064 - val_f1_score_mod: 0.3942 - val_recall_mod: 0.2770 - val_precision_mod: 0.6883 - val_dur_error: 0.2384 - val_maestro_dur_loss: 0.0238\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.10608 to 0.10552, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1055 - f1_score_mod: 0.4441 - recall_mod: 0.3141 - precision_mod: 0.7600 - dur_error: 0.3096 - maestro_dur_loss: 0.0310 - val_loss: 0.1055 - val_f1_score_mod: 0.3921 - val_recall_mod: 0.2703 - val_precision_mod: 0.7177 - val_dur_error: 0.2339 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10552\n",
      "50/50 - 129s - loss: 0.1048 - f1_score_mod: 0.4454 - recall_mod: 0.3153 - precision_mod: 0.7612 - dur_error: 0.3067 - maestro_dur_loss: 0.0307 - val_loss: 0.1090 - val_f1_score_mod: 0.3931 - val_recall_mod: 0.2711 - val_precision_mod: 0.7197 - val_dur_error: 0.2737 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.10552 to 0.10534, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1046 - f1_score_mod: 0.4509 - recall_mod: 0.3211 - precision_mod: 0.7595 - dur_error: 0.3081 - maestro_dur_loss: 0.0308 - val_loss: 0.1053 - val_f1_score_mod: 0.3877 - val_recall_mod: 0.2647 - val_precision_mod: 0.7312 - val_dur_error: 0.2332 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10534\n",
      "50/50 - 130s - loss: 0.1039 - f1_score_mod: 0.4548 - recall_mod: 0.3243 - precision_mod: 0.7627 - dur_error: 0.3059 - maestro_dur_loss: 0.0306 - val_loss: 0.1076 - val_f1_score_mod: 0.4018 - val_recall_mod: 0.2840 - val_precision_mod: 0.6916 - val_dur_error: 0.2573 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.10534 to 0.10509, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1037 - f1_score_mod: 0.4558 - recall_mod: 0.3258 - precision_mod: 0.7596 - dur_error: 0.3057 - maestro_dur_loss: 0.0306 - val_loss: 0.1051 - val_f1_score_mod: 0.4033 - val_recall_mod: 0.2834 - val_precision_mod: 0.7021 - val_dur_error: 0.2359 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10509\n",
      "50/50 - 128s - loss: 0.1028 - f1_score_mod: 0.4623 - recall_mod: 0.3320 - precision_mod: 0.7629 - dur_error: 0.3049 - maestro_dur_loss: 0.0305 - val_loss: 0.1072 - val_f1_score_mod: 0.4046 - val_recall_mod: 0.2873 - val_precision_mod: 0.6862 - val_dur_error: 0.2541 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10509\n",
      "50/50 - 129s - loss: 0.1022 - f1_score_mod: 0.4665 - recall_mod: 0.3368 - precision_mod: 0.7605 - dur_error: 0.3032 - maestro_dur_loss: 0.0303 - val_loss: 0.1102 - val_f1_score_mod: 0.4142 - val_recall_mod: 0.2951 - val_precision_mod: 0.6986 - val_dur_error: 0.2921 - val_maestro_dur_loss: 0.0292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/150\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10509\n",
      "50/50 - 130s - loss: 0.1016 - f1_score_mod: 0.4684 - recall_mod: 0.3383 - precision_mod: 0.7632 - dur_error: 0.3012 - maestro_dur_loss: 0.0301 - val_loss: 0.1084 - val_f1_score_mod: 0.4102 - val_recall_mod: 0.2913 - val_precision_mod: 0.6978 - val_dur_error: 0.2709 - val_maestro_dur_loss: 0.0271\n",
      "Epoch 87/150\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.10509 to 0.10439, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1012 - f1_score_mod: 0.4743 - recall_mod: 0.3446 - precision_mod: 0.7623 - dur_error: 0.3000 - maestro_dur_loss: 0.0300 - val_loss: 0.1044 - val_f1_score_mod: 0.4204 - val_recall_mod: 0.3017 - val_precision_mod: 0.6977 - val_dur_error: 0.2318 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 88/150\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10439\n",
      "50/50 - 129s - loss: 0.1004 - f1_score_mod: 0.4787 - recall_mod: 0.3496 - precision_mod: 0.7611 - dur_error: 0.2985 - maestro_dur_loss: 0.0298 - val_loss: 0.1045 - val_f1_score_mod: 0.4118 - val_recall_mod: 0.2898 - val_precision_mod: 0.7150 - val_dur_error: 0.2327 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 89/150\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10439\n",
      "50/50 - 129s - loss: 0.1001 - f1_score_mod: 0.4813 - recall_mod: 0.3520 - precision_mod: 0.7623 - dur_error: 0.2991 - maestro_dur_loss: 0.0299 - val_loss: 0.1070 - val_f1_score_mod: 0.4188 - val_recall_mod: 0.2981 - val_precision_mod: 0.7105 - val_dur_error: 0.2637 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 90/150\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10439\n",
      "50/50 - 129s - loss: 0.0994 - f1_score_mod: 0.4851 - recall_mod: 0.3560 - precision_mod: 0.7633 - dur_error: 0.2977 - maestro_dur_loss: 0.0298 - val_loss: 0.1055 - val_f1_score_mod: 0.4260 - val_recall_mod: 0.3086 - val_precision_mod: 0.6935 - val_dur_error: 0.2426 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 91/150\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.10439 to 0.10411, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.0992 - f1_score_mod: 0.4888 - recall_mod: 0.3599 - precision_mod: 0.7634 - dur_error: 0.2984 - maestro_dur_loss: 0.0298 - val_loss: 0.1041 - val_f1_score_mod: 0.4233 - val_recall_mod: 0.3073 - val_precision_mod: 0.6840 - val_dur_error: 0.2244 - val_maestro_dur_loss: 0.0224\n",
      "Epoch 92/150\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10411\n",
      "50/50 - 128s - loss: 0.0985 - f1_score_mod: 0.4934 - recall_mod: 0.3651 - precision_mod: 0.7626 - dur_error: 0.2964 - maestro_dur_loss: 0.0296 - val_loss: 0.1045 - val_f1_score_mod: 0.4287 - val_recall_mod: 0.3134 - val_precision_mod: 0.6827 - val_dur_error: 0.2340 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 93/150\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.10411\n",
      "50/50 - 129s - loss: 0.0977 - f1_score_mod: 0.5002 - recall_mod: 0.3712 - precision_mod: 0.7678 - dur_error: 0.2948 - maestro_dur_loss: 0.0295 - val_loss: 0.1059 - val_f1_score_mod: 0.4291 - val_recall_mod: 0.3129 - val_precision_mod: 0.6862 - val_dur_error: 0.2435 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 94/150\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.10411\n",
      "50/50 - 128s - loss: 0.0972 - f1_score_mod: 0.5017 - recall_mod: 0.3737 - precision_mod: 0.7646 - dur_error: 0.2940 - maestro_dur_loss: 0.0294 - val_loss: 0.1074 - val_f1_score_mod: 0.4339 - val_recall_mod: 0.3213 - val_precision_mod: 0.6714 - val_dur_error: 0.2639 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 95/150\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10411\n",
      "50/50 - 129s - loss: 0.0968 - f1_score_mod: 0.5062 - recall_mod: 0.3785 - precision_mod: 0.7658 - dur_error: 0.2930 - maestro_dur_loss: 0.0293 - val_loss: 0.1044 - val_f1_score_mod: 0.4350 - val_recall_mod: 0.3195 - val_precision_mod: 0.6845 - val_dur_error: 0.2312 - val_maestro_dur_loss: 0.0231\n",
      "Epoch 96/150\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.10411\n",
      "50/50 - 129s - loss: 0.0959 - f1_score_mod: 0.5105 - recall_mod: 0.3826 - precision_mod: 0.7684 - dur_error: 0.2912 - maestro_dur_loss: 0.0291 - val_loss: 0.1054 - val_f1_score_mod: 0.4372 - val_recall_mod: 0.3235 - val_precision_mod: 0.6771 - val_dur_error: 0.2435 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 97/150\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.10411\n",
      "50/50 - 129s - loss: 0.0955 - f1_score_mod: 0.5152 - recall_mod: 0.3887 - precision_mod: 0.7656 - dur_error: 0.2911 - maestro_dur_loss: 0.0291 - val_loss: 0.1128 - val_f1_score_mod: 0.4445 - val_recall_mod: 0.3364 - val_precision_mod: 0.6578 - val_dur_error: 0.3140 - val_maestro_dur_loss: 0.0314\n",
      "Epoch 98/150\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.10411\n",
      "50/50 - 129s - loss: 0.0949 - f1_score_mod: 0.5185 - recall_mod: 0.3922 - precision_mod: 0.7664 - dur_error: 0.2910 - maestro_dur_loss: 0.0291 - val_loss: 0.1099 - val_f1_score_mod: 0.4510 - val_recall_mod: 0.3459 - val_precision_mod: 0.6495 - val_dur_error: 0.2900 - val_maestro_dur_loss: 0.0290\n",
      "Epoch 99/150\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.10411\n",
      "50/50 - 129s - loss: 0.0943 - f1_score_mod: 0.5244 - recall_mod: 0.3985 - precision_mod: 0.7680 - dur_error: 0.2898 - maestro_dur_loss: 0.0290 - val_loss: 0.1056 - val_f1_score_mod: 0.4412 - val_recall_mod: 0.3240 - val_precision_mod: 0.6945 - val_dur_error: 0.2530 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 100/150\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.10411 to 0.10361, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.0937 - f1_score_mod: 0.5279 - recall_mod: 0.4022 - precision_mod: 0.7692 - dur_error: 0.2895 - maestro_dur_loss: 0.0289 - val_loss: 0.1036 - val_f1_score_mod: 0.4450 - val_recall_mod: 0.3312 - val_precision_mod: 0.6801 - val_dur_error: 0.2250 - val_maestro_dur_loss: 0.0225\n",
      "Epoch 101/150\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.10361\n",
      "50/50 - 129s - loss: 0.0932 - f1_score_mod: 0.5331 - recall_mod: 0.4087 - precision_mod: 0.7671 - dur_error: 0.2875 - maestro_dur_loss: 0.0288 - val_loss: 0.1047 - val_f1_score_mod: 0.4517 - val_recall_mod: 0.3406 - val_precision_mod: 0.6731 - val_dur_error: 0.2376 - val_maestro_dur_loss: 0.0238\n",
      "Epoch 102/150\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.10361\n",
      "50/50 - 130s - loss: 0.0924 - f1_score_mod: 0.5337 - recall_mod: 0.4095 - precision_mod: 0.7674 - dur_error: 0.2856 - maestro_dur_loss: 0.0286 - val_loss: 0.1043 - val_f1_score_mod: 0.4534 - val_recall_mod: 0.3439 - val_precision_mod: 0.6674 - val_dur_error: 0.2298 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 103/150\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.10361\n",
      "50/50 - 129s - loss: 0.0920 - f1_score_mod: 0.5403 - recall_mod: 0.4168 - precision_mod: 0.7689 - dur_error: 0.2857 - maestro_dur_loss: 0.0286 - val_loss: 0.1073 - val_f1_score_mod: 0.4555 - val_recall_mod: 0.3455 - val_precision_mod: 0.6706 - val_dur_error: 0.2611 - val_maestro_dur_loss: 0.0261\n",
      "Epoch 104/150\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.10361\n",
      "50/50 - 128s - loss: 0.0913 - f1_score_mod: 0.5439 - recall_mod: 0.4214 - precision_mod: 0.7680 - dur_error: 0.2849 - maestro_dur_loss: 0.0285 - val_loss: 0.1048 - val_f1_score_mod: 0.4619 - val_recall_mod: 0.3544 - val_precision_mod: 0.6652 - val_dur_error: 0.2324 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 105/150\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.10361\n",
      "50/50 - 129s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 0.25, dropout_rate = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! this change yielded the largest improvement yet, in terms of validation loss, but especially training loss. We are really reaching the point where the network is overfitting to the training set which was the goal. However, we also must be wary since lowering the dropout_rate increases overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19064, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 133s - loss: 0.2832 - f1_score_mod: 0.0105 - recall_mod: 0.0245 - precision_mod: 0.0712 - dur_error: 0.9878 - maestro_dur_loss: 0.0988 - val_loss: 0.1906 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6633 - val_maestro_dur_loss: 0.0663\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19064 to 0.18398, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1981 - f1_score_mod: 1.9077e-04 - recall_mod: 9.5513e-05 - precision_mod: 0.0900 - dur_error: 0.6627 - maestro_dur_loss: 0.0663 - val_loss: 0.1840 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6016 - val_maestro_dur_loss: 0.0602\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.18398 to 0.15944, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1846 - f1_score_mod: 0.0033 - recall_mod: 0.0017 - precision_mod: 0.3402 - dur_error: 0.5916 - maestro_dur_loss: 0.0592 - val_loss: 0.1594 - val_f1_score_mod: 3.1263e-04 - val_recall_mod: 1.5651e-04 - val_precision_mod: 0.1364 - val_dur_error: 0.4300 - val_maestro_dur_loss: 0.0430\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15944 to 0.15252, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1749 - f1_score_mod: 0.0162 - recall_mod: 0.0083 - precision_mod: 0.5090 - dur_error: 0.5503 - maestro_dur_loss: 0.0550 - val_loss: 0.1525 - val_f1_score_mod: 0.0089 - val_recall_mod: 0.0045 - val_precision_mod: 0.7778 - val_dur_error: 0.4027 - val_maestro_dur_loss: 0.0403\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15252 to 0.15129, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1688 - f1_score_mod: 0.0381 - recall_mod: 0.0198 - precision_mod: 0.5873 - dur_error: 0.5234 - maestro_dur_loss: 0.0523 - val_loss: 0.1513 - val_f1_score_mod: 0.0666 - val_recall_mod: 0.0356 - val_precision_mod: 0.5329 - val_dur_error: 0.4134 - val_maestro_dur_loss: 0.0413\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15129\n",
      "50/50 - 129s - loss: 0.1637 - f1_score_mod: 0.0616 - recall_mod: 0.0327 - precision_mod: 0.6092 - dur_error: 0.5029 - maestro_dur_loss: 0.0503 - val_loss: 0.1543 - val_f1_score_mod: 0.0508 - val_recall_mod: 0.0265 - val_precision_mod: 0.6469 - val_dur_error: 0.4600 - val_maestro_dur_loss: 0.0460\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15129 to 0.14526, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 129s - loss: 0.1608 - f1_score_mod: 0.0753 - recall_mod: 0.0404 - precision_mod: 0.6342 - dur_error: 0.4910 - maestro_dur_loss: 0.0491 - val_loss: 0.1453 - val_f1_score_mod: 0.0860 - val_recall_mod: 0.0461 - val_precision_mod: 0.6679 - val_dur_error: 0.3958 - val_maestro_dur_loss: 0.0396\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14526 to 0.14524, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 130s - loss: 0.1570 - f1_score_mod: 0.0936 - recall_mod: 0.0507 - precision_mod: 0.6496 - dur_error: 0.4744 - maestro_dur_loss: 0.0474 - val_loss: 0.1452 - val_f1_score_mod: 0.0785 - val_recall_mod: 0.0416 - val_precision_mod: 0.7284 - val_dur_error: 0.4005 - val_maestro_dur_loss: 0.0400\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14524 to 0.13740, saving model to ../models/best_maestro_model_2_1_512_0pt2_lr_5e-04_cn_0pt25.h5\n",
      "50/50 - 128s - loss: 0.1551 - f1_score_mod: 0.1130 - recall_mod: 0.0621 - precision_mod: 0.6603 - dur_error: 0.4667 - maestro_dur_loss: 0.0467 - val_loss: 0.1374 - val_f1_score_mod: 0.0945 - val_recall_mod: 0.0507 - val_precision_mod: 0.7383 - val_dur_error: 0.3446 - val_maestro_dur_loss: 0.0345\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13740\n",
      "50/50 - 129s - loss: 0.1523 - f1_score_mod: 0.1220 - recall_mod: 0.0675 - precision_mod: 0.6671 - dur_error: 0.4517 - maestro_dur_loss: 0.0452 - val_loss: 0.1458 - val_f1_score_mod: 0.1241 - val_recall_mod: 0.0680 - val_precision_mod: 0.7162 - val_dur_error: 0.4387 - val_maestro_dur_loss: 0.0439\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13740\n",
      "50/50 - 130s - loss: 0.1499 - f1_score_mod: 0.1369 - recall_mod: 0.0762 - precision_mod: 0.6789 - dur_error: 0.4403 - maestro_dur_loss: 0.0440 - val_loss: 0.1407 - val_f1_score_mod: 0.1281 - val_recall_mod: 0.0705 - val_precision_mod: 0.7129 - val_dur_error: 0.4008 - val_maestro_dur_loss: 0.0401\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13740\n",
      "50/50 - 130s - loss: 0.1477 - f1_score_mod: 0.1461 - recall_mod: 0.0822 - precision_mod: 0.6746 - dur_error: 0.4301 - maestro_dur_loss: 0.0430 - val_loss: 0.1393 - val_f1_score_mod: 0.1474 - val_recall_mod: 0.0825 - val_precision_mod: 0.7024 - val_dur_error: 0.3866 - val_maestro_dur_loss: 0.0387\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-b6fa568eacc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-d0718b19ceac>\u001b[0m in \u001b[0;36mtrain_lstm_model\u001b[0;34m(n_lstm_layers, n_dense_layers, n_lstm_nodes, dropout_rate, batch_size, harshness, lr, clipnorm, clipvalue, epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \\\n\u001b[0;32m---> 36\u001b[0;31m                     validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# In most preliminary tests model training has failed at some point when the loss becomes NaN during\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 0.25, dropout_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 0.25, dropout_rate = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18762, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 128s - loss: 0.3107 - f1_score_mod: 0.0238 - recall_mod: 0.0387 - precision_mod: 0.0529 - dur_error: 1.0092 - maestro_dur_loss: 0.1009 - val_loss: 0.1876 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6077 - val_maestro_dur_loss: 0.0608\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18762 to 0.17048, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 127s - loss: 0.2130 - f1_score_mod: 0.0029 - recall_mod: 0.0015 - precision_mod: 0.1101 - dur_error: 0.7159 - maestro_dur_loss: 0.0716 - val_loss: 0.1705 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4973 - val_maestro_dur_loss: 0.0497\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17048 to 0.16469, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1967 - f1_score_mod: 0.0051 - recall_mod: 0.0026 - precision_mod: 0.3133 - dur_error: 0.6490 - maestro_dur_loss: 0.0649 - val_loss: 0.1647 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4651 - val_maestro_dur_loss: 0.0465\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16469 to 0.16211, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1876 - f1_score_mod: 0.0095 - recall_mod: 0.0048 - precision_mod: 0.4000 - dur_error: 0.6137 - maestro_dur_loss: 0.0614 - val_loss: 0.1621 - val_f1_score_mod: 0.0045 - val_recall_mod: 0.0022 - val_precision_mod: 0.6843 - val_dur_error: 0.4780 - val_maestro_dur_loss: 0.0478\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16211 to 0.15958, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1821 - f1_score_mod: 0.0142 - recall_mod: 0.0072 - precision_mod: 0.4568 - dur_error: 0.5928 - maestro_dur_loss: 0.0593 - val_loss: 0.1596 - val_f1_score_mod: 0.0074 - val_recall_mod: 0.0037 - val_precision_mod: 0.7432 - val_dur_error: 0.4737 - val_maestro_dur_loss: 0.0474\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15958 to 0.15549, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 124s - loss: 0.1781 - f1_score_mod: 0.0236 - recall_mod: 0.0121 - precision_mod: 0.4940 - dur_error: 0.5785 - maestro_dur_loss: 0.0578 - val_loss: 0.1555 - val_f1_score_mod: 0.0303 - val_recall_mod: 0.0156 - val_precision_mod: 0.5688 - val_dur_error: 0.4448 - val_maestro_dur_loss: 0.0445\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15549 to 0.14860, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 124s - loss: 0.1748 - f1_score_mod: 0.0325 - recall_mod: 0.0168 - precision_mod: 0.5298 - dur_error: 0.5637 - maestro_dur_loss: 0.0564 - val_loss: 0.1486 - val_f1_score_mod: 0.0205 - val_recall_mod: 0.0104 - val_precision_mod: 0.7119 - val_dur_error: 0.3935 - val_maestro_dur_loss: 0.0394\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14860 to 0.14835, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1716 - f1_score_mod: 0.0378 - recall_mod: 0.0196 - precision_mod: 0.5784 - dur_error: 0.5500 - maestro_dur_loss: 0.0550 - val_loss: 0.1484 - val_f1_score_mod: 0.0456 - val_recall_mod: 0.0236 - val_precision_mod: 0.7357 - val_dur_error: 0.4042 - val_maestro_dur_loss: 0.0404\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14835\n",
      "50/50 - 124s - loss: 0.1684 - f1_score_mod: 0.0451 - recall_mod: 0.0235 - precision_mod: 0.5947 - dur_error: 0.5318 - maestro_dur_loss: 0.0532 - val_loss: 0.1899 - val_f1_score_mod: 0.0564 - val_recall_mod: 0.0294 - val_precision_mod: 0.7150 - val_dur_error: 0.8100 - val_maestro_dur_loss: 0.0810\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14835 to 0.13873, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 124s - loss: 0.1669 - f1_score_mod: 0.0506 - recall_mod: 0.0265 - precision_mod: 0.6062 - dur_error: 0.5247 - maestro_dur_loss: 0.0525 - val_loss: 0.1387 - val_f1_score_mod: 0.0591 - val_recall_mod: 0.0309 - val_precision_mod: 0.7405 - val_dur_error: 0.3298 - val_maestro_dur_loss: 0.0330\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13873 to 0.13758, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 124s - loss: 0.1647 - f1_score_mod: 0.0613 - recall_mod: 0.0323 - precision_mod: 0.6139 - dur_error: 0.5138 - maestro_dur_loss: 0.0514 - val_loss: 0.1376 - val_f1_score_mod: 0.0691 - val_recall_mod: 0.0364 - val_precision_mod: 0.7393 - val_dur_error: 0.3234 - val_maestro_dur_loss: 0.0323\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13758\n",
      "50/50 - 124s - loss: 0.1625 - f1_score_mod: 0.0676 - recall_mod: 0.0358 - precision_mod: 0.6328 - dur_error: 0.5022 - maestro_dur_loss: 0.0502 - val_loss: 0.1421 - val_f1_score_mod: 0.0861 - val_recall_mod: 0.0461 - val_precision_mod: 0.6909 - val_dur_error: 0.3782 - val_maestro_dur_loss: 0.0378\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13758\n",
      "50/50 - 124s - loss: 0.1612 - f1_score_mod: 0.0786 - recall_mod: 0.0420 - precision_mod: 0.6413 - dur_error: 0.4975 - maestro_dur_loss: 0.0497 - val_loss: 0.1433 - val_f1_score_mod: 0.0978 - val_recall_mod: 0.0528 - val_precision_mod: 0.6870 - val_dur_error: 0.3961 - val_maestro_dur_loss: 0.0396\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13758\n",
      "50/50 - 127s - loss: 0.1599 - f1_score_mod: 0.0833 - recall_mod: 0.0447 - precision_mod: 0.6489 - dur_error: 0.4886 - maestro_dur_loss: 0.0489 - val_loss: 0.1398 - val_f1_score_mod: 0.0996 - val_recall_mod: 0.0538 - val_precision_mod: 0.6875 - val_dur_error: 0.3660 - val_maestro_dur_loss: 0.0366\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.13758 to 0.13548, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1585 - f1_score_mod: 0.0889 - recall_mod: 0.0478 - precision_mod: 0.6478 - dur_error: 0.4831 - maestro_dur_loss: 0.0483 - val_loss: 0.1355 - val_f1_score_mod: 0.0842 - val_recall_mod: 0.0447 - val_precision_mod: 0.7516 - val_dur_error: 0.3245 - val_maestro_dur_loss: 0.0324\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13548\n",
      "50/50 - 124s - loss: 0.1575 - f1_score_mod: 0.0957 - recall_mod: 0.0519 - precision_mod: 0.6450 - dur_error: 0.4785 - maestro_dur_loss: 0.0478 - val_loss: 0.1452 - val_f1_score_mod: 0.1055 - val_recall_mod: 0.0571 - val_precision_mod: 0.7300 - val_dur_error: 0.4284 - val_maestro_dur_loss: 0.0428\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13548\n",
      "50/50 - 124s - loss: 0.1560 - f1_score_mod: 0.1002 - recall_mod: 0.0545 - precision_mod: 0.6479 - dur_error: 0.4698 - maestro_dur_loss: 0.0470 - val_loss: 0.1492 - val_f1_score_mod: 0.1162 - val_recall_mod: 0.0636 - val_precision_mod: 0.6998 - val_dur_error: 0.4728 - val_maestro_dur_loss: 0.0473\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13548\n",
      "50/50 - 125s - loss: 0.1555 - f1_score_mod: 0.1046 - recall_mod: 0.0569 - precision_mod: 0.6570 - dur_error: 0.4690 - maestro_dur_loss: 0.0469 - val_loss: 0.1421 - val_f1_score_mod: 0.1334 - val_recall_mod: 0.0744 - val_precision_mod: 0.6633 - val_dur_error: 0.4113 - val_maestro_dur_loss: 0.0411\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.13548 to 0.13027, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 124s - loss: 0.1544 - f1_score_mod: 0.1102 - recall_mod: 0.0604 - precision_mod: 0.6499 - dur_error: 0.4653 - maestro_dur_loss: 0.0465 - val_loss: 0.1303 - val_f1_score_mod: 0.1100 - val_recall_mod: 0.0595 - val_precision_mod: 0.7482 - val_dur_error: 0.2910 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.13027\n",
      "50/50 - 124s - loss: 0.1534 - f1_score_mod: 0.1147 - recall_mod: 0.0629 - precision_mod: 0.6677 - dur_error: 0.4608 - maestro_dur_loss: 0.0461 - val_loss: 0.1406 - val_f1_score_mod: 0.1409 - val_recall_mod: 0.0789 - val_precision_mod: 0.6710 - val_dur_error: 0.3947 - val_maestro_dur_loss: 0.0395\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.13027\n",
      "50/50 - 125s - loss: 0.1522 - f1_score_mod: 0.1209 - recall_mod: 0.0666 - precision_mod: 0.6724 - dur_error: 0.4536 - maestro_dur_loss: 0.0454 - val_loss: 0.1444 - val_f1_score_mod: 0.1337 - val_recall_mod: 0.0741 - val_precision_mod: 0.6986 - val_dur_error: 0.4370 - val_maestro_dur_loss: 0.0437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.13027 to 0.12866, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 124s - loss: 0.1518 - f1_score_mod: 0.1200 - recall_mod: 0.0661 - precision_mod: 0.6658 - dur_error: 0.4536 - maestro_dur_loss: 0.0454 - val_loss: 0.1287 - val_f1_score_mod: 0.1323 - val_recall_mod: 0.0730 - val_precision_mod: 0.7245 - val_dur_error: 0.2933 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12866\n",
      "50/50 - 124s - loss: 0.1510 - f1_score_mod: 0.1279 - recall_mod: 0.0710 - precision_mod: 0.6673 - dur_error: 0.4502 - maestro_dur_loss: 0.0450 - val_loss: 0.1306 - val_f1_score_mod: 0.1212 - val_recall_mod: 0.0659 - val_precision_mod: 0.7688 - val_dur_error: 0.3141 - val_maestro_dur_loss: 0.0314\n",
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12866 to 0.12658, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 124s - loss: 0.1507 - f1_score_mod: 0.1291 - recall_mod: 0.0717 - precision_mod: 0.6634 - dur_error: 0.4504 - maestro_dur_loss: 0.0450 - val_loss: 0.1266 - val_f1_score_mod: 0.1489 - val_recall_mod: 0.0835 - val_precision_mod: 0.7050 - val_dur_error: 0.2780 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12658\n",
      "50/50 - 125s - loss: 0.1497 - f1_score_mod: 0.1338 - recall_mod: 0.0746 - precision_mod: 0.6685 - dur_error: 0.4444 - maestro_dur_loss: 0.0444 - val_loss: 0.1341 - val_f1_score_mod: 0.1431 - val_recall_mod: 0.0796 - val_precision_mod: 0.7232 - val_dur_error: 0.3566 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12658\n",
      "50/50 - 125s - loss: 0.1489 - f1_score_mod: 0.1362 - recall_mod: 0.0760 - precision_mod: 0.6737 - dur_error: 0.4410 - maestro_dur_loss: 0.0441 - val_loss: 0.1288 - val_f1_score_mod: 0.1414 - val_recall_mod: 0.0784 - val_precision_mod: 0.7318 - val_dur_error: 0.3065 - val_maestro_dur_loss: 0.0307\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12658\n",
      "50/50 - 124s - loss: 0.1482 - f1_score_mod: 0.1399 - recall_mod: 0.0782 - precision_mod: 0.6798 - dur_error: 0.4386 - maestro_dur_loss: 0.0439 - val_loss: 0.1456 - val_f1_score_mod: 0.1569 - val_recall_mod: 0.0881 - val_precision_mod: 0.7253 - val_dur_error: 0.4729 - val_maestro_dur_loss: 0.0473\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.12658 to 0.12524, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1481 - f1_score_mod: 0.1442 - recall_mod: 0.0809 - precision_mod: 0.6785 - dur_error: 0.4397 - maestro_dur_loss: 0.0440 - val_loss: 0.1252 - val_f1_score_mod: 0.1488 - val_recall_mod: 0.0832 - val_precision_mod: 0.7229 - val_dur_error: 0.2785 - val_maestro_dur_loss: 0.0279\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12524\n",
      "50/50 - 125s - loss: 0.1475 - f1_score_mod: 0.1500 - recall_mod: 0.0844 - precision_mod: 0.6815 - dur_error: 0.4374 - maestro_dur_loss: 0.0437 - val_loss: 0.1302 - val_f1_score_mod: 0.1681 - val_recall_mod: 0.0960 - val_precision_mod: 0.6878 - val_dur_error: 0.3291 - val_maestro_dur_loss: 0.0329\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12524\n",
      "50/50 - 125s - loss: 0.1467 - f1_score_mod: 0.1501 - recall_mod: 0.0848 - precision_mod: 0.6694 - dur_error: 0.4344 - maestro_dur_loss: 0.0434 - val_loss: 0.1254 - val_f1_score_mod: 0.1431 - val_recall_mod: 0.0792 - val_precision_mod: 0.7614 - val_dur_error: 0.2781 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.12524\n",
      "50/50 - 125s - loss: 0.1464 - f1_score_mod: 0.1553 - recall_mod: 0.0881 - precision_mod: 0.6738 - dur_error: 0.4335 - maestro_dur_loss: 0.0433 - val_loss: 0.1256 - val_f1_score_mod: 0.1521 - val_recall_mod: 0.0849 - val_precision_mod: 0.7474 - val_dur_error: 0.2842 - val_maestro_dur_loss: 0.0284\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.12524\n",
      "50/50 - 125s - loss: 0.1456 - f1_score_mod: 0.1588 - recall_mod: 0.0899 - precision_mod: 0.6884 - dur_error: 0.4301 - maestro_dur_loss: 0.0430 - val_loss: 0.1324 - val_f1_score_mod: 0.1653 - val_recall_mod: 0.0936 - val_precision_mod: 0.7270 - val_dur_error: 0.3617 - val_maestro_dur_loss: 0.0362\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.12524 to 0.12391, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1454 - f1_score_mod: 0.1599 - recall_mod: 0.0908 - precision_mod: 0.6815 - dur_error: 0.4299 - maestro_dur_loss: 0.0430 - val_loss: 0.1239 - val_f1_score_mod: 0.1700 - val_recall_mod: 0.0965 - val_precision_mod: 0.7299 - val_dur_error: 0.2778 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.12391\n",
      "50/50 - 124s - loss: 0.1447 - f1_score_mod: 0.1629 - recall_mod: 0.0928 - precision_mod: 0.6819 - dur_error: 0.4257 - maestro_dur_loss: 0.0426 - val_loss: 0.1244 - val_f1_score_mod: 0.1772 - val_recall_mod: 0.1014 - val_precision_mod: 0.7110 - val_dur_error: 0.2846 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.12391\n",
      "50/50 - 125s - loss: 0.1440 - f1_score_mod: 0.1659 - recall_mod: 0.0948 - precision_mod: 0.6803 - dur_error: 0.4234 - maestro_dur_loss: 0.0423 - val_loss: 0.1241 - val_f1_score_mod: 0.1698 - val_recall_mod: 0.0962 - val_precision_mod: 0.7404 - val_dur_error: 0.2819 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.12391\n",
      "50/50 - 129s - loss: 0.1437 - f1_score_mod: 0.1693 - recall_mod: 0.0969 - precision_mod: 0.6855 - dur_error: 0.4223 - maestro_dur_loss: 0.0422 - val_loss: 0.1274 - val_f1_score_mod: 0.1738 - val_recall_mod: 0.0987 - val_precision_mod: 0.7462 - val_dur_error: 0.3230 - val_maestro_dur_loss: 0.0323\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.12391\n",
      "50/50 - 127s - loss: 0.1436 - f1_score_mod: 0.1703 - recall_mod: 0.0976 - precision_mod: 0.6768 - dur_error: 0.4229 - maestro_dur_loss: 0.0423 - val_loss: 0.1256 - val_f1_score_mod: 0.1881 - val_recall_mod: 0.1085 - val_precision_mod: 0.7156 - val_dur_error: 0.3040 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.12391 to 0.12241, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 127s - loss: 0.1425 - f1_score_mod: 0.1747 - recall_mod: 0.1003 - precision_mod: 0.6906 - dur_error: 0.4165 - maestro_dur_loss: 0.0416 - val_loss: 0.1224 - val_f1_score_mod: 0.1850 - val_recall_mod: 0.1063 - val_precision_mod: 0.7262 - val_dur_error: 0.2753 - val_maestro_dur_loss: 0.0275\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.12241\n",
      "50/50 - 127s - loss: 0.1422 - f1_score_mod: 0.1747 - recall_mod: 0.1004 - precision_mod: 0.6866 - dur_error: 0.4158 - maestro_dur_loss: 0.0416 - val_loss: 0.1349 - val_f1_score_mod: 0.1881 - val_recall_mod: 0.1085 - val_precision_mod: 0.7215 - val_dur_error: 0.4041 - val_maestro_dur_loss: 0.0404\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.12241 to 0.12147, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1420 - f1_score_mod: 0.1810 - recall_mod: 0.1046 - precision_mod: 0.6837 - dur_error: 0.4164 - maestro_dur_loss: 0.0416 - val_loss: 0.1215 - val_f1_score_mod: 0.2038 - val_recall_mod: 0.1199 - val_precision_mod: 0.6944 - val_dur_error: 0.2691 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.12147\n",
      "50/50 - 126s - loss: 0.1414 - f1_score_mod: 0.1857 - recall_mod: 0.1075 - precision_mod: 0.6939 - dur_error: 0.4141 - maestro_dur_loss: 0.0414 - val_loss: 0.1222 - val_f1_score_mod: 0.1827 - val_recall_mod: 0.1047 - val_precision_mod: 0.7325 - val_dur_error: 0.2756 - val_maestro_dur_loss: 0.0276\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.12147\n",
      "50/50 - 126s - loss: 0.1408 - f1_score_mod: 0.1879 - recall_mod: 0.1088 - precision_mod: 0.6963 - dur_error: 0.4098 - maestro_dur_loss: 0.0410 - val_loss: 0.1236 - val_f1_score_mod: 0.1951 - val_recall_mod: 0.1126 - val_precision_mod: 0.7432 - val_dur_error: 0.2952 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.12147\n",
      "50/50 - 128s - loss: 0.1410 - f1_score_mod: 0.1912 - recall_mod: 0.1111 - precision_mod: 0.6940 - dur_error: 0.4126 - maestro_dur_loss: 0.0413 - val_loss: 0.1278 - val_f1_score_mod: 0.1859 - val_recall_mod: 0.1067 - val_precision_mod: 0.7407 - val_dur_error: 0.3365 - val_maestro_dur_loss: 0.0336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.12147 to 0.12070, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1399 - f1_score_mod: 0.1936 - recall_mod: 0.1125 - precision_mod: 0.7067 - dur_error: 0.4079 - maestro_dur_loss: 0.0408 - val_loss: 0.1207 - val_f1_score_mod: 0.2009 - val_recall_mod: 0.1166 - val_precision_mod: 0.7380 - val_dur_error: 0.2704 - val_maestro_dur_loss: 0.0270\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.12070\n",
      "50/50 - 126s - loss: 0.1395 - f1_score_mod: 0.1929 - recall_mod: 0.1121 - precision_mod: 0.6986 - dur_error: 0.4043 - maestro_dur_loss: 0.0404 - val_loss: 0.1266 - val_f1_score_mod: 0.2042 - val_recall_mod: 0.1195 - val_precision_mod: 0.7148 - val_dur_error: 0.3315 - val_maestro_dur_loss: 0.0332\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.12070\n",
      "50/50 - 126s - loss: 0.1393 - f1_score_mod: 0.1946 - recall_mod: 0.1133 - precision_mod: 0.6985 - dur_error: 0.4054 - maestro_dur_loss: 0.0405 - val_loss: 0.1208 - val_f1_score_mod: 0.2002 - val_recall_mod: 0.1160 - val_precision_mod: 0.7419 - val_dur_error: 0.2757 - val_maestro_dur_loss: 0.0276\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.12070 to 0.11979, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 127s - loss: 0.1383 - f1_score_mod: 0.2022 - recall_mod: 0.1184 - precision_mod: 0.7024 - dur_error: 0.4002 - maestro_dur_loss: 0.0400 - val_loss: 0.1198 - val_f1_score_mod: 0.2117 - val_recall_mod: 0.1241 - val_precision_mod: 0.7289 - val_dur_error: 0.2679 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11979\n",
      "50/50 - 127s - loss: 0.1385 - f1_score_mod: 0.2040 - recall_mod: 0.1196 - precision_mod: 0.7050 - dur_error: 0.4008 - maestro_dur_loss: 0.0401 - val_loss: 0.1199 - val_f1_score_mod: 0.2091 - val_recall_mod: 0.1224 - val_precision_mod: 0.7306 - val_dur_error: 0.2714 - val_maestro_dur_loss: 0.0271\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.11979 to 0.11918, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1376 - f1_score_mod: 0.2074 - recall_mod: 0.1220 - precision_mod: 0.7036 - dur_error: 0.3983 - maestro_dur_loss: 0.0398 - val_loss: 0.1192 - val_f1_score_mod: 0.2280 - val_recall_mod: 0.1366 - val_precision_mod: 0.6980 - val_dur_error: 0.2616 - val_maestro_dur_loss: 0.0262\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11918\n",
      "50/50 - 127s - loss: 0.1378 - f1_score_mod: 0.2070 - recall_mod: 0.1218 - precision_mod: 0.7029 - dur_error: 0.4016 - maestro_dur_loss: 0.0402 - val_loss: 0.1266 - val_f1_score_mod: 0.2246 - val_recall_mod: 0.1345 - val_precision_mod: 0.6930 - val_dur_error: 0.3357 - val_maestro_dur_loss: 0.0336\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11918\n",
      "50/50 - 126s - loss: 0.1373 - f1_score_mod: 0.2099 - recall_mod: 0.1236 - precision_mod: 0.7051 - dur_error: 0.3978 - maestro_dur_loss: 0.0398 - val_loss: 0.1213 - val_f1_score_mod: 0.2145 - val_recall_mod: 0.1257 - val_precision_mod: 0.7506 - val_dur_error: 0.2904 - val_maestro_dur_loss: 0.0290\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11918\n",
      "50/50 - 126s - loss: 0.1369 - f1_score_mod: 0.2131 - recall_mod: 0.1257 - precision_mod: 0.7092 - dur_error: 0.3955 - maestro_dur_loss: 0.0396 - val_loss: 0.1196 - val_f1_score_mod: 0.2292 - val_recall_mod: 0.1371 - val_precision_mod: 0.7078 - val_dur_error: 0.2741 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.11918 to 0.11703, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1365 - f1_score_mod: 0.2173 - recall_mod: 0.1289 - precision_mod: 0.7036 - dur_error: 0.3964 - maestro_dur_loss: 0.0396 - val_loss: 0.1170 - val_f1_score_mod: 0.2186 - val_recall_mod: 0.1285 - val_precision_mod: 0.7398 - val_dur_error: 0.2523 - val_maestro_dur_loss: 0.0252\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11703\n",
      "50/50 - 126s - loss: 0.1359 - f1_score_mod: 0.2156 - recall_mod: 0.1274 - precision_mod: 0.7111 - dur_error: 0.3924 - maestro_dur_loss: 0.0392 - val_loss: 0.1195 - val_f1_score_mod: 0.2269 - val_recall_mod: 0.1342 - val_precision_mod: 0.7452 - val_dur_error: 0.2776 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11703\n",
      "50/50 - 126s - loss: 0.1356 - f1_score_mod: 0.2237 - recall_mod: 0.1330 - precision_mod: 0.7132 - dur_error: 0.3922 - maestro_dur_loss: 0.0392 - val_loss: 0.1187 - val_f1_score_mod: 0.2156 - val_recall_mod: 0.1256 - val_precision_mod: 0.7716 - val_dur_error: 0.2714 - val_maestro_dur_loss: 0.0271\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11703\n",
      "50/50 - 127s - loss: 0.1350 - f1_score_mod: 0.2254 - recall_mod: 0.1342 - precision_mod: 0.7138 - dur_error: 0.3905 - maestro_dur_loss: 0.0390 - val_loss: 0.1179 - val_f1_score_mod: 0.2288 - val_recall_mod: 0.1352 - val_precision_mod: 0.7602 - val_dur_error: 0.2651 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11703\n",
      "50/50 - 126s - loss: 0.1349 - f1_score_mod: 0.2249 - recall_mod: 0.1339 - precision_mod: 0.7097 - dur_error: 0.3906 - maestro_dur_loss: 0.0391 - val_loss: 0.1216 - val_f1_score_mod: 0.2340 - val_recall_mod: 0.1392 - val_precision_mod: 0.7523 - val_dur_error: 0.3050 - val_maestro_dur_loss: 0.0305\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11703\n",
      "50/50 - 126s - loss: 0.1344 - f1_score_mod: 0.2335 - recall_mod: 0.1398 - precision_mod: 0.7126 - dur_error: 0.3880 - maestro_dur_loss: 0.0388 - val_loss: 0.1291 - val_f1_score_mod: 0.2398 - val_recall_mod: 0.1434 - val_precision_mod: 0.7472 - val_dur_error: 0.3795 - val_maestro_dur_loss: 0.0380\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.11703 to 0.11610, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1341 - f1_score_mod: 0.2329 - recall_mod: 0.1397 - precision_mod: 0.7121 - dur_error: 0.3892 - maestro_dur_loss: 0.0389 - val_loss: 0.1161 - val_f1_score_mod: 0.2404 - val_recall_mod: 0.1441 - val_precision_mod: 0.7411 - val_dur_error: 0.2540 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11610\n",
      "50/50 - 126s - loss: 0.1338 - f1_score_mod: 0.2346 - recall_mod: 0.1406 - precision_mod: 0.7130 - dur_error: 0.3869 - maestro_dur_loss: 0.0387 - val_loss: 0.1176 - val_f1_score_mod: 0.2403 - val_recall_mod: 0.1435 - val_precision_mod: 0.7502 - val_dur_error: 0.2699 - val_maestro_dur_loss: 0.0270\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.11610 to 0.11545, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1330 - f1_score_mod: 0.2375 - recall_mod: 0.1427 - precision_mod: 0.7149 - dur_error: 0.3820 - maestro_dur_loss: 0.0382 - val_loss: 0.1154 - val_f1_score_mod: 0.2562 - val_recall_mod: 0.1565 - val_precision_mod: 0.7160 - val_dur_error: 0.2484 - val_maestro_dur_loss: 0.0248\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.11545\n",
      "50/50 - 126s - loss: 0.1327 - f1_score_mod: 0.2425 - recall_mod: 0.1466 - precision_mod: 0.7106 - dur_error: 0.3814 - maestro_dur_loss: 0.0381 - val_loss: 0.1178 - val_f1_score_mod: 0.2412 - val_recall_mod: 0.1438 - val_precision_mod: 0.7618 - val_dur_error: 0.2731 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.11545\n",
      "50/50 - 126s - loss: 0.1321 - f1_score_mod: 0.2475 - recall_mod: 0.1500 - precision_mod: 0.7180 - dur_error: 0.3790 - maestro_dur_loss: 0.0379 - val_loss: 0.1221 - val_f1_score_mod: 0.2421 - val_recall_mod: 0.1450 - val_precision_mod: 0.7500 - val_dur_error: 0.3199 - val_maestro_dur_loss: 0.0320\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.11545 to 0.11500, saving model to models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5\n",
      "50/50 - 127s - loss: 0.1321 - f1_score_mod: 0.2439 - recall_mod: 0.1475 - precision_mod: 0.7104 - dur_error: 0.3806 - maestro_dur_loss: 0.0381 - val_loss: 0.1150 - val_f1_score_mod: 0.2532 - val_recall_mod: 0.1531 - val_precision_mod: 0.7404 - val_dur_error: 0.2500 - val_maestro_dur_loss: 0.0250\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.11500\n",
      "50/50 - 126s - loss: 0.1317 - f1_score_mod: 0.2480 - recall_mod: 0.1504 - precision_mod: 0.7130 - dur_error: 0.3784 - maestro_dur_loss: 0.0378 - val_loss: 0.1171 - val_f1_score_mod: 0.2554 - val_recall_mod: 0.1547 - val_precision_mod: 0.7436 - val_dur_error: 0.2737 - val_maestro_dur_loss: 0.0274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.11500\n",
      "50/50 - 126s - loss: 0.1311 - f1_score_mod: 0.2514 - recall_mod: 0.1526 - precision_mod: 0.7196 - dur_error: 0.3772 - maestro_dur_loss: 0.0377 - val_loss: 0.1152 - val_f1_score_mod: 0.2683 - val_recall_mod: 0.1649 - val_precision_mod: 0.7349 - val_dur_error: 0.2550 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.11500\n",
      "50/50 - 127s - loss: 0.1308 - f1_score_mod: 0.2563 - recall_mod: 0.1562 - precision_mod: 0.7194 - dur_error: 0.3761 - maestro_dur_loss: 0.0376 - val_loss: 0.1167 - val_f1_score_mod: 0.2625 - val_recall_mod: 0.1605 - val_precision_mod: 0.7353 - val_dur_error: 0.2732 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.11500\n",
      "50/50 - 126s - loss: 0.1302 - f1_score_mod: 0.2602 - recall_mod: 0.1591 - precision_mod: 0.7206 - dur_error: 0.3732 - maestro_dur_loss: 0.0373 - val_loss: 0.1286 - val_f1_score_mod: 0.2697 - val_recall_mod: 0.1661 - val_precision_mod: 0.7312 - val_dur_error: 0.3888 - val_maestro_dur_loss: 0.0389\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.11500\n",
      "50/50 - 126s - loss: 0.1302 - f1_score_mod: 0.2591 - recall_mod: 0.1587 - precision_mod: 0.7137 - dur_error: 0.3745 - maestro_dur_loss: 0.0374 - val_loss: 0.1189 - val_f1_score_mod: 0.2530 - val_recall_mod: 0.1518 - val_precision_mod: 0.7707 - val_dur_error: 0.2985 - val_maestro_dur_loss: 0.0298\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.11500\n",
      "50/50 - 126s - loss: 0.1299 - f1_score_mod: 0.2608 - recall_mod: 0.1594 - precision_mod: 0.7245 - dur_error: 0.3737 - maestro_dur_loss: 0.0374 - val_loss: 0.1226 - val_f1_score_mod: 0.2754 - val_recall_mod: 0.1703 - val_precision_mod: 0.7328 - val_dur_error: 0.3351 - val_maestro_dur_loss: 0.0335\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.11500\n",
      "50/50 - 126s - loss: 0.1294 - f1_score_mod: 0.2653 - recall_mod: 0.1630 - precision_mod: 0.7197 - dur_error: 0.3724 - maestro_dur_loss: 0.0372 - val_loss: 0.1154 - val_f1_score_mod: 0.2652 - val_recall_mod: 0.1611 - val_precision_mod: 0.7625 - val_dur_error: 0.2676 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 72/120\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.11500\n",
      "50/50 - 129s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.5)\n",
    "opt = RMSprop(lr = 0.0005, clipnorm = 1.0)\n",
    "model.compile(loss = maestro_loss_wr(0.1), optimizer = opt, metrics = [f1_score_mod, recall_mod, precision_mod, dur_error, maestro_dur_loss_wr(0.1)])\n",
    "mc = ModelCheckpoint('models/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 120, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "    for key, value in history.history.items():\n",
    "        if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "            value.append(np.nan)  \n",
    "df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "df.index.name = 'Epochs'\n",
    "df.to_csv('model_data/best_maestro_model_2_1_512_pt5_lr_5e-4_cn_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is clearly the wrong direction. Let's reduce dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17175, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 128s - loss: 0.2896 - f1_score_mod: 0.0152 - recall_mod: 0.0273 - precision_mod: 0.0643 - dur_error: 1.0074 - maestro_dur_loss: 0.1007 - val_loss: 0.1718 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4849 - val_maestro_dur_loss: 0.0485\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.17175\n",
      "50/50 - 126s - loss: 0.2013 - f1_score_mod: 5.1734e-04 - recall_mod: 2.5937e-04 - precision_mod: 0.1294 - dur_error: 0.6707 - maestro_dur_loss: 0.0671 - val_loss: 0.1850 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6270 - val_maestro_dur_loss: 0.0627\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.17175\n",
      "50/50 - 126s - loss: 0.1880 - f1_score_mod: 0.0017 - recall_mod: 8.6894e-04 - precision_mod: 0.2536 - dur_error: 0.6028 - maestro_dur_loss: 0.0603 - val_loss: 0.1774 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6019 - val_maestro_dur_loss: 0.0602\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17175 to 0.16001, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1795 - f1_score_mod: 0.0108 - recall_mod: 0.0055 - precision_mod: 0.4886 - dur_error: 0.5672 - maestro_dur_loss: 0.0567 - val_loss: 0.1600 - val_f1_score_mod: 0.0117 - val_recall_mod: 0.0059 - val_precision_mod: 0.6613 - val_dur_error: 0.4544 - val_maestro_dur_loss: 0.0454\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16001 to 0.15009, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 127s - loss: 0.1737 - f1_score_mod: 0.0252 - recall_mod: 0.0130 - precision_mod: 0.5624 - dur_error: 0.5470 - maestro_dur_loss: 0.0547 - val_loss: 0.1501 - val_f1_score_mod: 0.0279 - val_recall_mod: 0.0142 - val_precision_mod: 0.7200 - val_dur_error: 0.3966 - val_maestro_dur_loss: 0.0397\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15009\n",
      "50/50 - 125s - loss: 0.1684 - f1_score_mod: 0.0401 - recall_mod: 0.0209 - precision_mod: 0.6003 - dur_error: 0.5225 - maestro_dur_loss: 0.0522 - val_loss: 0.1721 - val_f1_score_mod: 0.0298 - val_recall_mod: 0.0153 - val_precision_mod: 0.7612 - val_dur_error: 0.6211 - val_maestro_dur_loss: 0.0621\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15009 to 0.14494, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1651 - f1_score_mod: 0.0548 - recall_mod: 0.0288 - precision_mod: 0.6282 - dur_error: 0.5118 - maestro_dur_loss: 0.0512 - val_loss: 0.1449 - val_f1_score_mod: 0.0684 - val_recall_mod: 0.0362 - val_precision_mod: 0.6768 - val_dur_error: 0.3765 - val_maestro_dur_loss: 0.0377\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14494 to 0.14269, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1622 - f1_score_mod: 0.0710 - recall_mod: 0.0378 - precision_mod: 0.6441 - dur_error: 0.4998 - maestro_dur_loss: 0.0500 - val_loss: 0.1427 - val_f1_score_mod: 0.0643 - val_recall_mod: 0.0336 - val_precision_mod: 0.7762 - val_dur_error: 0.3670 - val_maestro_dur_loss: 0.0367\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14269 to 0.14035, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1595 - f1_score_mod: 0.0870 - recall_mod: 0.0468 - precision_mod: 0.6596 - dur_error: 0.4879 - maestro_dur_loss: 0.0488 - val_loss: 0.1404 - val_f1_score_mod: 0.0704 - val_recall_mod: 0.0370 - val_precision_mod: 0.7525 - val_dur_error: 0.3569 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14035 to 0.13933, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1565 - f1_score_mod: 0.0969 - recall_mod: 0.0525 - precision_mod: 0.6543 - dur_error: 0.4716 - maestro_dur_loss: 0.0472 - val_loss: 0.1393 - val_f1_score_mod: 0.0876 - val_recall_mod: 0.0467 - val_precision_mod: 0.7315 - val_dur_error: 0.3539 - val_maestro_dur_loss: 0.0354\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13933\n",
      "50/50 - 125s - loss: 0.1545 - f1_score_mod: 0.1079 - recall_mod: 0.0588 - precision_mod: 0.6745 - dur_error: 0.4606 - maestro_dur_loss: 0.0461 - val_loss: 0.1482 - val_f1_score_mod: 0.1061 - val_recall_mod: 0.0576 - val_precision_mod: 0.7003 - val_dur_error: 0.4455 - val_maestro_dur_loss: 0.0445\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.13933 to 0.13507, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1520 - f1_score_mod: 0.1198 - recall_mod: 0.0661 - precision_mod: 0.6727 - dur_error: 0.4469 - maestro_dur_loss: 0.0447 - val_loss: 0.1351 - val_f1_score_mod: 0.1087 - val_recall_mod: 0.0588 - val_precision_mod: 0.7377 - val_dur_error: 0.3309 - val_maestro_dur_loss: 0.0331\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13507\n",
      "50/50 - 125s - loss: 0.1504 - f1_score_mod: 0.1288 - recall_mod: 0.0715 - precision_mod: 0.6685 - dur_error: 0.4401 - maestro_dur_loss: 0.0440 - val_loss: 0.1436 - val_f1_score_mod: 0.1195 - val_recall_mod: 0.0651 - val_precision_mod: 0.7494 - val_dur_error: 0.4260 - val_maestro_dur_loss: 0.0426\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13507 to 0.13421, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1489 - f1_score_mod: 0.1392 - recall_mod: 0.0778 - precision_mod: 0.6745 - dur_error: 0.4340 - maestro_dur_loss: 0.0434 - val_loss: 0.1342 - val_f1_score_mod: 0.1453 - val_recall_mod: 0.0812 - val_precision_mod: 0.7057 - val_dur_error: 0.3397 - val_maestro_dur_loss: 0.0340\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13421\n",
      "50/50 - 126s - loss: 0.1477 - f1_score_mod: 0.1472 - recall_mod: 0.0829 - precision_mod: 0.6732 - dur_error: 0.4312 - maestro_dur_loss: 0.0431 - val_loss: 0.1347 - val_f1_score_mod: 0.1471 - val_recall_mod: 0.0824 - val_precision_mod: 0.6994 - val_dur_error: 0.3514 - val_maestro_dur_loss: 0.0351\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13421\n",
      "50/50 - 128s - loss: 0.1459 - f1_score_mod: 0.1531 - recall_mod: 0.0865 - precision_mod: 0.6799 - dur_error: 0.4215 - maestro_dur_loss: 0.0421 - val_loss: 0.1407 - val_f1_score_mod: 0.1825 - val_recall_mod: 0.1059 - val_precision_mod: 0.6682 - val_dur_error: 0.4147 - val_maestro_dur_loss: 0.0415\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.13421 to 0.13129, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1449 - f1_score_mod: 0.1654 - recall_mod: 0.0944 - precision_mod: 0.6783 - dur_error: 0.4188 - maestro_dur_loss: 0.0419 - val_loss: 0.1313 - val_f1_score_mod: 0.1722 - val_recall_mod: 0.0987 - val_precision_mod: 0.6991 - val_dur_error: 0.3285 - val_maestro_dur_loss: 0.0328\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13129 to 0.12706, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1438 - f1_score_mod: 0.1707 - recall_mod: 0.0977 - precision_mod: 0.6882 - dur_error: 0.4137 - maestro_dur_loss: 0.0414 - val_loss: 0.1271 - val_f1_score_mod: 0.1726 - val_recall_mod: 0.0988 - val_precision_mod: 0.6986 - val_dur_error: 0.2937 - val_maestro_dur_loss: 0.0294\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12706\n",
      "50/50 - 125s - loss: 0.1425 - f1_score_mod: 0.1735 - recall_mod: 0.0998 - precision_mod: 0.6843 - dur_error: 0.4067 - maestro_dur_loss: 0.0407 - val_loss: 0.1283 - val_f1_score_mod: 0.1753 - val_recall_mod: 0.1002 - val_precision_mod: 0.7188 - val_dur_error: 0.3111 - val_maestro_dur_loss: 0.0311\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12706\n",
      "50/50 - 125s - loss: 0.1416 - f1_score_mod: 0.1815 - recall_mod: 0.1049 - precision_mod: 0.6834 - dur_error: 0.4041 - maestro_dur_loss: 0.0404 - val_loss: 0.1300 - val_f1_score_mod: 0.1760 - val_recall_mod: 0.1002 - val_precision_mod: 0.7386 - val_dur_error: 0.3338 - val_maestro_dur_loss: 0.0334\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12706\n",
      "50/50 - 126s - loss: 0.1406 - f1_score_mod: 0.1887 - recall_mod: 0.1097 - precision_mod: 0.6875 - dur_error: 0.4004 - maestro_dur_loss: 0.0400 - val_loss: 0.1322 - val_f1_score_mod: 0.1730 - val_recall_mod: 0.0981 - val_precision_mod: 0.7460 - val_dur_error: 0.3596 - val_maestro_dur_loss: 0.0360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.12706 to 0.12589, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1396 - f1_score_mod: 0.1956 - recall_mod: 0.1142 - precision_mod: 0.6896 - dur_error: 0.3961 - maestro_dur_loss: 0.0396 - val_loss: 0.1259 - val_f1_score_mod: 0.1847 - val_recall_mod: 0.1061 - val_precision_mod: 0.7263 - val_dur_error: 0.3002 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12589\n",
      "50/50 - 126s - loss: 0.1395 - f1_score_mod: 0.2003 - recall_mod: 0.1175 - precision_mod: 0.6908 - dur_error: 0.3993 - maestro_dur_loss: 0.0399 - val_loss: 0.1370 - val_f1_score_mod: 0.1911 - val_recall_mod: 0.1103 - val_precision_mod: 0.7283 - val_dur_error: 0.4177 - val_maestro_dur_loss: 0.0418\n",
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12589\n",
      "50/50 - 125s - loss: 0.1381 - f1_score_mod: 0.2060 - recall_mod: 0.1211 - precision_mod: 0.6971 - dur_error: 0.3934 - maestro_dur_loss: 0.0393 - val_loss: 0.1348 - val_f1_score_mod: 0.2126 - val_recall_mod: 0.1258 - val_precision_mod: 0.6948 - val_dur_error: 0.3992 - val_maestro_dur_loss: 0.0399\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12589 to 0.12273, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1372 - f1_score_mod: 0.2101 - recall_mod: 0.1239 - precision_mod: 0.7026 - dur_error: 0.3884 - maestro_dur_loss: 0.0388 - val_loss: 0.1227 - val_f1_score_mod: 0.2036 - val_recall_mod: 0.1188 - val_precision_mod: 0.7241 - val_dur_error: 0.2847 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.12273 to 0.12250, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 127s - loss: 0.1366 - f1_score_mod: 0.2161 - recall_mod: 0.1280 - precision_mod: 0.7024 - dur_error: 0.3890 - maestro_dur_loss: 0.0389 - val_loss: 0.1225 - val_f1_score_mod: 0.2105 - val_recall_mod: 0.1233 - val_precision_mod: 0.7348 - val_dur_error: 0.2847 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.12250 to 0.12106, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1358 - f1_score_mod: 0.2227 - recall_mod: 0.1326 - precision_mod: 0.7024 - dur_error: 0.3844 - maestro_dur_loss: 0.0384 - val_loss: 0.1211 - val_f1_score_mod: 0.2133 - val_recall_mod: 0.1256 - val_precision_mod: 0.7268 - val_dur_error: 0.2716 - val_maestro_dur_loss: 0.0272\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12106\n",
      "50/50 - 125s - loss: 0.1353 - f1_score_mod: 0.2229 - recall_mod: 0.1328 - precision_mod: 0.7025 - dur_error: 0.3838 - maestro_dur_loss: 0.0384 - val_loss: 0.1234 - val_f1_score_mod: 0.2202 - val_recall_mod: 0.1304 - val_precision_mod: 0.7167 - val_dur_error: 0.3024 - val_maestro_dur_loss: 0.0302\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12106\n",
      "50/50 - 126s - loss: 0.1342 - f1_score_mod: 0.2314 - recall_mod: 0.1384 - precision_mod: 0.7129 - dur_error: 0.3783 - maestro_dur_loss: 0.0378 - val_loss: 0.1260 - val_f1_score_mod: 0.2399 - val_recall_mod: 0.1454 - val_precision_mod: 0.7039 - val_dur_error: 0.3278 - val_maestro_dur_loss: 0.0328\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12106\n",
      "50/50 - 125s - loss: 0.1336 - f1_score_mod: 0.2369 - recall_mod: 0.1427 - precision_mod: 0.7078 - dur_error: 0.3770 - maestro_dur_loss: 0.0377 - val_loss: 0.1239 - val_f1_score_mod: 0.2217 - val_recall_mod: 0.1307 - val_precision_mod: 0.7468 - val_dur_error: 0.3118 - val_maestro_dur_loss: 0.0312\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.12106\n",
      "50/50 - 125s - loss: 0.1329 - f1_score_mod: 0.2392 - recall_mod: 0.1440 - precision_mod: 0.7128 - dur_error: 0.3733 - maestro_dur_loss: 0.0373 - val_loss: 0.1241 - val_f1_score_mod: 0.2388 - val_recall_mod: 0.1442 - val_precision_mod: 0.7075 - val_dur_error: 0.3177 - val_maestro_dur_loss: 0.0318\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.12106 to 0.11888, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1322 - f1_score_mod: 0.2485 - recall_mod: 0.1506 - precision_mod: 0.7152 - dur_error: 0.3730 - maestro_dur_loss: 0.0373 - val_loss: 0.1189 - val_f1_score_mod: 0.2461 - val_recall_mod: 0.1494 - val_precision_mod: 0.7086 - val_dur_error: 0.2682 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11888\n",
      "50/50 - 126s - loss: 0.1314 - f1_score_mod: 0.2489 - recall_mod: 0.1509 - precision_mod: 0.7189 - dur_error: 0.3696 - maestro_dur_loss: 0.0370 - val_loss: 0.1264 - val_f1_score_mod: 0.2466 - val_recall_mod: 0.1486 - val_precision_mod: 0.7350 - val_dur_error: 0.3476 - val_maestro_dur_loss: 0.0348\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11888 to 0.11772, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1309 - f1_score_mod: 0.2575 - recall_mod: 0.1571 - precision_mod: 0.7191 - dur_error: 0.3682 - maestro_dur_loss: 0.0368 - val_loss: 0.1177 - val_f1_score_mod: 0.2445 - val_recall_mod: 0.1468 - val_precision_mod: 0.7444 - val_dur_error: 0.2690 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11772\n",
      "50/50 - 126s - loss: 0.1301 - f1_score_mod: 0.2609 - recall_mod: 0.1594 - precision_mod: 0.7255 - dur_error: 0.3661 - maestro_dur_loss: 0.0366 - val_loss: 0.1256 - val_f1_score_mod: 0.2483 - val_recall_mod: 0.1500 - val_precision_mod: 0.7315 - val_dur_error: 0.3464 - val_maestro_dur_loss: 0.0346\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11772\n",
      "50/50 - 125s - loss: 0.1294 - f1_score_mod: 0.2671 - recall_mod: 0.1641 - precision_mod: 0.7229 - dur_error: 0.3636 - maestro_dur_loss: 0.0364 - val_loss: 0.1264 - val_f1_score_mod: 0.2476 - val_recall_mod: 0.1491 - val_precision_mod: 0.7412 - val_dur_error: 0.3574 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11772\n",
      "50/50 - 126s - loss: 0.1294 - f1_score_mod: 0.2706 - recall_mod: 0.1664 - precision_mod: 0.7291 - dur_error: 0.3674 - maestro_dur_loss: 0.0367 - val_loss: 0.1268 - val_f1_score_mod: 0.2719 - val_recall_mod: 0.1686 - val_precision_mod: 0.7121 - val_dur_error: 0.3614 - val_maestro_dur_loss: 0.0361\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11772\n",
      "50/50 - 126s - loss: 0.1286 - f1_score_mod: 0.2765 - recall_mod: 0.1712 - precision_mod: 0.7253 - dur_error: 0.3628 - maestro_dur_loss: 0.0363 - val_loss: 0.1196 - val_f1_score_mod: 0.2639 - val_recall_mod: 0.1623 - val_precision_mod: 0.7178 - val_dur_error: 0.2921 - val_maestro_dur_loss: 0.0292\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.11772 to 0.11524, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1278 - f1_score_mod: 0.2839 - recall_mod: 0.1767 - precision_mod: 0.7289 - dur_error: 0.3609 - maestro_dur_loss: 0.0361 - val_loss: 0.1152 - val_f1_score_mod: 0.2648 - val_recall_mod: 0.1626 - val_precision_mod: 0.7286 - val_dur_error: 0.2566 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11524\n",
      "50/50 - 126s - loss: 0.1271 - f1_score_mod: 0.2832 - recall_mod: 0.1759 - precision_mod: 0.7337 - dur_error: 0.3581 - maestro_dur_loss: 0.0358 - val_loss: 0.1296 - val_f1_score_mod: 0.2857 - val_recall_mod: 0.1802 - val_precision_mod: 0.7017 - val_dur_error: 0.3973 - val_maestro_dur_loss: 0.0397\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11524\n",
      "50/50 - 126s - loss: 0.1272 - f1_score_mod: 0.2904 - recall_mod: 0.1817 - precision_mod: 0.7291 - dur_error: 0.3617 - maestro_dur_loss: 0.0362 - val_loss: 0.1173 - val_f1_score_mod: 0.2883 - val_recall_mod: 0.1832 - val_precision_mod: 0.6925 - val_dur_error: 0.2770 - val_maestro_dur_loss: 0.0277\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11524\n",
      "50/50 - 126s - loss: 0.1265 - f1_score_mod: 0.2911 - recall_mod: 0.1822 - precision_mod: 0.7297 - dur_error: 0.3596 - maestro_dur_loss: 0.0360 - val_loss: 0.1248 - val_f1_score_mod: 0.2829 - val_recall_mod: 0.1761 - val_precision_mod: 0.7291 - val_dur_error: 0.3571 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.11524 to 0.11449, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1253 - f1_score_mod: 0.2996 - recall_mod: 0.1884 - precision_mod: 0.7356 - dur_error: 0.3539 - maestro_dur_loss: 0.0354 - val_loss: 0.1145 - val_f1_score_mod: 0.2793 - val_recall_mod: 0.1729 - val_precision_mod: 0.7377 - val_dur_error: 0.2580 - val_maestro_dur_loss: 0.0258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11449\n",
      "50/50 - 127s - loss: 0.1248 - f1_score_mod: 0.3061 - recall_mod: 0.1938 - precision_mod: 0.7368 - dur_error: 0.3527 - maestro_dur_loss: 0.0353 - val_loss: 0.1154 - val_f1_score_mod: 0.2956 - val_recall_mod: 0.1861 - val_precision_mod: 0.7265 - val_dur_error: 0.2725 - val_maestro_dur_loss: 0.0272\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11449\n",
      "50/50 - 127s - loss: 0.1242 - f1_score_mod: 0.3068 - recall_mod: 0.1940 - precision_mod: 0.7367 - dur_error: 0.3500 - maestro_dur_loss: 0.0350 - val_loss: 0.1172 - val_f1_score_mod: 0.2907 - val_recall_mod: 0.1820 - val_precision_mod: 0.7333 - val_dur_error: 0.2868 - val_maestro_dur_loss: 0.0287\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11449\n",
      "50/50 - 126s - loss: 0.1235 - f1_score_mod: 0.3110 - recall_mod: 0.1973 - precision_mod: 0.7383 - dur_error: 0.3482 - maestro_dur_loss: 0.0348 - val_loss: 0.1179 - val_f1_score_mod: 0.2933 - val_recall_mod: 0.1838 - val_precision_mod: 0.7361 - val_dur_error: 0.2992 - val_maestro_dur_loss: 0.0299\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11449\n",
      "50/50 - 126s - loss: 0.1231 - f1_score_mod: 0.3157 - recall_mod: 0.2014 - precision_mod: 0.7363 - dur_error: 0.3469 - maestro_dur_loss: 0.0347 - val_loss: 0.1268 - val_f1_score_mod: 0.2962 - val_recall_mod: 0.1861 - val_precision_mod: 0.7342 - val_dur_error: 0.3886 - val_maestro_dur_loss: 0.0389\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.11449 to 0.11447, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1224 - f1_score_mod: 0.3208 - recall_mod: 0.2048 - precision_mod: 0.7448 - dur_error: 0.3457 - maestro_dur_loss: 0.0346 - val_loss: 0.1145 - val_f1_score_mod: 0.2997 - val_recall_mod: 0.1891 - val_precision_mod: 0.7315 - val_dur_error: 0.2654 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11447\n",
      "50/50 - 126s - loss: 0.1221 - f1_score_mod: 0.3250 - recall_mod: 0.2084 - precision_mod: 0.7419 - dur_error: 0.3475 - maestro_dur_loss: 0.0348 - val_loss: 0.1245 - val_f1_score_mod: 0.3158 - val_recall_mod: 0.2038 - val_precision_mod: 0.7116 - val_dur_error: 0.3687 - val_maestro_dur_loss: 0.0369\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11447\n",
      "50/50 - 126s - loss: 0.1215 - f1_score_mod: 0.3327 - recall_mod: 0.2146 - precision_mod: 0.7434 - dur_error: 0.3450 - maestro_dur_loss: 0.0345 - val_loss: 0.1146 - val_f1_score_mod: 0.3199 - val_recall_mod: 0.2064 - val_precision_mod: 0.7191 - val_dur_error: 0.2753 - val_maestro_dur_loss: 0.0275\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.11447 to 0.11243, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1207 - f1_score_mod: 0.3321 - recall_mod: 0.2141 - precision_mod: 0.7434 - dur_error: 0.3390 - maestro_dur_loss: 0.0339 - val_loss: 0.1124 - val_f1_score_mod: 0.3258 - val_recall_mod: 0.2123 - val_precision_mod: 0.7069 - val_dur_error: 0.2581 - val_maestro_dur_loss: 0.0258\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11243\n",
      "50/50 - 125s - loss: 0.1202 - f1_score_mod: 0.3383 - recall_mod: 0.2194 - precision_mod: 0.7442 - dur_error: 0.3397 - maestro_dur_loss: 0.0340 - val_loss: 0.1138 - val_f1_score_mod: 0.3149 - val_recall_mod: 0.2024 - val_precision_mod: 0.7193 - val_dur_error: 0.2706 - val_maestro_dur_loss: 0.0271\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11243\n",
      "50/50 - 126s - loss: 0.1196 - f1_score_mod: 0.3408 - recall_mod: 0.2216 - precision_mod: 0.7412 - dur_error: 0.3364 - maestro_dur_loss: 0.0336 - val_loss: 0.1131 - val_f1_score_mod: 0.3223 - val_recall_mod: 0.2079 - val_precision_mod: 0.7268 - val_dur_error: 0.2677 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11243\n",
      "50/50 - 125s - loss: 0.1191 - f1_score_mod: 0.3460 - recall_mod: 0.2259 - precision_mod: 0.7422 - dur_error: 0.3356 - maestro_dur_loss: 0.0336 - val_loss: 0.1128 - val_f1_score_mod: 0.3174 - val_recall_mod: 0.2025 - val_precision_mod: 0.7436 - val_dur_error: 0.2664 - val_maestro_dur_loss: 0.0266\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11243\n",
      "50/50 - 125s - loss: 0.1191 - f1_score_mod: 0.3480 - recall_mod: 0.2277 - precision_mod: 0.7450 - dur_error: 0.3397 - maestro_dur_loss: 0.0340 - val_loss: 0.1190 - val_f1_score_mod: 0.3301 - val_recall_mod: 0.2149 - val_precision_mod: 0.7209 - val_dur_error: 0.3291 - val_maestro_dur_loss: 0.0329\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.11243 to 0.11143, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1178 - f1_score_mod: 0.3557 - recall_mod: 0.2339 - precision_mod: 0.7465 - dur_error: 0.3316 - maestro_dur_loss: 0.0332 - val_loss: 0.1114 - val_f1_score_mod: 0.3252 - val_recall_mod: 0.2093 - val_precision_mod: 0.7401 - val_dur_error: 0.2559 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.11143 to 0.10980, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1179 - f1_score_mod: 0.3596 - recall_mod: 0.2372 - precision_mod: 0.7465 - dur_error: 0.3378 - maestro_dur_loss: 0.0338 - val_loss: 0.1098 - val_f1_score_mod: 0.3369 - val_recall_mod: 0.2187 - val_precision_mod: 0.7425 - val_dur_error: 0.2459 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.10980\n",
      "50/50 - 126s - loss: 0.1171 - f1_score_mod: 0.3601 - recall_mod: 0.2381 - precision_mod: 0.7440 - dur_error: 0.3340 - maestro_dur_loss: 0.0334 - val_loss: 0.1188 - val_f1_score_mod: 0.3442 - val_recall_mod: 0.2281 - val_precision_mod: 0.7065 - val_dur_error: 0.3310 - val_maestro_dur_loss: 0.0331\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.10980\n",
      "50/50 - 126s - loss: 0.1164 - f1_score_mod: 0.3634 - recall_mod: 0.2406 - precision_mod: 0.7465 - dur_error: 0.3295 - maestro_dur_loss: 0.0329 - val_loss: 0.1119 - val_f1_score_mod: 0.3423 - val_recall_mod: 0.2250 - val_precision_mod: 0.7230 - val_dur_error: 0.2701 - val_maestro_dur_loss: 0.0270\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.10980 to 0.10974, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1157 - f1_score_mod: 0.3684 - recall_mod: 0.2445 - precision_mod: 0.7515 - dur_error: 0.3274 - maestro_dur_loss: 0.0327 - val_loss: 0.1097 - val_f1_score_mod: 0.3394 - val_recall_mod: 0.2208 - val_precision_mod: 0.7433 - val_dur_error: 0.2489 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.10974\n",
      "50/50 - 126s - loss: 0.1151 - f1_score_mod: 0.3736 - recall_mod: 0.2492 - precision_mod: 0.7494 - dur_error: 0.3269 - maestro_dur_loss: 0.0327 - val_loss: 0.1120 - val_f1_score_mod: 0.3455 - val_recall_mod: 0.2269 - val_precision_mod: 0.7353 - val_dur_error: 0.2737 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.10974\n",
      "50/50 - 125s - loss: 0.1150 - f1_score_mod: 0.3738 - recall_mod: 0.2493 - precision_mod: 0.7506 - dur_error: 0.3283 - maestro_dur_loss: 0.0328 - val_loss: 0.1161 - val_f1_score_mod: 0.3597 - val_recall_mod: 0.2420 - val_precision_mod: 0.7076 - val_dur_error: 0.3174 - val_maestro_dur_loss: 0.0317\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.10974\n",
      "50/50 - 126s - loss: 0.1146 - f1_score_mod: 0.3799 - recall_mod: 0.2546 - precision_mod: 0.7513 - dur_error: 0.3283 - maestro_dur_loss: 0.0328 - val_loss: 0.1171 - val_f1_score_mod: 0.3555 - val_recall_mod: 0.2375 - val_precision_mod: 0.7144 - val_dur_error: 0.3270 - val_maestro_dur_loss: 0.0327\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.10974\n",
      "50/50 - 126s - loss: 0.1138 - f1_score_mod: 0.3863 - recall_mod: 0.2607 - precision_mod: 0.7497 - dur_error: 0.3256 - maestro_dur_loss: 0.0326 - val_loss: 0.1146 - val_f1_score_mod: 0.3524 - val_recall_mod: 0.2335 - val_precision_mod: 0.7290 - val_dur_error: 0.3058 - val_maestro_dur_loss: 0.0306\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10974\n",
      "50/50 - 125s - loss: 0.1131 - f1_score_mod: 0.3895 - recall_mod: 0.2628 - precision_mod: 0.7550 - dur_error: 0.3232 - maestro_dur_loss: 0.0323 - val_loss: 0.1143 - val_f1_score_mod: 0.3487 - val_recall_mod: 0.2296 - val_precision_mod: 0.7336 - val_dur_error: 0.3048 - val_maestro_dur_loss: 0.0305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.10974 to 0.10786, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1127 - f1_score_mod: 0.3909 - recall_mod: 0.2642 - precision_mod: 0.7539 - dur_error: 0.3228 - maestro_dur_loss: 0.0323 - val_loss: 0.1079 - val_f1_score_mod: 0.3609 - val_recall_mod: 0.2414 - val_precision_mod: 0.7232 - val_dur_error: 0.2385 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.10786\n",
      "50/50 - 126s - loss: 0.1120 - f1_score_mod: 0.3972 - recall_mod: 0.2703 - precision_mod: 0.7514 - dur_error: 0.3216 - maestro_dur_loss: 0.0322 - val_loss: 0.1175 - val_f1_score_mod: 0.3620 - val_recall_mod: 0.2429 - val_precision_mod: 0.7190 - val_dur_error: 0.3374 - val_maestro_dur_loss: 0.0337\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.10786 to 0.10784, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1116 - f1_score_mod: 0.3973 - recall_mod: 0.2704 - precision_mod: 0.7511 - dur_error: 0.3202 - maestro_dur_loss: 0.0320 - val_loss: 0.1078 - val_f1_score_mod: 0.3625 - val_recall_mod: 0.2426 - val_precision_mod: 0.7275 - val_dur_error: 0.2409 - val_maestro_dur_loss: 0.0241\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.10784 to 0.10703, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1111 - f1_score_mod: 0.4040 - recall_mod: 0.2766 - precision_mod: 0.7520 - dur_error: 0.3195 - maestro_dur_loss: 0.0319 - val_loss: 0.1070 - val_f1_score_mod: 0.3703 - val_recall_mod: 0.2538 - val_precision_mod: 0.6902 - val_dur_error: 0.2295 - val_maestro_dur_loss: 0.0229\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.10703\n",
      "50/50 - 125s - loss: 0.1106 - f1_score_mod: 0.4073 - recall_mod: 0.2793 - precision_mod: 0.7543 - dur_error: 0.3189 - maestro_dur_loss: 0.0319 - val_loss: 0.1098 - val_f1_score_mod: 0.3739 - val_recall_mod: 0.2533 - val_precision_mod: 0.7202 - val_dur_error: 0.2666 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.10703\n",
      "50/50 - 126s - loss: 0.1098 - f1_score_mod: 0.4103 - recall_mod: 0.2821 - precision_mod: 0.7551 - dur_error: 0.3147 - maestro_dur_loss: 0.0315 - val_loss: 0.1083 - val_f1_score_mod: 0.3652 - val_recall_mod: 0.2445 - val_precision_mod: 0.7271 - val_dur_error: 0.2491 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 72/120\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.10703\n",
      "50/50 - 126s - loss: 0.1095 - f1_score_mod: 0.4166 - recall_mod: 0.2872 - precision_mod: 0.7600 - dur_error: 0.3163 - maestro_dur_loss: 0.0316 - val_loss: 0.1124 - val_f1_score_mod: 0.3812 - val_recall_mod: 0.2624 - val_precision_mod: 0.7020 - val_dur_error: 0.2956 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 73/120\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.10703\n",
      "50/50 - 127s - loss: 0.1087 - f1_score_mod: 0.4187 - recall_mod: 0.2897 - precision_mod: 0.7572 - dur_error: 0.3131 - maestro_dur_loss: 0.0313 - val_loss: 0.1111 - val_f1_score_mod: 0.3756 - val_recall_mod: 0.2541 - val_precision_mod: 0.7267 - val_dur_error: 0.2827 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 74/120\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10703\n",
      "50/50 - 127s - loss: 0.1082 - f1_score_mod: 0.4233 - recall_mod: 0.2944 - precision_mod: 0.7547 - dur_error: 0.3122 - maestro_dur_loss: 0.0312 - val_loss: 0.1120 - val_f1_score_mod: 0.3728 - val_recall_mod: 0.2512 - val_precision_mod: 0.7322 - val_dur_error: 0.2949 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 75/120\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10703\n",
      "50/50 - 126s - loss: 0.1076 - f1_score_mod: 0.4257 - recall_mod: 0.2964 - precision_mod: 0.7585 - dur_error: 0.3110 - maestro_dur_loss: 0.0311 - val_loss: 0.1071 - val_f1_score_mod: 0.3872 - val_recall_mod: 0.2687 - val_precision_mod: 0.6985 - val_dur_error: 0.2454 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 76/120\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.10703\n",
      "50/50 - 125s - loss: 0.1072 - f1_score_mod: 0.4295 - recall_mod: 0.3005 - precision_mod: 0.7549 - dur_error: 0.3118 - maestro_dur_loss: 0.0312 - val_loss: 0.1083 - val_f1_score_mod: 0.3895 - val_recall_mod: 0.2705 - val_precision_mod: 0.7016 - val_dur_error: 0.2586 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 77/120\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10703\n",
      "50/50 - 125s - loss: 0.1066 - f1_score_mod: 0.4333 - recall_mod: 0.3035 - precision_mod: 0.7592 - dur_error: 0.3079 - maestro_dur_loss: 0.0308 - val_loss: 0.1116 - val_f1_score_mod: 0.3965 - val_recall_mod: 0.2804 - val_precision_mod: 0.6838 - val_dur_error: 0.2911 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 78/120\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.10703 to 0.10617, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1062 - f1_score_mod: 0.4345 - recall_mod: 0.3054 - precision_mod: 0.7551 - dur_error: 0.3088 - maestro_dur_loss: 0.0309 - val_loss: 0.1062 - val_f1_score_mod: 0.3873 - val_recall_mod: 0.2682 - val_precision_mod: 0.7035 - val_dur_error: 0.2358 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 79/120\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.10617 to 0.10617, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1056 - f1_score_mod: 0.4401 - recall_mod: 0.3109 - precision_mod: 0.7549 - dur_error: 0.3084 - maestro_dur_loss: 0.0308 - val_loss: 0.1062 - val_f1_score_mod: 0.3979 - val_recall_mod: 0.2828 - val_precision_mod: 0.6766 - val_dur_error: 0.2348 - val_maestro_dur_loss: 0.0235\n",
      "Epoch 80/120\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10617\n",
      "50/50 - 125s - loss: 0.1050 - f1_score_mod: 0.4447 - recall_mod: 0.3148 - precision_mod: 0.7588 - dur_error: 0.3053 - maestro_dur_loss: 0.0305 - val_loss: 0.1085 - val_f1_score_mod: 0.4028 - val_recall_mod: 0.2860 - val_precision_mod: 0.6863 - val_dur_error: 0.2643 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 81/120\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10617\n",
      "50/50 - 125s - loss: 0.1043 - f1_score_mod: 0.4485 - recall_mod: 0.3194 - precision_mod: 0.7554 - dur_error: 0.3041 - maestro_dur_loss: 0.0304 - val_loss: 0.1068 - val_f1_score_mod: 0.4009 - val_recall_mod: 0.2822 - val_precision_mod: 0.6990 - val_dur_error: 0.2446 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 82/120\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10617\n",
      "50/50 - 125s - loss: 0.1038 - f1_score_mod: 0.4531 - recall_mod: 0.3235 - precision_mod: 0.7579 - dur_error: 0.3045 - maestro_dur_loss: 0.0304 - val_loss: 0.1113 - val_f1_score_mod: 0.3967 - val_recall_mod: 0.2745 - val_precision_mod: 0.7194 - val_dur_error: 0.2952 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 83/120\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.10617 to 0.10532, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.1035 - f1_score_mod: 0.4593 - recall_mod: 0.3296 - precision_mod: 0.7597 - dur_error: 0.3050 - maestro_dur_loss: 0.0305 - val_loss: 0.1053 - val_f1_score_mod: 0.4062 - val_recall_mod: 0.2879 - val_precision_mod: 0.6928 - val_dur_error: 0.2335 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 84/120\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.10532 to 0.10485, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.1030 - f1_score_mod: 0.4594 - recall_mod: 0.3298 - precision_mod: 0.7590 - dur_error: 0.3042 - maestro_dur_loss: 0.0304 - val_loss: 0.1048 - val_f1_score_mod: 0.4097 - val_recall_mod: 0.2906 - val_precision_mod: 0.6983 - val_dur_error: 0.2297 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 85/120\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10485\n",
      "50/50 - 125s - loss: 0.1022 - f1_score_mod: 0.4653 - recall_mod: 0.3358 - precision_mod: 0.7596 - dur_error: 0.3010 - maestro_dur_loss: 0.0301 - val_loss: 0.1075 - val_f1_score_mod: 0.4116 - val_recall_mod: 0.2932 - val_precision_mod: 0.6922 - val_dur_error: 0.2567 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 86/120\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10485\n",
      "50/50 - 125s - loss: 0.1016 - f1_score_mod: 0.4693 - recall_mod: 0.3406 - precision_mod: 0.7570 - dur_error: 0.2984 - maestro_dur_loss: 0.0298 - val_loss: 0.1053 - val_f1_score_mod: 0.4151 - val_recall_mod: 0.2964 - val_precision_mod: 0.6958 - val_dur_error: 0.2367 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 87/120\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10485\n",
      "50/50 - 125s - loss: 0.1013 - f1_score_mod: 0.4727 - recall_mod: 0.3442 - precision_mod: 0.7575 - dur_error: 0.3003 - maestro_dur_loss: 0.0300 - val_loss: 0.1069 - val_f1_score_mod: 0.4093 - val_recall_mod: 0.2911 - val_precision_mod: 0.6931 - val_dur_error: 0.2505 - val_maestro_dur_loss: 0.0251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/120\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10485\n",
      "50/50 - 126s - loss: 0.1004 - f1_score_mod: 0.4796 - recall_mod: 0.3496 - precision_mod: 0.7647 - dur_error: 0.2976 - maestro_dur_loss: 0.0298 - val_loss: 0.1068 - val_f1_score_mod: 0.4124 - val_recall_mod: 0.2947 - val_precision_mod: 0.6903 - val_dur_error: 0.2527 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 89/120\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10485\n",
      "50/50 - 126s - loss: 0.1001 - f1_score_mod: 0.4769 - recall_mod: 0.3483 - precision_mod: 0.7575 - dur_error: 0.2983 - maestro_dur_loss: 0.0298 - val_loss: 0.1072 - val_f1_score_mod: 0.4224 - val_recall_mod: 0.3051 - val_precision_mod: 0.6908 - val_dur_error: 0.2623 - val_maestro_dur_loss: 0.0262\n",
      "Epoch 90/120\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10485\n",
      "50/50 - 125s - loss: 0.0995 - f1_score_mod: 0.4858 - recall_mod: 0.3563 - precision_mod: 0.7648 - dur_error: 0.2968 - maestro_dur_loss: 0.0297 - val_loss: 0.1150 - val_f1_score_mod: 0.4260 - val_recall_mod: 0.3098 - val_precision_mod: 0.6869 - val_dur_error: 0.3355 - val_maestro_dur_loss: 0.0335\n",
      "Epoch 91/120\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10485\n",
      "50/50 - 126s - loss: 0.0989 - f1_score_mod: 0.4870 - recall_mod: 0.3587 - precision_mod: 0.7595 - dur_error: 0.2943 - maestro_dur_loss: 0.0294 - val_loss: 0.1092 - val_f1_score_mod: 0.4243 - val_recall_mod: 0.3088 - val_precision_mod: 0.6798 - val_dur_error: 0.2765 - val_maestro_dur_loss: 0.0276\n",
      "Epoch 92/120\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10485\n",
      "50/50 - 126s - loss: 0.0984 - f1_score_mod: 0.4950 - recall_mod: 0.3662 - precision_mod: 0.7654 - dur_error: 0.2963 - maestro_dur_loss: 0.0296 - val_loss: 0.1053 - val_f1_score_mod: 0.4295 - val_recall_mod: 0.3143 - val_precision_mod: 0.6799 - val_dur_error: 0.2396 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 93/120\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.10485 to 0.10419, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.0981 - f1_score_mod: 0.4991 - recall_mod: 0.3712 - precision_mod: 0.7630 - dur_error: 0.2957 - maestro_dur_loss: 0.0296 - val_loss: 0.1042 - val_f1_score_mod: 0.4245 - val_recall_mod: 0.3061 - val_precision_mod: 0.6943 - val_dur_error: 0.2257 - val_maestro_dur_loss: 0.0226\n",
      "Epoch 94/120\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.10419\n",
      "50/50 - 126s - loss: 0.0973 - f1_score_mod: 0.5019 - recall_mod: 0.3750 - precision_mod: 0.7597 - dur_error: 0.2948 - maestro_dur_loss: 0.0295 - val_loss: 0.1049 - val_f1_score_mod: 0.4304 - val_recall_mod: 0.3154 - val_precision_mod: 0.6804 - val_dur_error: 0.2389 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 95/120\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10419\n",
      "50/50 - 126s - loss: 0.0967 - f1_score_mod: 0.5066 - recall_mod: 0.3799 - precision_mod: 0.7621 - dur_error: 0.2940 - maestro_dur_loss: 0.0294 - val_loss: 0.1051 - val_f1_score_mod: 0.4343 - val_recall_mod: 0.3197 - val_precision_mod: 0.6791 - val_dur_error: 0.2394 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 96/120\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.10419\n",
      "50/50 - 126s - loss: 0.0959 - f1_score_mod: 0.5119 - recall_mod: 0.3851 - precision_mod: 0.7650 - dur_error: 0.2918 - maestro_dur_loss: 0.0292 - val_loss: 0.1178 - val_f1_score_mod: 0.4400 - val_recall_mod: 0.3313 - val_precision_mod: 0.6566 - val_dur_error: 0.3651 - val_maestro_dur_loss: 0.0365\n",
      "Epoch 97/120\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.10419\n",
      "50/50 - 126s - loss: 0.0955 - f1_score_mod: 0.5158 - recall_mod: 0.3889 - precision_mod: 0.7670 - dur_error: 0.2914 - maestro_dur_loss: 0.0291 - val_loss: 0.1051 - val_f1_score_mod: 0.4394 - val_recall_mod: 0.3298 - val_precision_mod: 0.6605 - val_dur_error: 0.2333 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 98/120\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.10419 to 0.10403, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 125s - loss: 0.0947 - f1_score_mod: 0.5197 - recall_mod: 0.3944 - precision_mod: 0.7633 - dur_error: 0.2891 - maestro_dur_loss: 0.0289 - val_loss: 0.1040 - val_f1_score_mod: 0.4391 - val_recall_mod: 0.3221 - val_precision_mod: 0.6923 - val_dur_error: 0.2248 - val_maestro_dur_loss: 0.0225\n",
      "Epoch 99/120\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.10403\n",
      "50/50 - 126s - loss: 0.0940 - f1_score_mod: 0.5231 - recall_mod: 0.3981 - precision_mod: 0.7640 - dur_error: 0.2872 - maestro_dur_loss: 0.0287 - val_loss: 0.1089 - val_f1_score_mod: 0.4342 - val_recall_mod: 0.3196 - val_precision_mod: 0.6792 - val_dur_error: 0.2787 - val_maestro_dur_loss: 0.0279\n",
      "Epoch 100/120\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.10403 to 0.10362, saving model to models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5\n",
      "50/50 - 126s - loss: 0.0937 - f1_score_mod: 0.5262 - recall_mod: 0.4000 - precision_mod: 0.7700 - dur_error: 0.2878 - maestro_dur_loss: 0.0288 - val_loss: 0.1036 - val_f1_score_mod: 0.4463 - val_recall_mod: 0.3343 - val_precision_mod: 0.6734 - val_dur_error: 0.2235 - val_maestro_dur_loss: 0.0223\n",
      "Epoch 101/120\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.10362\n",
      "50/50 - 125s - loss: 0.0931 - f1_score_mod: 0.5293 - recall_mod: 0.4058 - precision_mod: 0.7618 - dur_error: 0.2859 - maestro_dur_loss: 0.0286 - val_loss: 0.1109 - val_f1_score_mod: 0.4486 - val_recall_mod: 0.3393 - val_precision_mod: 0.6642 - val_dur_error: 0.3010 - val_maestro_dur_loss: 0.0301\n",
      "Epoch 102/120\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.10362\n",
      "50/50 - 129s - loss: 0.0924 - f1_score_mod: 0.5385 - recall_mod: 0.4153 - precision_mod: 0.7671 - dur_error: 0.2857 - maestro_dur_loss: 0.0286 - val_loss: 0.1041 - val_f1_score_mod: 0.4463 - val_recall_mod: 0.3341 - val_precision_mod: 0.6734 - val_dur_error: 0.2264 - val_maestro_dur_loss: 0.0226\n",
      "Epoch 103/120\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.10362\n",
      "50/50 - 125s - loss: 0.0919 - f1_score_mod: 0.5386 - recall_mod: 0.4162 - precision_mod: 0.7647 - dur_error: 0.2848 - maestro_dur_loss: 0.0285 - val_loss: 0.1043 - val_f1_score_mod: 0.4563 - val_recall_mod: 0.3481 - val_precision_mod: 0.6635 - val_dur_error: 0.2275 - val_maestro_dur_loss: 0.0228\n",
      "Epoch 104/120\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.10362\n",
      "50/50 - 125s - loss: 0.0911 - f1_score_mod: 0.5442 - recall_mod: 0.4223 - precision_mod: 0.7656 - dur_error: 0.2831 - maestro_dur_loss: 0.0283 - val_loss: 0.1067 - val_f1_score_mod: 0.4517 - val_recall_mod: 0.3424 - val_precision_mod: 0.6647 - val_dur_error: 0.2541 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 105/120\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.10362\n",
      "50/50 - 126s - loss: 0.0908 - f1_score_mod: 0.5492 - recall_mod: 0.4268 - precision_mod: 0.7711 - dur_error: 0.2834 - maestro_dur_loss: 0.0283 - val_loss: 0.1051 - val_f1_score_mod: 0.4620 - val_recall_mod: 0.3590 - val_precision_mod: 0.6491 - val_dur_error: 0.2393 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 106/120\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.10362\n",
      "50/50 - 125s - loss: 0.0899 - f1_score_mod: 0.5530 - recall_mod: 0.4324 - precision_mod: 0.7675 - dur_error: 0.2815 - maestro_dur_loss: 0.0282 - val_loss: 0.1042 - val_f1_score_mod: 0.4560 - val_recall_mod: 0.3456 - val_precision_mod: 0.6718 - val_dur_error: 0.2291 - val_maestro_dur_loss: 0.0229\n",
      "Epoch 107/120\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.10362\n",
      "50/50 - 125s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.3)\n",
    "opt = RMSprop(lr = 0.0005, clipnorm = 1.0)\n",
    "model.compile(loss = maestro_loss_wr(0.1), optimizer = opt, metrics = [f1_score_mod, recall_mod, precision_mod, dur_error, maestro_dur_loss_wr(0.1)])\n",
    "mc = ModelCheckpoint('models/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 120, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "    for key, value in history.history.items():\n",
    "        if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "            value.append(np.nan)  \n",
    "df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "df.index.name = 'Epochs'\n",
    "df.to_csv('model_data/best_maestro_model_2_1_512_pt3_lr_5e-4_cn_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model yet! Let's go further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18113, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 160s - loss: 0.3111 - f1_score_mod: 0.0080 - recall_mod: 0.0190 - precision_mod: 0.0749 - dur_error: 1.4496 - maestro_dur_loss: 0.1450 - val_loss: 0.1811 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5785 - val_maestro_dur_loss: 0.0579\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18113 to 0.17334, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 140s - loss: 0.2009 - f1_score_mod: 2.7100e-05 - recall_mod: 1.3559e-05 - precision_mod: 0.0200 - dur_error: 0.6975 - maestro_dur_loss: 0.0697 - val_loss: 0.1733 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5090 - val_maestro_dur_loss: 0.0509\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17334 to 0.17147, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 145s - loss: 0.1859 - f1_score_mod: 2.9377e-04 - recall_mod: 1.4711e-04 - precision_mod: 0.1100 - dur_error: 0.5999 - maestro_dur_loss: 0.0600 - val_loss: 0.1715 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5379 - val_maestro_dur_loss: 0.0538\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.17147\n",
      "50/50 - 142s - loss: 0.1755 - f1_score_mod: 0.0081 - recall_mod: 0.0041 - precision_mod: 0.5105 - dur_error: 0.5508 - maestro_dur_loss: 0.0551 - val_loss: 0.1779 - val_f1_score_mod: 0.0027 - val_recall_mod: 0.0013 - val_precision_mod: 0.7159 - val_dur_error: 0.6368 - val_maestro_dur_loss: 0.0637\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.17147 to 0.15001, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 141s - loss: 0.1697 - f1_score_mod: 0.0304 - recall_mod: 0.0157 - precision_mod: 0.6039 - dur_error: 0.5305 - maestro_dur_loss: 0.0530 - val_loss: 0.1500 - val_f1_score_mod: 0.0235 - val_recall_mod: 0.0120 - val_precision_mod: 0.6943 - val_dur_error: 0.4022 - val_maestro_dur_loss: 0.0402\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15001\n",
      "50/50 - 138s - loss: 0.1639 - f1_score_mod: 0.0542 - recall_mod: 0.0284 - precision_mod: 0.6366 - dur_error: 0.5056 - maestro_dur_loss: 0.0506 - val_loss: 0.1574 - val_f1_score_mod: 0.0505 - val_recall_mod: 0.0263 - val_precision_mod: 0.6605 - val_dur_error: 0.4753 - val_maestro_dur_loss: 0.0475\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15001\n",
      "50/50 - 131s - loss: 0.1604 - f1_score_mod: 0.0729 - recall_mod: 0.0390 - precision_mod: 0.6403 - dur_error: 0.4883 - maestro_dur_loss: 0.0488 - val_loss: 0.1510 - val_f1_score_mod: 0.0745 - val_recall_mod: 0.0393 - val_precision_mod: 0.7560 - val_dur_error: 0.4508 - val_maestro_dur_loss: 0.0451\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15001 to 0.14664, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 132s - loss: 0.1577 - f1_score_mod: 0.0889 - recall_mod: 0.0478 - precision_mod: 0.6765 - dur_error: 0.4807 - maestro_dur_loss: 0.0481 - val_loss: 0.1466 - val_f1_score_mod: 0.1066 - val_recall_mod: 0.0581 - val_precision_mod: 0.6781 - val_dur_error: 0.4207 - val_maestro_dur_loss: 0.0421\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14664 to 0.14542, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 134s - loss: 0.1546 - f1_score_mod: 0.1091 - recall_mod: 0.0597 - precision_mod: 0.6670 - dur_error: 0.4657 - maestro_dur_loss: 0.0466 - val_loss: 0.1454 - val_f1_score_mod: 0.1151 - val_recall_mod: 0.0626 - val_precision_mod: 0.7324 - val_dur_error: 0.4251 - val_maestro_dur_loss: 0.0425\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14542 to 0.13733, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 132s - loss: 0.1520 - f1_score_mod: 0.1242 - recall_mod: 0.0685 - precision_mod: 0.6844 - dur_error: 0.4531 - maestro_dur_loss: 0.0453 - val_loss: 0.1373 - val_f1_score_mod: 0.1220 - val_recall_mod: 0.0665 - val_precision_mod: 0.7552 - val_dur_error: 0.3520 - val_maestro_dur_loss: 0.0352\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13733\n",
      "50/50 - 131s - loss: 0.1495 - f1_score_mod: 0.1350 - recall_mod: 0.0751 - precision_mod: 0.6896 - dur_error: 0.4396 - maestro_dur_loss: 0.0440 - val_loss: 0.1430 - val_f1_score_mod: 0.1467 - val_recall_mod: 0.0821 - val_precision_mod: 0.7087 - val_dur_error: 0.4194 - val_maestro_dur_loss: 0.0419\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13733\n",
      "50/50 - 131s - loss: 0.1475 - f1_score_mod: 0.1496 - recall_mod: 0.0842 - precision_mod: 0.6932 - dur_error: 0.4303 - maestro_dur_loss: 0.0430 - val_loss: 0.1430 - val_f1_score_mod: 0.1547 - val_recall_mod: 0.0871 - val_precision_mod: 0.7058 - val_dur_error: 0.4242 - val_maestro_dur_loss: 0.0424\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13733\n",
      "50/50 - 131s - loss: 0.1449 - f1_score_mod: 0.1586 - recall_mod: 0.0898 - precision_mod: 0.6969 - dur_error: 0.4152 - maestro_dur_loss: 0.0415 - val_loss: 0.1399 - val_f1_score_mod: 0.1406 - val_recall_mod: 0.0776 - val_precision_mod: 0.7670 - val_dur_error: 0.4050 - val_maestro_dur_loss: 0.0405\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13733 to 0.12930, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 132s - loss: 0.1433 - f1_score_mod: 0.1724 - recall_mod: 0.0987 - precision_mod: 0.6986 - dur_error: 0.4103 - maestro_dur_loss: 0.0410 - val_loss: 0.1293 - val_f1_score_mod: 0.1612 - val_recall_mod: 0.0906 - val_precision_mod: 0.7493 - val_dur_error: 0.3048 - val_maestro_dur_loss: 0.0305\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12930\n",
      "50/50 - 131s - loss: 0.1417 - f1_score_mod: 0.1795 - recall_mod: 0.1033 - precision_mod: 0.6956 - dur_error: 0.4010 - maestro_dur_loss: 0.0401 - val_loss: 0.1338 - val_f1_score_mod: 0.1585 - val_recall_mod: 0.0886 - val_precision_mod: 0.7677 - val_dur_error: 0.3599 - val_maestro_dur_loss: 0.0360\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12930\n",
      "50/50 - 131s - loss: 0.1406 - f1_score_mod: 0.1872 - recall_mod: 0.1083 - precision_mod: 0.7017 - dur_error: 0.3981 - maestro_dur_loss: 0.0398 - val_loss: 0.1390 - val_f1_score_mod: 0.1856 - val_recall_mod: 0.1071 - val_precision_mod: 0.7115 - val_dur_error: 0.4175 - val_maestro_dur_loss: 0.0418\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.12930\n",
      "50/50 - 131s - loss: 0.1389 - f1_score_mod: 0.1958 - recall_mod: 0.1140 - precision_mod: 0.7085 - dur_error: 0.3889 - maestro_dur_loss: 0.0389 - val_loss: 0.1302 - val_f1_score_mod: 0.1642 - val_recall_mod: 0.0922 - val_precision_mod: 0.7651 - val_dur_error: 0.3320 - val_maestro_dur_loss: 0.0332\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.12930 to 0.12757, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1376 - f1_score_mod: 0.2025 - recall_mod: 0.1187 - precision_mod: 0.7043 - dur_error: 0.3864 - maestro_dur_loss: 0.0386 - val_loss: 0.1276 - val_f1_score_mod: 0.1785 - val_recall_mod: 0.1015 - val_precision_mod: 0.7535 - val_dur_error: 0.3123 - val_maestro_dur_loss: 0.0312\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.12757 to 0.12402, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 132s - loss: 0.1365 - f1_score_mod: 0.2125 - recall_mod: 0.1255 - precision_mod: 0.7056 - dur_error: 0.3803 - maestro_dur_loss: 0.0380 - val_loss: 0.1240 - val_f1_score_mod: 0.2023 - val_recall_mod: 0.1181 - val_precision_mod: 0.7185 - val_dur_error: 0.2854 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.12402 to 0.12207, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1353 - f1_score_mod: 0.2185 - recall_mod: 0.1294 - precision_mod: 0.7109 - dur_error: 0.3753 - maestro_dur_loss: 0.0375 - val_loss: 0.1221 - val_f1_score_mod: 0.2001 - val_recall_mod: 0.1163 - val_precision_mod: 0.7303 - val_dur_error: 0.2731 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12207\n",
      "50/50 - 132s - loss: 0.1346 - f1_score_mod: 0.2255 - recall_mod: 0.1343 - precision_mod: 0.7125 - dur_error: 0.3751 - maestro_dur_loss: 0.0375 - val_loss: 0.1280 - val_f1_score_mod: 0.2223 - val_recall_mod: 0.1322 - val_precision_mod: 0.7102 - val_dur_error: 0.3291 - val_maestro_dur_loss: 0.0329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12207\n",
      "50/50 - 131s - loss: 0.1336 - f1_score_mod: 0.2341 - recall_mod: 0.1403 - precision_mod: 0.7140 - dur_error: 0.3718 - maestro_dur_loss: 0.0372 - val_loss: 0.1244 - val_f1_score_mod: 0.2191 - val_recall_mod: 0.1291 - val_precision_mod: 0.7326 - val_dur_error: 0.3067 - val_maestro_dur_loss: 0.0307\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12207\n",
      "50/50 - 132s - loss: 0.1326 - f1_score_mod: 0.2415 - recall_mod: 0.1456 - precision_mod: 0.7148 - dur_error: 0.3697 - maestro_dur_loss: 0.0370 - val_loss: 0.1277 - val_f1_score_mod: 0.2258 - val_recall_mod: 0.1338 - val_precision_mod: 0.7377 - val_dur_error: 0.3453 - val_maestro_dur_loss: 0.0345\n",
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12207\n",
      "50/50 - 131s - loss: 0.1317 - f1_score_mod: 0.2481 - recall_mod: 0.1502 - precision_mod: 0.7213 - dur_error: 0.3656 - maestro_dur_loss: 0.0366 - val_loss: 0.1266 - val_f1_score_mod: 0.2384 - val_recall_mod: 0.1431 - val_precision_mod: 0.7272 - val_dur_error: 0.3432 - val_maestro_dur_loss: 0.0343\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12207\n",
      "50/50 - 131s - loss: 0.1303 - f1_score_mod: 0.2579 - recall_mod: 0.1572 - precision_mod: 0.7256 - dur_error: 0.3604 - maestro_dur_loss: 0.0360 - val_loss: 0.1259 - val_f1_score_mod: 0.2333 - val_recall_mod: 0.1390 - val_precision_mod: 0.7407 - val_dur_error: 0.3360 - val_maestro_dur_loss: 0.0336\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.12207 to 0.12138, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 132s - loss: 0.1298 - f1_score_mod: 0.2604 - recall_mod: 0.1591 - precision_mod: 0.7237 - dur_error: 0.3601 - maestro_dur_loss: 0.0360 - val_loss: 0.1214 - val_f1_score_mod: 0.2344 - val_recall_mod: 0.1395 - val_precision_mod: 0.7497 - val_dur_error: 0.2935 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12138\n",
      "50/50 - 132s - loss: 0.1288 - f1_score_mod: 0.2666 - recall_mod: 0.1638 - precision_mod: 0.7263 - dur_error: 0.3579 - maestro_dur_loss: 0.0358 - val_loss: 0.1245 - val_f1_score_mod: 0.2555 - val_recall_mod: 0.1559 - val_precision_mod: 0.7155 - val_dur_error: 0.3304 - val_maestro_dur_loss: 0.0330\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.12138 to 0.11719, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1281 - f1_score_mod: 0.2785 - recall_mod: 0.1723 - precision_mod: 0.7360 - dur_error: 0.3565 - maestro_dur_loss: 0.0356 - val_loss: 0.1172 - val_f1_score_mod: 0.2542 - val_recall_mod: 0.1539 - val_precision_mod: 0.7418 - val_dur_error: 0.2663 - val_maestro_dur_loss: 0.0266\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11719\n",
      "50/50 - 132s - loss: 0.1270 - f1_score_mod: 0.2826 - recall_mod: 0.1752 - precision_mod: 0.7368 - dur_error: 0.3499 - maestro_dur_loss: 0.0350 - val_loss: 0.1189 - val_f1_score_mod: 0.2485 - val_recall_mod: 0.1492 - val_precision_mod: 0.7619 - val_dur_error: 0.2851 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11719\n",
      "50/50 - 132s - loss: 0.1264 - f1_score_mod: 0.2865 - recall_mod: 0.1784 - precision_mod: 0.7326 - dur_error: 0.3501 - maestro_dur_loss: 0.0350 - val_loss: 0.1190 - val_f1_score_mod: 0.2661 - val_recall_mod: 0.1624 - val_precision_mod: 0.7510 - val_dur_error: 0.2864 - val_maestro_dur_loss: 0.0286\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11719\n",
      "50/50 - 131s - loss: 0.1253 - f1_score_mod: 0.2928 - recall_mod: 0.1833 - precision_mod: 0.7346 - dur_error: 0.3455 - maestro_dur_loss: 0.0345 - val_loss: 0.1243 - val_f1_score_mod: 0.2678 - val_recall_mod: 0.1651 - val_precision_mod: 0.7215 - val_dur_error: 0.3372 - val_maestro_dur_loss: 0.0337\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11719\n",
      "50/50 - 131s - loss: 0.1246 - f1_score_mod: 0.3004 - recall_mod: 0.1884 - precision_mod: 0.7456 - dur_error: 0.3443 - maestro_dur_loss: 0.0344 - val_loss: 0.1188 - val_f1_score_mod: 0.2806 - val_recall_mod: 0.1746 - val_precision_mod: 0.7281 - val_dur_error: 0.2924 - val_maestro_dur_loss: 0.0292\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.11719 to 0.11572, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1237 - f1_score_mod: 0.3078 - recall_mod: 0.1944 - precision_mod: 0.7454 - dur_error: 0.3422 - maestro_dur_loss: 0.0342 - val_loss: 0.1157 - val_f1_score_mod: 0.2821 - val_recall_mod: 0.1763 - val_precision_mod: 0.7179 - val_dur_error: 0.2658 - val_maestro_dur_loss: 0.0266\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11572 to 0.11473, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 130s - loss: 0.1231 - f1_score_mod: 0.3110 - recall_mod: 0.1973 - precision_mod: 0.7412 - dur_error: 0.3411 - maestro_dur_loss: 0.0341 - val_loss: 0.1147 - val_f1_score_mod: 0.2802 - val_recall_mod: 0.1733 - val_precision_mod: 0.7433 - val_dur_error: 0.2612 - val_maestro_dur_loss: 0.0261\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11473\n",
      "50/50 - 130s - loss: 0.1222 - f1_score_mod: 0.3191 - recall_mod: 0.2034 - precision_mod: 0.7442 - dur_error: 0.3370 - maestro_dur_loss: 0.0337 - val_loss: 0.1197 - val_f1_score_mod: 0.2776 - val_recall_mod: 0.1715 - val_precision_mod: 0.7402 - val_dur_error: 0.3095 - val_maestro_dur_loss: 0.0310\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11473\n",
      "50/50 - 131s - loss: 0.1216 - f1_score_mod: 0.3229 - recall_mod: 0.2060 - precision_mod: 0.7517 - dur_error: 0.3383 - maestro_dur_loss: 0.0338 - val_loss: 0.1182 - val_f1_score_mod: 0.2982 - val_recall_mod: 0.1876 - val_precision_mod: 0.7393 - val_dur_error: 0.3014 - val_maestro_dur_loss: 0.0301\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11473\n",
      "50/50 - 131s - loss: 0.1209 - f1_score_mod: 0.3287 - recall_mod: 0.2109 - precision_mod: 0.7502 - dur_error: 0.3372 - maestro_dur_loss: 0.0337 - val_loss: 0.1209 - val_f1_score_mod: 0.3073 - val_recall_mod: 0.1961 - val_precision_mod: 0.7220 - val_dur_error: 0.3324 - val_maestro_dur_loss: 0.0332\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11473\n",
      "50/50 - 131s - loss: 0.1200 - f1_score_mod: 0.3359 - recall_mod: 0.2169 - precision_mod: 0.7487 - dur_error: 0.3318 - maestro_dur_loss: 0.0332 - val_loss: 0.1184 - val_f1_score_mod: 0.3234 - val_recall_mod: 0.2126 - val_precision_mod: 0.6877 - val_dur_error: 0.3073 - val_maestro_dur_loss: 0.0307\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11473\n",
      "50/50 - 130s - loss: 0.1193 - f1_score_mod: 0.3415 - recall_mod: 0.2211 - precision_mod: 0.7523 - dur_error: 0.3324 - maestro_dur_loss: 0.0332 - val_loss: 0.1205 - val_f1_score_mod: 0.3118 - val_recall_mod: 0.1993 - val_precision_mod: 0.7281 - val_dur_error: 0.3340 - val_maestro_dur_loss: 0.0334\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11473\n",
      "50/50 - 131s - loss: 0.1185 - f1_score_mod: 0.3445 - recall_mod: 0.2240 - precision_mod: 0.7515 - dur_error: 0.3290 - maestro_dur_loss: 0.0329 - val_loss: 0.1156 - val_f1_score_mod: 0.3253 - val_recall_mod: 0.2119 - val_precision_mod: 0.7100 - val_dur_error: 0.2872 - val_maestro_dur_loss: 0.0287\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.11473 to 0.11235, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1178 - f1_score_mod: 0.3525 - recall_mod: 0.2305 - precision_mod: 0.7539 - dur_error: 0.3272 - maestro_dur_loss: 0.0327 - val_loss: 0.1124 - val_f1_score_mod: 0.3244 - val_recall_mod: 0.2103 - val_precision_mod: 0.7198 - val_dur_error: 0.2569 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11235\n",
      "50/50 - 131s - loss: 0.1170 - f1_score_mod: 0.3563 - recall_mod: 0.2337 - precision_mod: 0.7537 - dur_error: 0.3251 - maestro_dur_loss: 0.0325 - val_loss: 0.1128 - val_f1_score_mod: 0.3147 - val_recall_mod: 0.2004 - val_precision_mod: 0.7483 - val_dur_error: 0.2623 - val_maestro_dur_loss: 0.0262\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11235\n",
      "50/50 - 131s - loss: 0.1163 - f1_score_mod: 0.3613 - recall_mod: 0.2378 - precision_mod: 0.7560 - dur_error: 0.3233 - maestro_dur_loss: 0.0323 - val_loss: 0.1137 - val_f1_score_mod: 0.3328 - val_recall_mod: 0.2175 - val_precision_mod: 0.7205 - val_dur_error: 0.2790 - val_maestro_dur_loss: 0.0279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11235\n",
      "50/50 - 130s - loss: 0.1155 - f1_score_mod: 0.3669 - recall_mod: 0.2423 - precision_mod: 0.7585 - dur_error: 0.3206 - maestro_dur_loss: 0.0321 - val_loss: 0.1127 - val_f1_score_mod: 0.3224 - val_recall_mod: 0.2079 - val_precision_mod: 0.7338 - val_dur_error: 0.2644 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.11235 to 0.11098, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1148 - f1_score_mod: 0.3701 - recall_mod: 0.2459 - precision_mod: 0.7529 - dur_error: 0.3194 - maestro_dur_loss: 0.0319 - val_loss: 0.1110 - val_f1_score_mod: 0.3218 - val_recall_mod: 0.2050 - val_precision_mod: 0.7615 - val_dur_error: 0.2556 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11098\n",
      "50/50 - 133s - loss: 0.1144 - f1_score_mod: 0.3734 - recall_mod: 0.2483 - precision_mod: 0.7576 - dur_error: 0.3194 - maestro_dur_loss: 0.0319 - val_loss: 0.1114 - val_f1_score_mod: 0.3414 - val_recall_mod: 0.2250 - val_precision_mod: 0.7220 - val_dur_error: 0.2604 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11098\n",
      "50/50 - 131s - loss: 0.1134 - f1_score_mod: 0.3814 - recall_mod: 0.2549 - precision_mod: 0.7601 - dur_error: 0.3161 - maestro_dur_loss: 0.0316 - val_loss: 0.1149 - val_f1_score_mod: 0.3402 - val_recall_mod: 0.2219 - val_precision_mod: 0.7396 - val_dur_error: 0.2966 - val_maestro_dur_loss: 0.0297\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.11098 to 0.11092, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 132s - loss: 0.1129 - f1_score_mod: 0.3850 - recall_mod: 0.2584 - precision_mod: 0.7575 - dur_error: 0.3144 - maestro_dur_loss: 0.0314 - val_loss: 0.1109 - val_f1_score_mod: 0.3427 - val_recall_mod: 0.2253 - val_precision_mod: 0.7286 - val_dur_error: 0.2599 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11092\n",
      "50/50 - 130s - loss: 0.1122 - f1_score_mod: 0.3894 - recall_mod: 0.2620 - precision_mod: 0.7635 - dur_error: 0.3140 - maestro_dur_loss: 0.0314 - val_loss: 0.1196 - val_f1_score_mod: 0.3548 - val_recall_mod: 0.2381 - val_precision_mod: 0.7078 - val_dur_error: 0.3501 - val_maestro_dur_loss: 0.0350\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11092 to 0.10972, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1114 - f1_score_mod: 0.3951 - recall_mod: 0.2676 - precision_mod: 0.7592 - dur_error: 0.3107 - maestro_dur_loss: 0.0311 - val_loss: 0.1097 - val_f1_score_mod: 0.3453 - val_recall_mod: 0.2273 - val_precision_mod: 0.7321 - val_dur_error: 0.2526 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.10972\n",
      "50/50 - 130s - loss: 0.1108 - f1_score_mod: 0.4012 - recall_mod: 0.2728 - precision_mod: 0.7622 - dur_error: 0.3090 - maestro_dur_loss: 0.0309 - val_loss: 0.1173 - val_f1_score_mod: 0.3553 - val_recall_mod: 0.2356 - val_precision_mod: 0.7334 - val_dur_error: 0.3341 - val_maestro_dur_loss: 0.0334\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.10972 to 0.10871, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1101 - f1_score_mod: 0.4054 - recall_mod: 0.2759 - precision_mod: 0.7685 - dur_error: 0.3085 - maestro_dur_loss: 0.0309 - val_loss: 0.1087 - val_f1_score_mod: 0.3592 - val_recall_mod: 0.2402 - val_precision_mod: 0.7245 - val_dur_error: 0.2470 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.10871\n",
      "50/50 - 131s - loss: 0.1096 - f1_score_mod: 0.4083 - recall_mod: 0.2788 - precision_mod: 0.7645 - dur_error: 0.3085 - maestro_dur_loss: 0.0308 - val_loss: 0.1105 - val_f1_score_mod: 0.3682 - val_recall_mod: 0.2516 - val_precision_mod: 0.6922 - val_dur_error: 0.2663 - val_maestro_dur_loss: 0.0266\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.10871\n",
      "50/50 - 130s - loss: 0.1086 - f1_score_mod: 0.4112 - recall_mod: 0.2821 - precision_mod: 0.7604 - dur_error: 0.3033 - maestro_dur_loss: 0.0303 - val_loss: 0.1135 - val_f1_score_mod: 0.3593 - val_recall_mod: 0.2411 - val_precision_mod: 0.7135 - val_dur_error: 0.2902 - val_maestro_dur_loss: 0.0290\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.10871 to 0.10785, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 130s - loss: 0.1081 - f1_score_mod: 0.4173 - recall_mod: 0.2877 - precision_mod: 0.7618 - dur_error: 0.3025 - maestro_dur_loss: 0.0303 - val_loss: 0.1079 - val_f1_score_mod: 0.3637 - val_recall_mod: 0.2439 - val_precision_mod: 0.7254 - val_dur_error: 0.2433 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.10785\n",
      "50/50 - 131s - loss: 0.1074 - f1_score_mod: 0.4201 - recall_mod: 0.2896 - precision_mod: 0.7672 - dur_error: 0.3035 - maestro_dur_loss: 0.0303 - val_loss: 0.1151 - val_f1_score_mod: 0.3715 - val_recall_mod: 0.2516 - val_precision_mod: 0.7217 - val_dur_error: 0.3188 - val_maestro_dur_loss: 0.0319\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.10785\n",
      "50/50 - 131s - loss: 0.1071 - f1_score_mod: 0.4275 - recall_mod: 0.2969 - precision_mod: 0.7655 - dur_error: 0.3042 - maestro_dur_loss: 0.0304 - val_loss: 0.1112 - val_f1_score_mod: 0.3712 - val_recall_mod: 0.2512 - val_precision_mod: 0.7217 - val_dur_error: 0.2841 - val_maestro_dur_loss: 0.0284\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.10785 to 0.10669, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 130s - loss: 0.1061 - f1_score_mod: 0.4297 - recall_mod: 0.2987 - precision_mod: 0.7679 - dur_error: 0.2997 - maestro_dur_loss: 0.0300 - val_loss: 0.1067 - val_f1_score_mod: 0.3754 - val_recall_mod: 0.2544 - val_precision_mod: 0.7268 - val_dur_error: 0.2396 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.10669 to 0.10659, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1054 - f1_score_mod: 0.4362 - recall_mod: 0.3053 - precision_mod: 0.7659 - dur_error: 0.2987 - maestro_dur_loss: 0.0299 - val_loss: 0.1066 - val_f1_score_mod: 0.3807 - val_recall_mod: 0.2624 - val_precision_mod: 0.7004 - val_dur_error: 0.2367 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.10659\n",
      "50/50 - 130s - loss: 0.1052 - f1_score_mod: 0.4414 - recall_mod: 0.3100 - precision_mod: 0.7678 - dur_error: 0.3004 - maestro_dur_loss: 0.0300 - val_loss: 0.1127 - val_f1_score_mod: 0.3869 - val_recall_mod: 0.2682 - val_precision_mod: 0.7016 - val_dur_error: 0.3036 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.10659\n",
      "50/50 - 131s - loss: 0.1043 - f1_score_mod: 0.4436 - recall_mod: 0.3119 - precision_mod: 0.7711 - dur_error: 0.2974 - maestro_dur_loss: 0.0297 - val_loss: 0.1110 - val_f1_score_mod: 0.3921 - val_recall_mod: 0.2736 - val_precision_mod: 0.6971 - val_dur_error: 0.2814 - val_maestro_dur_loss: 0.0281\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.10659\n",
      "50/50 - 130s - loss: 0.1036 - f1_score_mod: 0.4464 - recall_mod: 0.3157 - precision_mod: 0.7641 - dur_error: 0.2953 - maestro_dur_loss: 0.0295 - val_loss: 0.1140 - val_f1_score_mod: 0.3782 - val_recall_mod: 0.2575 - val_precision_mod: 0.7202 - val_dur_error: 0.3190 - val_maestro_dur_loss: 0.0319\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.10659\n",
      "50/50 - 130s - loss: 0.1029 - f1_score_mod: 0.4499 - recall_mod: 0.3186 - precision_mod: 0.7688 - dur_error: 0.2924 - maestro_dur_loss: 0.0292 - val_loss: 0.1069 - val_f1_score_mod: 0.3930 - val_recall_mod: 0.2736 - val_precision_mod: 0.7047 - val_dur_error: 0.2488 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.10659\n",
      "50/50 - 131s - loss: 0.1023 - f1_score_mod: 0.4562 - recall_mod: 0.3246 - precision_mod: 0.7699 - dur_error: 0.2906 - maestro_dur_loss: 0.0291 - val_loss: 0.1074 - val_f1_score_mod: 0.3931 - val_recall_mod: 0.2731 - val_precision_mod: 0.7083 - val_dur_error: 0.2552 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10659\n",
      "50/50 - 131s - loss: 0.1012 - f1_score_mod: 0.4628 - recall_mod: 0.3309 - precision_mod: 0.7728 - dur_error: 0.2878 - maestro_dur_loss: 0.0288 - val_loss: 0.1127 - val_f1_score_mod: 0.3997 - val_recall_mod: 0.2825 - val_precision_mod: 0.6923 - val_dur_error: 0.3102 - val_maestro_dur_loss: 0.0310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.10659\n",
      "50/50 - 131s - loss: 0.1008 - f1_score_mod: 0.4674 - recall_mod: 0.3358 - precision_mod: 0.7707 - dur_error: 0.2881 - maestro_dur_loss: 0.0288 - val_loss: 0.1130 - val_f1_score_mod: 0.4016 - val_recall_mod: 0.2820 - val_precision_mod: 0.7021 - val_dur_error: 0.3150 - val_maestro_dur_loss: 0.0315\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.10659\n",
      "50/50 - 131s - loss: 0.1002 - f1_score_mod: 0.4723 - recall_mod: 0.3411 - precision_mod: 0.7690 - dur_error: 0.2886 - maestro_dur_loss: 0.0289 - val_loss: 0.1149 - val_f1_score_mod: 0.4023 - val_recall_mod: 0.2840 - val_precision_mod: 0.6982 - val_dur_error: 0.3365 - val_maestro_dur_loss: 0.0337\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.10659\n",
      "50/50 - 131s - loss: 0.0994 - f1_score_mod: 0.4766 - recall_mod: 0.3454 - precision_mod: 0.7702 - dur_error: 0.2873 - maestro_dur_loss: 0.0287 - val_loss: 0.1093 - val_f1_score_mod: 0.4047 - val_recall_mod: 0.2866 - val_precision_mod: 0.6962 - val_dur_error: 0.2811 - val_maestro_dur_loss: 0.0281\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.10659 to 0.10610, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.0990 - f1_score_mod: 0.4819 - recall_mod: 0.3514 - precision_mod: 0.7691 - dur_error: 0.2860 - maestro_dur_loss: 0.0286 - val_loss: 0.1061 - val_f1_score_mod: 0.4116 - val_recall_mod: 0.2938 - val_precision_mod: 0.6920 - val_dur_error: 0.2428 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.10610 to 0.10542, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.0982 - f1_score_mod: 0.4839 - recall_mod: 0.3529 - precision_mod: 0.7716 - dur_error: 0.2846 - maestro_dur_loss: 0.0285 - val_loss: 0.1054 - val_f1_score_mod: 0.4116 - val_recall_mod: 0.2926 - val_precision_mod: 0.6981 - val_dur_error: 0.2415 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.10542\n",
      "50/50 - 130s - loss: 0.0973 - f1_score_mod: 0.4899 - recall_mod: 0.3600 - precision_mod: 0.7690 - dur_error: 0.2831 - maestro_dur_loss: 0.0283 - val_loss: 0.1107 - val_f1_score_mod: 0.4180 - val_recall_mod: 0.3013 - val_precision_mod: 0.6859 - val_dur_error: 0.2960 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 72/120\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.10542 to 0.10525, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.0968 - f1_score_mod: 0.4932 - recall_mod: 0.3619 - precision_mod: 0.7755 - dur_error: 0.2815 - maestro_dur_loss: 0.0282 - val_loss: 0.1053 - val_f1_score_mod: 0.4179 - val_recall_mod: 0.3020 - val_precision_mod: 0.6827 - val_dur_error: 0.2398 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 73/120\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.10525\n",
      "50/50 - 131s - loss: 0.0963 - f1_score_mod: 0.5002 - recall_mod: 0.3700 - precision_mod: 0.7735 - dur_error: 0.2815 - maestro_dur_loss: 0.0282 - val_loss: 0.1056 - val_f1_score_mod: 0.4200 - val_recall_mod: 0.3029 - val_precision_mod: 0.6877 - val_dur_error: 0.2423 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 74/120\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10525\n",
      "50/50 - 130s - loss: 0.0953 - f1_score_mod: 0.5053 - recall_mod: 0.3753 - precision_mod: 0.7749 - dur_error: 0.2793 - maestro_dur_loss: 0.0279 - val_loss: 0.1061 - val_f1_score_mod: 0.4229 - val_recall_mod: 0.3070 - val_precision_mod: 0.6827 - val_dur_error: 0.2444 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 75/120\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10525\n",
      "50/50 - 132s - loss: 0.0947 - f1_score_mod: 0.5099 - recall_mod: 0.3806 - precision_mod: 0.7736 - dur_error: 0.2782 - maestro_dur_loss: 0.0278 - val_loss: 0.1153 - val_f1_score_mod: 0.4337 - val_recall_mod: 0.3252 - val_precision_mod: 0.6536 - val_dur_error: 0.3401 - val_maestro_dur_loss: 0.0340\n",
      "Epoch 76/120\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.10525 to 0.10458, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 132s - loss: 0.0940 - f1_score_mod: 0.5123 - recall_mod: 0.3841 - precision_mod: 0.7709 - dur_error: 0.2759 - maestro_dur_loss: 0.0276 - val_loss: 0.1046 - val_f1_score_mod: 0.4249 - val_recall_mod: 0.3066 - val_precision_mod: 0.6971 - val_dur_error: 0.2337 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 77/120\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10458\n",
      "50/50 - 131s - loss: 0.0934 - f1_score_mod: 0.5177 - recall_mod: 0.3887 - precision_mod: 0.7768 - dur_error: 0.2755 - maestro_dur_loss: 0.0275 - val_loss: 0.1066 - val_f1_score_mod: 0.4352 - val_recall_mod: 0.3241 - val_precision_mod: 0.6646 - val_dur_error: 0.2528 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 78/120\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10458\n",
      "50/50 - 131s - loss: 0.0922 - f1_score_mod: 0.5236 - recall_mod: 0.3944 - precision_mod: 0.7797 - dur_error: 0.2715 - maestro_dur_loss: 0.0271 - val_loss: 0.1115 - val_f1_score_mod: 0.4370 - val_recall_mod: 0.3254 - val_precision_mod: 0.6680 - val_dur_error: 0.3040 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 79/120\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.10458\n",
      "50/50 - 130s - loss: 0.0921 - f1_score_mod: 0.5248 - recall_mod: 0.3974 - precision_mod: 0.7741 - dur_error: 0.2744 - maestro_dur_loss: 0.0274 - val_loss: 0.1046 - val_f1_score_mod: 0.4412 - val_recall_mod: 0.3276 - val_precision_mod: 0.6786 - val_dur_error: 0.2362 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 80/120\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10458\n",
      "50/50 - 130s - loss: 0.0912 - f1_score_mod: 0.5361 - recall_mod: 0.4087 - precision_mod: 0.7801 - dur_error: 0.2735 - maestro_dur_loss: 0.0274 - val_loss: 0.1050 - val_f1_score_mod: 0.4403 - val_recall_mod: 0.3283 - val_precision_mod: 0.6699 - val_dur_error: 0.2358 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 81/120\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.10458 to 0.10439, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.0906 - f1_score_mod: 0.5386 - recall_mod: 0.4125 - precision_mod: 0.7775 - dur_error: 0.2717 - maestro_dur_loss: 0.0272 - val_loss: 0.1044 - val_f1_score_mod: 0.4377 - val_recall_mod: 0.3261 - val_precision_mod: 0.6680 - val_dur_error: 0.2288 - val_maestro_dur_loss: 0.0229\n",
      "Epoch 82/120\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10439\n",
      "50/50 - 130s - loss: 0.0897 - f1_score_mod: 0.5435 - recall_mod: 0.4174 - precision_mod: 0.7798 - dur_error: 0.2683 - maestro_dur_loss: 0.0268 - val_loss: 0.1060 - val_f1_score_mod: 0.4441 - val_recall_mod: 0.3321 - val_precision_mod: 0.6729 - val_dur_error: 0.2486 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 83/120\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10439\n",
      "50/50 - 131s - loss: 0.0891 - f1_score_mod: 0.5494 - recall_mod: 0.4243 - precision_mod: 0.7802 - dur_error: 0.2684 - maestro_dur_loss: 0.0268 - val_loss: 0.1046 - val_f1_score_mod: 0.4504 - val_recall_mod: 0.3421 - val_precision_mod: 0.6604 - val_dur_error: 0.2320 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 84/120\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10439\n",
      "50/50 - 131s - loss: 0.0885 - f1_score_mod: 0.5526 - recall_mod: 0.4279 - precision_mod: 0.7809 - dur_error: 0.2687 - maestro_dur_loss: 0.0269 - val_loss: 0.1094 - val_f1_score_mod: 0.4495 - val_recall_mod: 0.3393 - val_precision_mod: 0.6676 - val_dur_error: 0.2852 - val_maestro_dur_loss: 0.0285\n",
      "Epoch 85/120\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10439\n",
      "50/50 - 131s - loss: 0.0876 - f1_score_mod: 0.5567 - recall_mod: 0.4321 - precision_mod: 0.7832 - dur_error: 0.2655 - maestro_dur_loss: 0.0265 - val_loss: 0.1059 - val_f1_score_mod: 0.4497 - val_recall_mod: 0.3375 - val_precision_mod: 0.6760 - val_dur_error: 0.2462 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 86/120\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10439 to 0.10397, saving model to models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5\n",
      "50/50 - 130s - loss: 0.0872 - f1_score_mod: 0.5617 - recall_mod: 0.4397 - precision_mod: 0.7782 - dur_error: 0.2654 - maestro_dur_loss: 0.0265 - val_loss: 0.1040 - val_f1_score_mod: 0.4571 - val_recall_mod: 0.3485 - val_precision_mod: 0.6661 - val_dur_error: 0.2303 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 87/120\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10397\n",
      "50/50 - 131s - loss: 0.0863 - f1_score_mod: 0.5678 - recall_mod: 0.4456 - precision_mod: 0.7835 - dur_error: 0.2629 - maestro_dur_loss: 0.0263 - val_loss: 0.1050 - val_f1_score_mod: 0.4568 - val_recall_mod: 0.3470 - val_precision_mod: 0.6698 - val_dur_error: 0.2357 - val_maestro_dur_loss: 0.0236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/120\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10397\n",
      "50/50 - 131s - loss: 0.0856 - f1_score_mod: 0.5733 - recall_mod: 0.4519 - precision_mod: 0.7852 - dur_error: 0.2621 - maestro_dur_loss: 0.0262 - val_loss: 0.1097 - val_f1_score_mod: 0.4637 - val_recall_mod: 0.3636 - val_precision_mod: 0.6412 - val_dur_error: 0.2839 - val_maestro_dur_loss: 0.0284\n",
      "Epoch 89/120\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10397\n",
      "50/50 - 130s - loss: 0.0851 - f1_score_mod: 0.5767 - recall_mod: 0.4573 - precision_mod: 0.7815 - dur_error: 0.2625 - maestro_dur_loss: 0.0263 - val_loss: 0.1091 - val_f1_score_mod: 0.4601 - val_recall_mod: 0.3581 - val_precision_mod: 0.6453 - val_dur_error: 0.2738 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 90/120\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10397\n",
      "50/50 - 130s - loss: 0.0840 - f1_score_mod: 0.5827 - recall_mod: 0.4635 - precision_mod: 0.7853 - dur_error: 0.2591 - maestro_dur_loss: 0.0259 - val_loss: 0.1041 - val_f1_score_mod: 0.4623 - val_recall_mod: 0.3580 - val_precision_mod: 0.6536 - val_dur_error: 0.2231 - val_maestro_dur_loss: 0.0223\n",
      "Epoch 91/120\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10397\n",
      "50/50 - 131s - loss: 0.0836 - f1_score_mod: 0.5855 - recall_mod: 0.4662 - precision_mod: 0.7877 - dur_error: 0.2594 - maestro_dur_loss: 0.0259 - val_loss: 0.1052 - val_f1_score_mod: 0.4669 - val_recall_mod: 0.3628 - val_precision_mod: 0.6562 - val_dur_error: 0.2294 - val_maestro_dur_loss: 0.0229\n",
      "Epoch 92/120\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10397\n",
      "50/50 - 132s - loss: 0.0826 - f1_score_mod: 0.5956 - recall_mod: 0.4788 - precision_mod: 0.7883 - dur_error: 0.2589 - maestro_dur_loss: 0.0259 - val_loss: 0.1060 - val_f1_score_mod: 0.4719 - val_recall_mod: 0.3726 - val_precision_mod: 0.6448 - val_dur_error: 0.2363 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 93/120\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.10397\n",
      "50/50 - 130s - loss: 0.0822 - f1_score_mod: 0.5967 - recall_mod: 0.4806 - precision_mod: 0.7872 - dur_error: 0.2573 - maestro_dur_loss: 0.0257 - val_loss: 0.1092 - val_f1_score_mod: 0.4684 - val_recall_mod: 0.3605 - val_precision_mod: 0.6711 - val_dur_error: 0.2779 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 94/120\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.10397\n",
      "50/50 - 130s - loss: 0.0815 - f1_score_mod: 0.5996 - recall_mod: 0.4831 - precision_mod: 0.7910 - dur_error: 0.2584 - maestro_dur_loss: 0.0258 - val_loss: 0.1087 - val_f1_score_mod: 0.4768 - val_recall_mod: 0.3821 - val_precision_mod: 0.6352 - val_dur_error: 0.2592 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 95/120\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10397\n",
      "50/50 - 130s - loss: 0.0805 - f1_score_mod: 0.6077 - recall_mod: 0.4942 - precision_mod: 0.7897 - dur_error: 0.2543 - maestro_dur_loss: 0.0254 - val_loss: 0.1054 - val_f1_score_mod: 0.4782 - val_recall_mod: 0.3777 - val_precision_mod: 0.6527 - val_dur_error: 0.2362 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 96/120\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.10397\n",
      "50/50 - 130s - loss: 0.0802 - f1_score_mod: 0.6117 - recall_mod: 0.4986 - precision_mod: 0.7920 - dur_error: 0.2569 - maestro_dur_loss: 0.0257 - val_loss: 0.1050 - val_f1_score_mod: 0.4743 - val_recall_mod: 0.3744 - val_precision_mod: 0.6486 - val_dur_error: 0.2267 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 97/120\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.10397\n",
      "50/50 - 131s - loss: 0.0794 - f1_score_mod: 0.6163 - recall_mod: 0.5041 - precision_mod: 0.7935 - dur_error: 0.2558 - maestro_dur_loss: 0.0256 - val_loss: 0.1094 - val_f1_score_mod: 0.4843 - val_recall_mod: 0.3931 - val_precision_mod: 0.6318 - val_dur_error: 0.2673 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 98/120\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.10397\n",
      "50/50 - 130s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.2)\n",
    "opt = RMSprop(lr = 0.0005, clipnorm = 1.0)\n",
    "model.compile(loss = maestro_loss_wr(0.1), optimizer = opt, metrics = [f1_score_mod, recall_mod, precision_mod, dur_error, maestro_dur_loss_wr(0.1)])\n",
    "mc = ModelCheckpoint('models/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 120, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "    for key, value in history.history.items():\n",
    "        if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "            value.append(np.nan)  \n",
    "df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "df.index.name = 'Epochs'\n",
    "df.to_csv('model_data/best_maestro_model_2_1_512_pt2_lr_5e-4_cn_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model for this one was about the same as the last time's. So let's do one final run in between (dropout_rate = 0.25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18919, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 170s - loss: 0.2849 - f1_score_mod: 0.0085 - recall_mod: 0.0178 - precision_mod: 0.0764 - dur_error: 1.0273 - maestro_dur_loss: 0.1027 - val_loss: 0.1892 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6566 - val_maestro_dur_loss: 0.0657\n",
      "Epoch 2/120\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18919 to 0.16723, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 154s - loss: 0.1978 - f1_score_mod: 4.9901e-04 - recall_mod: 2.5005e-04 - precision_mod: 0.1845 - dur_error: 0.6612 - maestro_dur_loss: 0.0661 - val_loss: 0.1672 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4796 - val_maestro_dur_loss: 0.0480\n",
      "Epoch 3/120\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16723\n",
      "50/50 - 145s - loss: 0.1839 - f1_score_mod: 0.0042 - recall_mod: 0.0021 - precision_mod: 0.3578 - dur_error: 0.5934 - maestro_dur_loss: 0.0593 - val_loss: 0.1771 - val_f1_score_mod: 0.0032 - val_recall_mod: 0.0016 - val_precision_mod: 0.5933 - val_dur_error: 0.6197 - val_maestro_dur_loss: 0.0620\n",
      "Epoch 4/120\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16723 to 0.15817, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 154s - loss: 0.1754 - f1_score_mod: 0.0160 - recall_mod: 0.0082 - precision_mod: 0.5121 - dur_error: 0.5552 - maestro_dur_loss: 0.0555 - val_loss: 0.1582 - val_f1_score_mod: 0.0087 - val_recall_mod: 0.0044 - val_precision_mod: 0.8131 - val_dur_error: 0.4651 - val_maestro_dur_loss: 0.0465\n",
      "Epoch 5/120\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15817 to 0.14860, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 155s - loss: 0.1694 - f1_score_mod: 0.0348 - recall_mod: 0.0180 - precision_mod: 0.5723 - dur_error: 0.5293 - maestro_dur_loss: 0.0529 - val_loss: 0.1486 - val_f1_score_mod: 0.0236 - val_recall_mod: 0.0120 - val_precision_mod: 0.7892 - val_dur_error: 0.3946 - val_maestro_dur_loss: 0.0395\n",
      "Epoch 6/120\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14860 to 0.14563, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 147s - loss: 0.1653 - f1_score_mod: 0.0525 - recall_mod: 0.0276 - precision_mod: 0.6031 - dur_error: 0.5135 - maestro_dur_loss: 0.0513 - val_loss: 0.1456 - val_f1_score_mod: 0.0400 - val_recall_mod: 0.0206 - val_precision_mod: 0.8161 - val_dur_error: 0.3841 - val_maestro_dur_loss: 0.0384\n",
      "Epoch 7/120\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14563\n",
      "50/50 - 136s - loss: 0.1618 - f1_score_mod: 0.0688 - recall_mod: 0.0366 - precision_mod: 0.6192 - dur_error: 0.4971 - maestro_dur_loss: 0.0497 - val_loss: 0.1490 - val_f1_score_mod: 0.0759 - val_recall_mod: 0.0402 - val_precision_mod: 0.7081 - val_dur_error: 0.4283 - val_maestro_dur_loss: 0.0428\n",
      "Epoch 8/120\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14563\n",
      "50/50 - 157s - loss: 0.1596 - f1_score_mod: 0.0870 - recall_mod: 0.0469 - precision_mod: 0.6388 - dur_error: 0.4921 - maestro_dur_loss: 0.0492 - val_loss: 0.1544 - val_f1_score_mod: 0.0878 - val_recall_mod: 0.0471 - val_precision_mod: 0.6855 - val_dur_error: 0.5013 - val_maestro_dur_loss: 0.0501\n",
      "Epoch 9/120\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14563 to 0.14327, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 155s - loss: 0.1558 - f1_score_mod: 0.0991 - recall_mod: 0.0539 - precision_mod: 0.6490 - dur_error: 0.4685 - maestro_dur_loss: 0.0468 - val_loss: 0.1433 - val_f1_score_mod: 0.0910 - val_recall_mod: 0.0486 - val_precision_mod: 0.7492 - val_dur_error: 0.4007 - val_maestro_dur_loss: 0.0401\n",
      "Epoch 10/120\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14327\n",
      "50/50 - 141s - loss: 0.1540 - f1_score_mod: 0.1141 - recall_mod: 0.0627 - precision_mod: 0.6668 - dur_error: 0.4633 - maestro_dur_loss: 0.0463 - val_loss: 0.1482 - val_f1_score_mod: 0.1089 - val_recall_mod: 0.0589 - val_precision_mod: 0.7532 - val_dur_error: 0.4576 - val_maestro_dur_loss: 0.0458\n",
      "Epoch 11/120\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14327\n",
      "50/50 - 143s - loss: 0.1512 - f1_score_mod: 0.1278 - recall_mod: 0.0709 - precision_mod: 0.6670 - dur_error: 0.4466 - maestro_dur_loss: 0.0447 - val_loss: 0.1533 - val_f1_score_mod: 0.1234 - val_recall_mod: 0.0677 - val_precision_mod: 0.7205 - val_dur_error: 0.5088 - val_maestro_dur_loss: 0.0509\n",
      "Epoch 12/120\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.14327 to 0.13834, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 153s - loss: 0.1487 - f1_score_mod: 0.1409 - recall_mod: 0.0788 - precision_mod: 0.6773 - dur_error: 0.4308 - maestro_dur_loss: 0.0431 - val_loss: 0.1383 - val_f1_score_mod: 0.1338 - val_recall_mod: 0.0741 - val_precision_mod: 0.7150 - val_dur_error: 0.3789 - val_maestro_dur_loss: 0.0379\n",
      "Epoch 13/120\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.13834 to 0.13131, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 157s - loss: 0.1474 - f1_score_mod: 0.1495 - recall_mod: 0.0844 - precision_mod: 0.6724 - dur_error: 0.4283 - maestro_dur_loss: 0.0428 - val_loss: 0.1313 - val_f1_score_mod: 0.1415 - val_recall_mod: 0.0785 - val_precision_mod: 0.7333 - val_dur_error: 0.3172 - val_maestro_dur_loss: 0.0317\n",
      "Epoch 14/120\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13131\n",
      "50/50 - 152s - loss: 0.1456 - f1_score_mod: 0.1590 - recall_mod: 0.0904 - precision_mod: 0.6804 - dur_error: 0.4191 - maestro_dur_loss: 0.0419 - val_loss: 0.1368 - val_f1_score_mod: 0.1543 - val_recall_mod: 0.0869 - val_precision_mod: 0.7008 - val_dur_error: 0.3745 - val_maestro_dur_loss: 0.0374\n",
      "Epoch 15/120\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13131\n",
      "50/50 - 147s - loss: 0.1443 - f1_score_mod: 0.1675 - recall_mod: 0.0958 - precision_mod: 0.6819 - dur_error: 0.4143 - maestro_dur_loss: 0.0414 - val_loss: 0.1344 - val_f1_score_mod: 0.1703 - val_recall_mod: 0.0973 - val_precision_mod: 0.6930 - val_dur_error: 0.3621 - val_maestro_dur_loss: 0.0362\n",
      "Epoch 16/120\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13131 to 0.12797, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 145s - loss: 0.1430 - f1_score_mod: 0.1757 - recall_mod: 0.1010 - precision_mod: 0.6871 - dur_error: 0.4098 - maestro_dur_loss: 0.0410 - val_loss: 0.1280 - val_f1_score_mod: 0.1622 - val_recall_mod: 0.0914 - val_precision_mod: 0.7379 - val_dur_error: 0.2989 - val_maestro_dur_loss: 0.0299\n",
      "Epoch 17/120\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.12797\n",
      "50/50 - 142s - loss: 0.1425 - f1_score_mod: 0.1802 - recall_mod: 0.1039 - precision_mod: 0.6878 - dur_error: 0.4090 - maestro_dur_loss: 0.0409 - val_loss: 0.1290 - val_f1_score_mod: 0.1743 - val_recall_mod: 0.0995 - val_precision_mod: 0.7183 - val_dur_error: 0.3198 - val_maestro_dur_loss: 0.0320\n",
      "Epoch 18/120\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12797\n",
      "50/50 - 153s - loss: 0.1437 - f1_score_mod: 0.1854 - recall_mod: 0.1075 - precision_mod: 0.6831 - dur_error: 0.4257 - maestro_dur_loss: 0.0426 - val_loss: 0.1332 - val_f1_score_mod: 0.1792 - val_recall_mod: 0.1025 - val_precision_mod: 0.7256 - val_dur_error: 0.3630 - val_maestro_dur_loss: 0.0363\n",
      "Epoch 19/120\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.12797 to 0.12745, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 156s - loss: 0.1424 - f1_score_mod: 0.1934 - recall_mod: 0.1128 - precision_mod: 0.6940 - dur_error: 0.4201 - maestro_dur_loss: 0.0420 - val_loss: 0.1275 - val_f1_score_mod: 0.1810 - val_recall_mod: 0.1041 - val_precision_mod: 0.7059 - val_dur_error: 0.3037 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 20/120\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.12745 to 0.12565, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 151s - loss: 0.1392 - f1_score_mod: 0.1976 - recall_mod: 0.1154 - precision_mod: 0.6963 - dur_error: 0.3945 - maestro_dur_loss: 0.0394 - val_loss: 0.1256 - val_f1_score_mod: 0.1739 - val_recall_mod: 0.0988 - val_precision_mod: 0.7409 - val_dur_error: 0.2952 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 21/120\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12565\n",
      "50/50 - 151s - loss: 0.1383 - f1_score_mod: 0.2027 - recall_mod: 0.1190 - precision_mod: 0.6962 - dur_error: 0.3918 - maestro_dur_loss: 0.0392 - val_loss: 0.1282 - val_f1_score_mod: 0.1903 - val_recall_mod: 0.1095 - val_precision_mod: 0.7389 - val_dur_error: 0.3249 - val_maestro_dur_loss: 0.0325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.12565 to 0.12517, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 151s - loss: 0.1373 - f1_score_mod: 0.2083 - recall_mod: 0.1228 - precision_mod: 0.6967 - dur_error: 0.3863 - maestro_dur_loss: 0.0386 - val_loss: 0.1252 - val_f1_score_mod: 0.1980 - val_recall_mod: 0.1147 - val_precision_mod: 0.7383 - val_dur_error: 0.3023 - val_maestro_dur_loss: 0.0302\n",
      "Epoch 23/120\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12517\n",
      "50/50 - 158s - loss: 0.1366 - f1_score_mod: 0.2161 - recall_mod: 0.1283 - precision_mod: 0.6953 - dur_error: 0.3847 - maestro_dur_loss: 0.0385 - val_loss: 0.1270 - val_f1_score_mod: 0.1922 - val_recall_mod: 0.1107 - val_precision_mod: 0.7412 - val_dur_error: 0.3220 - val_maestro_dur_loss: 0.0322\n",
      "Epoch 24/120\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12517 to 0.12235, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 152s - loss: 0.1350 - f1_score_mod: 0.2227 - recall_mod: 0.1325 - precision_mod: 0.7050 - dur_error: 0.3765 - maestro_dur_loss: 0.0377 - val_loss: 0.1224 - val_f1_score_mod: 0.2144 - val_recall_mod: 0.1262 - val_precision_mod: 0.7217 - val_dur_error: 0.2826 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 25/120\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12235\n",
      "50/50 - 147s - loss: 0.1347 - f1_score_mod: 0.2270 - recall_mod: 0.1356 - precision_mod: 0.7067 - dur_error: 0.3789 - maestro_dur_loss: 0.0379 - val_loss: 0.1224 - val_f1_score_mod: 0.2147 - val_recall_mod: 0.1265 - val_precision_mod: 0.7246 - val_dur_error: 0.2865 - val_maestro_dur_loss: 0.0286\n",
      "Epoch 26/120\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12235\n",
      "50/50 - 155s - loss: 0.1339 - f1_score_mod: 0.2348 - recall_mod: 0.1413 - precision_mod: 0.7049 - dur_error: 0.3763 - maestro_dur_loss: 0.0376 - val_loss: 0.1304 - val_f1_score_mod: 0.2245 - val_recall_mod: 0.1334 - val_precision_mod: 0.7192 - val_dur_error: 0.3704 - val_maestro_dur_loss: 0.0370\n",
      "Epoch 27/120\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12235\n",
      "50/50 - 142s - loss: 0.1330 - f1_score_mod: 0.2410 - recall_mod: 0.1452 - precision_mod: 0.7189 - dur_error: 0.3721 - maestro_dur_loss: 0.0372 - val_loss: 0.1269 - val_f1_score_mod: 0.2250 - val_recall_mod: 0.1335 - val_precision_mod: 0.7276 - val_dur_error: 0.3397 - val_maestro_dur_loss: 0.0340\n",
      "Epoch 28/120\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12235\n",
      "50/50 - 137s - loss: 0.1319 - f1_score_mod: 0.2454 - recall_mod: 0.1485 - precision_mod: 0.7163 - dur_error: 0.3670 - maestro_dur_loss: 0.0367 - val_loss: 0.1292 - val_f1_score_mod: 0.2270 - val_recall_mod: 0.1344 - val_precision_mod: 0.7430 - val_dur_error: 0.3616 - val_maestro_dur_loss: 0.0362\n",
      "Epoch 29/120\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12235\n",
      "50/50 - 150s - loss: 0.1317 - f1_score_mod: 0.2509 - recall_mod: 0.1524 - precision_mod: 0.7162 - dur_error: 0.3705 - maestro_dur_loss: 0.0370 - val_loss: 0.1288 - val_f1_score_mod: 0.2562 - val_recall_mod: 0.1577 - val_precision_mod: 0.6921 - val_dur_error: 0.3659 - val_maestro_dur_loss: 0.0366\n",
      "Epoch 30/120\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12235 to 0.12170, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 158s - loss: 0.1307 - f1_score_mod: 0.2570 - recall_mod: 0.1570 - precision_mod: 0.7167 - dur_error: 0.3643 - maestro_dur_loss: 0.0364 - val_loss: 0.1217 - val_f1_score_mod: 0.2398 - val_recall_mod: 0.1431 - val_precision_mod: 0.7482 - val_dur_error: 0.2977 - val_maestro_dur_loss: 0.0298\n",
      "Epoch 31/120\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.12170 to 0.12106, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 162s - loss: 0.1300 - f1_score_mod: 0.2629 - recall_mod: 0.1609 - precision_mod: 0.7258 - dur_error: 0.3629 - maestro_dur_loss: 0.0363 - val_loss: 0.1211 - val_f1_score_mod: 0.2410 - val_recall_mod: 0.1438 - val_precision_mod: 0.7527 - val_dur_error: 0.2934 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 32/120\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.12106 to 0.11802, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 158s - loss: 0.1294 - f1_score_mod: 0.2670 - recall_mod: 0.1641 - precision_mod: 0.7259 - dur_error: 0.3616 - maestro_dur_loss: 0.0362 - val_loss: 0.1180 - val_f1_score_mod: 0.2376 - val_recall_mod: 0.1415 - val_precision_mod: 0.7495 - val_dur_error: 0.2649 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 33/120\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11802\n",
      "50/50 - 151s - loss: 0.1283 - f1_score_mod: 0.2710 - recall_mod: 0.1671 - precision_mod: 0.7228 - dur_error: 0.3563 - maestro_dur_loss: 0.0356 - val_loss: 0.1210 - val_f1_score_mod: 0.2479 - val_recall_mod: 0.1498 - val_precision_mod: 0.7318 - val_dur_error: 0.2994 - val_maestro_dur_loss: 0.0299\n",
      "Epoch 34/120\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11802 to 0.11692, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 144s - loss: 0.1278 - f1_score_mod: 0.2754 - recall_mod: 0.1702 - precision_mod: 0.7267 - dur_error: 0.3566 - maestro_dur_loss: 0.0357 - val_loss: 0.1169 - val_f1_score_mod: 0.2452 - val_recall_mod: 0.1468 - val_precision_mod: 0.7553 - val_dur_error: 0.2593 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 35/120\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11692\n",
      "50/50 - 139s - loss: 0.1272 - f1_score_mod: 0.2804 - recall_mod: 0.1741 - precision_mod: 0.7271 - dur_error: 0.3542 - maestro_dur_loss: 0.0354 - val_loss: 0.1219 - val_f1_score_mod: 0.2743 - val_recall_mod: 0.1696 - val_precision_mod: 0.7264 - val_dur_error: 0.3167 - val_maestro_dur_loss: 0.0317\n",
      "Epoch 36/120\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11692\n",
      "50/50 - 132s - loss: 0.1263 - f1_score_mod: 0.2850 - recall_mod: 0.1777 - precision_mod: 0.7268 - dur_error: 0.3498 - maestro_dur_loss: 0.0350 - val_loss: 0.1279 - val_f1_score_mod: 0.2646 - val_recall_mod: 0.1615 - val_precision_mod: 0.7431 - val_dur_error: 0.3794 - val_maestro_dur_loss: 0.0379\n",
      "Epoch 37/120\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11692\n",
      "50/50 - 137s - loss: 0.1260 - f1_score_mod: 0.2944 - recall_mod: 0.1845 - precision_mod: 0.7360 - dur_error: 0.3521 - maestro_dur_loss: 0.0352 - val_loss: 0.1204 - val_f1_score_mod: 0.2653 - val_recall_mod: 0.1614 - val_precision_mod: 0.7590 - val_dur_error: 0.3112 - val_maestro_dur_loss: 0.0311\n",
      "Epoch 38/120\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11692\n",
      "50/50 - 149s - loss: 0.1251 - f1_score_mod: 0.2965 - recall_mod: 0.1861 - precision_mod: 0.7331 - dur_error: 0.3478 - maestro_dur_loss: 0.0348 - val_loss: 0.1248 - val_f1_score_mod: 0.2773 - val_recall_mod: 0.1717 - val_precision_mod: 0.7295 - val_dur_error: 0.3504 - val_maestro_dur_loss: 0.0350\n",
      "Epoch 39/120\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.11692 to 0.11457, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 163s - loss: 0.1241 - f1_score_mod: 0.3029 - recall_mod: 0.1913 - precision_mod: 0.7329 - dur_error: 0.3439 - maestro_dur_loss: 0.0344 - val_loss: 0.1146 - val_f1_score_mod: 0.2889 - val_recall_mod: 0.1811 - val_precision_mod: 0.7253 - val_dur_error: 0.2561 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 40/120\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11457\n",
      "50/50 - 153s - loss: 0.1240 - f1_score_mod: 0.3048 - recall_mod: 0.1929 - precision_mod: 0.7315 - dur_error: 0.3464 - maestro_dur_loss: 0.0346 - val_loss: 0.1158 - val_f1_score_mod: 0.2886 - val_recall_mod: 0.1812 - val_precision_mod: 0.7194 - val_dur_error: 0.2672 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 41/120\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11457\n",
      "50/50 - 145s - loss: 0.1234 - f1_score_mod: 0.3106 - recall_mod: 0.1971 - precision_mod: 0.7371 - dur_error: 0.3455 - maestro_dur_loss: 0.0346 - val_loss: 0.1155 - val_f1_score_mod: 0.2983 - val_recall_mod: 0.1886 - val_precision_mod: 0.7255 - val_dur_error: 0.2692 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 42/120\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11457\n",
      "50/50 - 161s - loss: 0.1219 - f1_score_mod: 0.3194 - recall_mod: 0.2040 - precision_mod: 0.7425 - dur_error: 0.3367 - maestro_dur_loss: 0.0337 - val_loss: 0.1240 - val_f1_score_mod: 0.3036 - val_recall_mod: 0.1941 - val_precision_mod: 0.7117 - val_dur_error: 0.3574 - val_maestro_dur_loss: 0.0357\n",
      "Epoch 43/120\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11457\n",
      "50/50 - 152s - loss: 0.1220 - f1_score_mod: 0.3207 - recall_mod: 0.2047 - precision_mod: 0.7455 - dur_error: 0.3410 - maestro_dur_loss: 0.0341 - val_loss: 0.1163 - val_f1_score_mod: 0.3057 - val_recall_mod: 0.1960 - val_precision_mod: 0.7094 - val_dur_error: 0.2791 - val_maestro_dur_loss: 0.0279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/120\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.11457 to 0.11334, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 143s - loss: 0.1211 - f1_score_mod: 0.3248 - recall_mod: 0.2087 - precision_mod: 0.7369 - dur_error: 0.3366 - maestro_dur_loss: 0.0337 - val_loss: 0.1133 - val_f1_score_mod: 0.2999 - val_recall_mod: 0.1881 - val_precision_mod: 0.7483 - val_dur_error: 0.2583 - val_maestro_dur_loss: 0.0258\n",
      "Epoch 45/120\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11334\n",
      "50/50 - 147s - loss: 0.1207 - f1_score_mod: 0.3320 - recall_mod: 0.2145 - precision_mod: 0.7387 - dur_error: 0.3369 - maestro_dur_loss: 0.0337 - val_loss: 0.1144 - val_f1_score_mod: 0.3038 - val_recall_mod: 0.1923 - val_precision_mod: 0.7334 - val_dur_error: 0.2731 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 46/120\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11334\n",
      "50/50 - 156s - loss: 0.1199 - f1_score_mod: 0.3332 - recall_mod: 0.2153 - precision_mod: 0.7433 - dur_error: 0.3331 - maestro_dur_loss: 0.0333 - val_loss: 0.1166 - val_f1_score_mod: 0.3167 - val_recall_mod: 0.2045 - val_precision_mod: 0.7123 - val_dur_error: 0.2930 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 47/120\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11334\n",
      "50/50 - 153s - loss: 0.1194 - f1_score_mod: 0.3361 - recall_mod: 0.2176 - precision_mod: 0.7457 - dur_error: 0.3343 - maestro_dur_loss: 0.0334 - val_loss: 0.1218 - val_f1_score_mod: 0.3215 - val_recall_mod: 0.2082 - val_precision_mod: 0.7146 - val_dur_error: 0.3468 - val_maestro_dur_loss: 0.0347\n",
      "Epoch 48/120\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.11334 to 0.11113, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 143s - loss: 0.1185 - f1_score_mod: 0.3448 - recall_mod: 0.2248 - precision_mod: 0.7448 - dur_error: 0.3290 - maestro_dur_loss: 0.0329 - val_loss: 0.1111 - val_f1_score_mod: 0.3224 - val_recall_mod: 0.2085 - val_precision_mod: 0.7220 - val_dur_error: 0.2471 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 49/120\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11113\n",
      "50/50 - 149s - loss: 0.1180 - f1_score_mod: 0.3504 - recall_mod: 0.2291 - precision_mod: 0.7486 - dur_error: 0.3300 - maestro_dur_loss: 0.0330 - val_loss: 0.1118 - val_f1_score_mod: 0.3185 - val_recall_mod: 0.2028 - val_precision_mod: 0.7514 - val_dur_error: 0.2569 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 50/120\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11113\n",
      "50/50 - 149s - loss: 0.1175 - f1_score_mod: 0.3541 - recall_mod: 0.2326 - precision_mod: 0.7458 - dur_error: 0.3295 - maestro_dur_loss: 0.0329 - val_loss: 0.1185 - val_f1_score_mod: 0.3272 - val_recall_mod: 0.2118 - val_precision_mod: 0.7302 - val_dur_error: 0.3196 - val_maestro_dur_loss: 0.0320\n",
      "Epoch 51/120\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11113\n",
      "50/50 - 155s - loss: 0.1169 - f1_score_mod: 0.3565 - recall_mod: 0.2343 - precision_mod: 0.7485 - dur_error: 0.3263 - maestro_dur_loss: 0.0326 - val_loss: 0.1118 - val_f1_score_mod: 0.3351 - val_recall_mod: 0.2189 - val_precision_mod: 0.7222 - val_dur_error: 0.2621 - val_maestro_dur_loss: 0.0262\n",
      "Epoch 52/120\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.11113 to 0.11041, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 152s - loss: 0.1164 - f1_score_mod: 0.3639 - recall_mod: 0.2409 - precision_mod: 0.7474 - dur_error: 0.3281 - maestro_dur_loss: 0.0328 - val_loss: 0.1104 - val_f1_score_mod: 0.3295 - val_recall_mod: 0.2137 - val_precision_mod: 0.7329 - val_dur_error: 0.2501 - val_maestro_dur_loss: 0.0250\n",
      "Epoch 53/120\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11041\n",
      "50/50 - 146s - loss: 0.1157 - f1_score_mod: 0.3666 - recall_mod: 0.2425 - precision_mod: 0.7544 - dur_error: 0.3255 - maestro_dur_loss: 0.0326 - val_loss: 0.1131 - val_f1_score_mod: 0.3448 - val_recall_mod: 0.2272 - val_precision_mod: 0.7214 - val_dur_error: 0.2794 - val_maestro_dur_loss: 0.0279\n",
      "Epoch 54/120\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11041\n",
      "50/50 - 160s - loss: 0.1149 - f1_score_mod: 0.3706 - recall_mod: 0.2464 - precision_mod: 0.7506 - dur_error: 0.3228 - maestro_dur_loss: 0.0323 - val_loss: 0.1140 - val_f1_score_mod: 0.3452 - val_recall_mod: 0.2273 - val_precision_mod: 0.7256 - val_dur_error: 0.2893 - val_maestro_dur_loss: 0.0289\n",
      "Epoch 55/120\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.11041 to 0.10992, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 135s - loss: 0.1146 - f1_score_mod: 0.3774 - recall_mod: 0.2521 - precision_mod: 0.7543 - dur_error: 0.3234 - maestro_dur_loss: 0.0323 - val_loss: 0.1099 - val_f1_score_mod: 0.3367 - val_recall_mod: 0.2191 - val_precision_mod: 0.7387 - val_dur_error: 0.2461 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 56/120\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.10992\n",
      "50/50 - 135s - loss: 0.1140 - f1_score_mod: 0.3790 - recall_mod: 0.2531 - precision_mod: 0.7581 - dur_error: 0.3207 - maestro_dur_loss: 0.0321 - val_loss: 0.1145 - val_f1_score_mod: 0.3544 - val_recall_mod: 0.2355 - val_precision_mod: 0.7217 - val_dur_error: 0.2997 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 57/120\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.10992 to 0.10888, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 135s - loss: 0.1134 - f1_score_mod: 0.3846 - recall_mod: 0.2589 - precision_mod: 0.7523 - dur_error: 0.3214 - maestro_dur_loss: 0.0321 - val_loss: 0.1089 - val_f1_score_mod: 0.3501 - val_recall_mod: 0.2331 - val_precision_mod: 0.7124 - val_dur_error: 0.2412 - val_maestro_dur_loss: 0.0241\n",
      "Epoch 58/120\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.10888\n",
      "50/50 - 135s - loss: 0.1129 - f1_score_mod: 0.3861 - recall_mod: 0.2601 - precision_mod: 0.7522 - dur_error: 0.3192 - maestro_dur_loss: 0.0319 - val_loss: 0.1120 - val_f1_score_mod: 0.3478 - val_recall_mod: 0.2292 - val_precision_mod: 0.7310 - val_dur_error: 0.2795 - val_maestro_dur_loss: 0.0279\n",
      "Epoch 59/120\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.10888\n",
      "50/50 - 131s - loss: 0.1120 - f1_score_mod: 0.3927 - recall_mod: 0.2658 - precision_mod: 0.7541 - dur_error: 0.3158 - maestro_dur_loss: 0.0316 - val_loss: 0.1111 - val_f1_score_mod: 0.3558 - val_recall_mod: 0.2368 - val_precision_mod: 0.7235 - val_dur_error: 0.2696 - val_maestro_dur_loss: 0.0270\n",
      "Epoch 60/120\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.10888 to 0.10749, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 130s - loss: 0.1117 - f1_score_mod: 0.3956 - recall_mod: 0.2686 - precision_mod: 0.7549 - dur_error: 0.3180 - maestro_dur_loss: 0.0318 - val_loss: 0.1075 - val_f1_score_mod: 0.3627 - val_recall_mod: 0.2424 - val_precision_mod: 0.7247 - val_dur_error: 0.2377 - val_maestro_dur_loss: 0.0238\n",
      "Epoch 61/120\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.10749\n",
      "50/50 - 131s - loss: 0.1112 - f1_score_mod: 0.3978 - recall_mod: 0.2704 - precision_mod: 0.7563 - dur_error: 0.3158 - maestro_dur_loss: 0.0316 - val_loss: 0.1082 - val_f1_score_mod: 0.3590 - val_recall_mod: 0.2381 - val_precision_mod: 0.7375 - val_dur_error: 0.2455 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 62/120\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.10749 to 0.10745, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 131s - loss: 0.1105 - f1_score_mod: 0.4045 - recall_mod: 0.2762 - precision_mod: 0.7571 - dur_error: 0.3156 - maestro_dur_loss: 0.0316 - val_loss: 0.1075 - val_f1_score_mod: 0.3587 - val_recall_mod: 0.2383 - val_precision_mod: 0.7319 - val_dur_error: 0.2401 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 63/120\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.10745\n",
      "50/50 - 130s - loss: 0.1093 - f1_score_mod: 0.4056 - recall_mod: 0.2778 - precision_mod: 0.7549 - dur_error: 0.3075 - maestro_dur_loss: 0.0307 - val_loss: 0.1102 - val_f1_score_mod: 0.3720 - val_recall_mod: 0.2559 - val_precision_mod: 0.6856 - val_dur_error: 0.2591 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 64/120\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.10745\n",
      "50/50 - 133s - loss: 0.1094 - f1_score_mod: 0.4128 - recall_mod: 0.2840 - precision_mod: 0.7575 - dur_error: 0.3132 - maestro_dur_loss: 0.0313 - val_loss: 0.1097 - val_f1_score_mod: 0.3762 - val_recall_mod: 0.2583 - val_precision_mod: 0.6986 - val_dur_error: 0.2651 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 65/120\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10745\n",
      "50/50 - 140s - loss: 0.1085 - f1_score_mod: 0.4156 - recall_mod: 0.2868 - precision_mod: 0.7577 - dur_error: 0.3088 - maestro_dur_loss: 0.0309 - val_loss: 0.1091 - val_f1_score_mod: 0.3704 - val_recall_mod: 0.2509 - val_precision_mod: 0.7150 - val_dur_error: 0.2620 - val_maestro_dur_loss: 0.0262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.10745 to 0.10690, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 133s - loss: 0.1081 - f1_score_mod: 0.4217 - recall_mod: 0.2924 - precision_mod: 0.7588 - dur_error: 0.3086 - maestro_dur_loss: 0.0309 - val_loss: 0.1069 - val_f1_score_mod: 0.3754 - val_recall_mod: 0.2561 - val_precision_mod: 0.7063 - val_dur_error: 0.2384 - val_maestro_dur_loss: 0.0238\n",
      "Epoch 67/120\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.10690\n",
      "50/50 - 130s - loss: 0.1073 - f1_score_mod: 0.4259 - recall_mod: 0.2963 - precision_mod: 0.7601 - dur_error: 0.3068 - maestro_dur_loss: 0.0307 - val_loss: 0.1162 - val_f1_score_mod: 0.3777 - val_recall_mod: 0.2581 - val_precision_mod: 0.7111 - val_dur_error: 0.3362 - val_maestro_dur_loss: 0.0336\n",
      "Epoch 68/120\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.10690\n",
      "50/50 - 130s - loss: 0.1066 - f1_score_mod: 0.4284 - recall_mod: 0.2983 - precision_mod: 0.7629 - dur_error: 0.3052 - maestro_dur_loss: 0.0305 - val_loss: 0.1156 - val_f1_score_mod: 0.3759 - val_recall_mod: 0.2563 - val_precision_mod: 0.7125 - val_dur_error: 0.3294 - val_maestro_dur_loss: 0.0329\n",
      "Epoch 69/120\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.10690 to 0.10536, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 130s - loss: 0.1063 - f1_score_mod: 0.4336 - recall_mod: 0.3035 - precision_mod: 0.7608 - dur_error: 0.3070 - maestro_dur_loss: 0.0307 - val_loss: 0.1054 - val_f1_score_mod: 0.3768 - val_recall_mod: 0.2567 - val_precision_mod: 0.7144 - val_dur_error: 0.2341 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 70/120\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.10536\n",
      "50/50 - 131s - loss: 0.1054 - f1_score_mod: 0.4369 - recall_mod: 0.3066 - precision_mod: 0.7618 - dur_error: 0.3034 - maestro_dur_loss: 0.0303 - val_loss: 0.1062 - val_f1_score_mod: 0.3812 - val_recall_mod: 0.2611 - val_precision_mod: 0.7146 - val_dur_error: 0.2369 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 71/120\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.10536\n",
      "50/50 - 130s - loss: 0.1052 - f1_score_mod: 0.4418 - recall_mod: 0.3115 - precision_mod: 0.7618 - dur_error: 0.3045 - maestro_dur_loss: 0.0305 - val_loss: 0.1159 - val_f1_score_mod: 0.3974 - val_recall_mod: 0.2811 - val_precision_mod: 0.6802 - val_dur_error: 0.3392 - val_maestro_dur_loss: 0.0339\n",
      "Epoch 72/120\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.10536\n",
      "50/50 - 129s - loss: 0.1044 - f1_score_mod: 0.4470 - recall_mod: 0.3163 - precision_mod: 0.7645 - dur_error: 0.3011 - maestro_dur_loss: 0.0301 - val_loss: 0.1081 - val_f1_score_mod: 0.3945 - val_recall_mod: 0.2739 - val_precision_mod: 0.7086 - val_dur_error: 0.2597 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 73/120\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.10536\n",
      "50/50 - 129s - loss: 0.1035 - f1_score_mod: 0.4511 - recall_mod: 0.3203 - precision_mod: 0.7648 - dur_error: 0.2989 - maestro_dur_loss: 0.0299 - val_loss: 0.1079 - val_f1_score_mod: 0.3931 - val_recall_mod: 0.2747 - val_precision_mod: 0.6954 - val_dur_error: 0.2517 - val_maestro_dur_loss: 0.0252\n",
      "Epoch 74/120\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10536\n",
      "50/50 - 129s - loss: 0.1032 - f1_score_mod: 0.4535 - recall_mod: 0.3229 - precision_mod: 0.7638 - dur_error: 0.3009 - maestro_dur_loss: 0.0301 - val_loss: 0.1073 - val_f1_score_mod: 0.3957 - val_recall_mod: 0.2754 - val_precision_mod: 0.7078 - val_dur_error: 0.2586 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 75/120\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10536\n",
      "50/50 - 128s - loss: 0.1023 - f1_score_mod: 0.4604 - recall_mod: 0.3298 - precision_mod: 0.7636 - dur_error: 0.2972 - maestro_dur_loss: 0.0297 - val_loss: 0.1068 - val_f1_score_mod: 0.3983 - val_recall_mod: 0.2780 - val_precision_mod: 0.7070 - val_dur_error: 0.2534 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 76/120\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.10536\n",
      "50/50 - 128s - loss: 0.1019 - f1_score_mod: 0.4618 - recall_mod: 0.3319 - precision_mod: 0.7618 - dur_error: 0.2978 - maestro_dur_loss: 0.0298 - val_loss: 0.1064 - val_f1_score_mod: 0.4006 - val_recall_mod: 0.2822 - val_precision_mod: 0.6931 - val_dur_error: 0.2434 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 77/120\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10536\n",
      "50/50 - 131s - loss: 0.1012 - f1_score_mod: 0.4665 - recall_mod: 0.3361 - precision_mod: 0.7644 - dur_error: 0.2950 - maestro_dur_loss: 0.0295 - val_loss: 0.1060 - val_f1_score_mod: 0.4083 - val_recall_mod: 0.2898 - val_precision_mod: 0.6928 - val_dur_error: 0.2460 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 78/120\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.10536 to 0.10477, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 128s - loss: 0.1007 - f1_score_mod: 0.4720 - recall_mod: 0.3417 - precision_mod: 0.7656 - dur_error: 0.2946 - maestro_dur_loss: 0.0295 - val_loss: 0.1048 - val_f1_score_mod: 0.4078 - val_recall_mod: 0.2873 - val_precision_mod: 0.7065 - val_dur_error: 0.2304 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 79/120\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.10477\n",
      "50/50 - 128s - loss: 0.1003 - f1_score_mod: 0.4762 - recall_mod: 0.3460 - precision_mod: 0.7650 - dur_error: 0.2955 - maestro_dur_loss: 0.0295 - val_loss: 0.1082 - val_f1_score_mod: 0.4071 - val_recall_mod: 0.2891 - val_precision_mod: 0.6923 - val_dur_error: 0.2660 - val_maestro_dur_loss: 0.0266\n",
      "Epoch 80/120\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.10477 to 0.10461, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 129s - loss: 0.0995 - f1_score_mod: 0.4819 - recall_mod: 0.3520 - precision_mod: 0.7659 - dur_error: 0.2929 - maestro_dur_loss: 0.0293 - val_loss: 0.1046 - val_f1_score_mod: 0.4110 - val_recall_mod: 0.2917 - val_precision_mod: 0.6987 - val_dur_error: 0.2322 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 81/120\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10461\n",
      "50/50 - 128s - loss: 0.0991 - f1_score_mod: 0.4851 - recall_mod: 0.3556 - precision_mod: 0.7650 - dur_error: 0.2948 - maestro_dur_loss: 0.0295 - val_loss: 0.1065 - val_f1_score_mod: 0.4156 - val_recall_mod: 0.2962 - val_precision_mod: 0.6995 - val_dur_error: 0.2528 - val_maestro_dur_loss: 0.0253\n",
      "Epoch 82/120\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10461\n",
      "50/50 - 128s - loss: 0.0981 - f1_score_mod: 0.4901 - recall_mod: 0.3601 - precision_mod: 0.7687 - dur_error: 0.2897 - maestro_dur_loss: 0.0290 - val_loss: 0.1121 - val_f1_score_mod: 0.4195 - val_recall_mod: 0.3033 - val_precision_mod: 0.6833 - val_dur_error: 0.3097 - val_maestro_dur_loss: 0.0310\n",
      "Epoch 83/120\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10461\n",
      "50/50 - 128s - loss: 0.0974 - f1_score_mod: 0.4930 - recall_mod: 0.3640 - precision_mod: 0.7655 - dur_error: 0.2882 - maestro_dur_loss: 0.0288 - val_loss: 0.1074 - val_f1_score_mod: 0.4260 - val_recall_mod: 0.3101 - val_precision_mod: 0.6826 - val_dur_error: 0.2602 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 84/120\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10461\n",
      "50/50 - 129s - loss: 0.0968 - f1_score_mod: 0.4996 - recall_mod: 0.3704 - precision_mod: 0.7693 - dur_error: 0.2884 - maestro_dur_loss: 0.0288 - val_loss: 0.1064 - val_f1_score_mod: 0.4265 - val_recall_mod: 0.3110 - val_precision_mod: 0.6807 - val_dur_error: 0.2429 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 85/120\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.10461 to 0.10442, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 129s - loss: 0.0966 - f1_score_mod: 0.5033 - recall_mod: 0.3747 - precision_mod: 0.7688 - dur_error: 0.2900 - maestro_dur_loss: 0.0290 - val_loss: 0.1044 - val_f1_score_mod: 0.4268 - val_recall_mod: 0.3115 - val_precision_mod: 0.6804 - val_dur_error: 0.2334 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 86/120\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10442 to 0.10424, saving model to models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5\n",
      "50/50 - 129s - loss: 0.0956 - f1_score_mod: 0.5080 - recall_mod: 0.3799 - precision_mod: 0.7678 - dur_error: 0.2858 - maestro_dur_loss: 0.0286 - val_loss: 0.1042 - val_f1_score_mod: 0.4234 - val_recall_mod: 0.3067 - val_precision_mod: 0.6863 - val_dur_error: 0.2270 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 87/120\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10424\n",
      "50/50 - 129s - loss: 0.0945 - f1_score_mod: 0.5114 - recall_mod: 0.3837 - precision_mod: 0.7680 - dur_error: 0.2814 - maestro_dur_loss: 0.0281 - val_loss: 0.1079 - val_f1_score_mod: 0.4374 - val_recall_mod: 0.3256 - val_precision_mod: 0.6679 - val_dur_error: 0.2679 - val_maestro_dur_loss: 0.0268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/120\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10424\n",
      "50/50 - 130s - loss: 0.0945 - f1_score_mod: 0.5171 - recall_mod: 0.3894 - precision_mod: 0.7712 - dur_error: 0.2839 - maestro_dur_loss: 0.0284 - val_loss: 0.1159 - val_f1_score_mod: 0.4373 - val_recall_mod: 0.3266 - val_precision_mod: 0.6636 - val_dur_error: 0.3519 - val_maestro_dur_loss: 0.0352\n",
      "Epoch 89/120\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10424\n",
      "50/50 - 129s - loss: 0.0936 - f1_score_mod: 0.5204 - recall_mod: 0.3945 - precision_mod: 0.7659 - dur_error: 0.2814 - maestro_dur_loss: 0.0281 - val_loss: 0.1043 - val_f1_score_mod: 0.4358 - val_recall_mod: 0.3212 - val_precision_mod: 0.6804 - val_dur_error: 0.2328 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 90/120\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10424\n",
      "50/50 - 130s - loss: 0.0933 - f1_score_mod: 0.5225 - recall_mod: 0.3957 - precision_mod: 0.7699 - dur_error: 0.2820 - maestro_dur_loss: 0.0282 - val_loss: 0.1042 - val_f1_score_mod: 0.4400 - val_recall_mod: 0.3239 - val_precision_mod: 0.6881 - val_dur_error: 0.2339 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 91/120\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10424\n",
      "50/50 - 130s - loss: 0.0923 - f1_score_mod: 0.5299 - recall_mod: 0.4039 - precision_mod: 0.7721 - dur_error: 0.2802 - maestro_dur_loss: 0.0280 - val_loss: 0.1074 - val_f1_score_mod: 0.4447 - val_recall_mod: 0.3365 - val_precision_mod: 0.6565 - val_dur_error: 0.2563 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 92/120\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10424\n",
      "50/50 - 130s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.25)\n",
    "opt = RMSprop(lr = 0.0005, clipnorm = 1.0)\n",
    "model.compile(loss = maestro_loss_wr(0.1), optimizer = opt, metrics = [f1_score_mod, recall_mod, precision_mod, dur_error, maestro_dur_loss_wr(0.1)])\n",
    "mc = ModelCheckpoint('models/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 120, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "    for key, value in history.history.items():\n",
    "        if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "            value.append(np.nan)  \n",
    "df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "df.index.name = 'Epochs'\n",
    "df.to_csv('model_data/best_maestro_model_2_1_512_pt25_lr_5e-4_cn_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have our final model. Please proceed to the visualize_performance.ipynb Notebook for performance plots and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
