{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this [tutorial](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import tempfile\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../train_and_val/X_train_ext.npy')\n",
    "X_val = np.load('../train_and_val/X_val_ext.npy')\n",
    "y_train = np.load('../train_and_val/y_train_ext.npy')\n",
    "y_val = np.load('../train_and_val/y_val_ext.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Scaled Duration for X_train: 0.860215053763441\n",
      "Maximum Scaled Duration for X_val: 0.5516975308641976\n",
      "Maximum Scaled Duration for y_train: 1.0\n",
      "Maximum Scaled Duration for y_val: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Maximum Scaled Duration for X_train: {}'.format(X_train[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for X_val: {}'.format(X_val[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for y_train: {}'.format(y_train[:, -1].max()))\n",
    "print('Maximum Scaled Duration for y_val: {}'.format(y_val[:, -1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Ratio of the Mean of the Scaled Duration:  0.9898257021922111\n",
      "Train-Validation Ratio of the Stdv of the Scaled Duration:  0.9209054376139967\n"
     ]
    }
   ],
   "source": [
    "print('Train-Validation Ratio of the Mean of the Scaled Duration: ', y_train[:, -1].mean() / y_val[:, -1].mean())\n",
    "print('Train-Validation Ratio of the Stdv of the Scaled Duration: ', y_train[:, -1].std() / y_val[:, -1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maestro_loss_wr(harshness, n_dur_nodes): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last n_dur_nodes elements (whose mean represents\n",
    "    the duration). The proportionality constant is the 'harshness' of the maestro in \n",
    "    regards to timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-n_dur_nodes] * K.log(ypred[:, :-n_dur_nodes]) + \\\n",
    "                            (1 - ytrue[:, :-n_dur_nodes]) * K.log(1 - ypred[:, :-n_dur_nodes]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs(K.mean(ytrue[:, -n_dur_nodes:], axis = 1) - \\\n",
    "                                                K.mean(ypred[:, -n_dur_nodes:], axis = 1)) / \\\n",
    "                                      (K.mean(ytrue[:, -n_dur_nodes:], axis = 1) + \\\n",
    "                                       K.mean(ypred[:, -n_dur_nodes:], axis = 1) + K.epsilon()))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, :-n_dur_nodes] elements will be zero\n",
    "            return bce_loss * 2     # (for a rest). This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "def precision_mod_wr(n_dur_nodes):\n",
    "    def precision_mod(ytrue, ypred):\n",
    "        \"\"\"Just a modified precision excluding the last n_dur_nodes elements (which are not\n",
    "        classification nodes)\"\"\"\n",
    "\n",
    "        true_positives = K.sum(K.round(ytrue[:, :-n_dur_nodes] * ypred[:, :-n_dur_nodes]))\n",
    "        pred_positives = K.sum(K.round(ypred[:, :-n_dur_nodes]))\n",
    "        return true_positives / (pred_positives + K.epsilon())\n",
    "    return precision_mod\n",
    "\n",
    "def recall_mod_wr(n_dur_nodes):\n",
    "    def recall_mod(ytrue, ypred):\n",
    "        \"\"\"Just a modified recall excluding the last n_dur_nodes elements (which are not\n",
    "        classification nodes)\"\"\"\n",
    "\n",
    "        true_positives = K.sum(K.round(ytrue[:, :-n_dur_nodes] * ypred[:, :-n_dur_nodes]))\n",
    "        poss_positives = K.sum(ytrue[:, :-n_dur_nodes])\n",
    "        return true_positives / (poss_positives + K.epsilon())\n",
    "    return recall_mod\n",
    "\n",
    "def f1_score_mod_wr(n_dur_nodes):\n",
    "    def f1_score_mod(ytrue, ypred):\n",
    "        \"\"\"Just a modified f1_score excluding the last n_dur_nodes elements (which are not\n",
    "        classification nodes)\"\"\"\n",
    "\n",
    "        precision = precision_mod_wr(n_dur_nodes)(ytrue, ypred)\n",
    "        recall = recall_mod_wr(n_dur_nodes)(ytrue, ypred)   \n",
    "        return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_score_mod\n",
    "\n",
    "def dur_error_wr(n_dur_nodes):\n",
    "    def dur_error(ytrue, ypred):\n",
    "        \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "        return 2 * K.mean(K.abs((K.mean(ytrue[:, -n_dur_nodes:], axis = 1) - \\\n",
    "                   K.mean(ypred[:, -n_dur_nodes:], axis = 1)) / (K.mean(ytrue[:, -n_dur_nodes:], \\\n",
    "                    axis = 1) + K.mean(ypred[:, -n_dur_nodes:], axis = 1) + K.epsilon())))\n",
    "    return dur_error\n",
    "\n",
    "def maestro_dur_loss_wr(harshness, n_dur_nodes):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((K.mean(ytrue[:, -n_dur_nodes:], axis = 1) - \\\n",
    "                                      K.mean(ypred[:, -n_dur_nodes:], axis = 1)) / \\\n",
    "                                      (K.mean(ytrue[:, -n_dur_nodes:], axis = 1) + \\\n",
    "                                      K.mean(ypred[:, -n_dur_nodes:], axis = 1) + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "harshness = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(file_path, harshness = harshness, n_dur_nodes = 20):\n",
    "    \n",
    "    custom_objects = {'maestro_loss': maestro_loss_wr(harshness, \\\n",
    "        n_dur_nodes), 'f1_score_mod': f1_score_mod_wr(n_dur_nodes), \\\n",
    "        'recall_mod': recall_mod_wr(n_dur_nodes), 'precision_mod': \\\n",
    "        precision_mod_wr(n_dur_nodes), 'dur_error': \\\n",
    "        dur_error_wr(n_dur_nodes), 'maestro_dur_loss': \\\n",
    "        maestro_dur_loss_wr(harshness, n_dur_nodes)}\n",
    "\n",
    "    return load_model(file_path, custom_objects = custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_with_checkpoint(model, filename = 'best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5', \\\n",
    "                                harshness = 0.05, n_dur_nodes = 20, batch_size = 512, epochs = 2, \\\n",
    "                                initial_sparsity = 0.5, final_sparsity = 0.8):\n",
    "    \n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    end_step = np.ceil(X_train.shape[0] / batch_size) * epochs\n",
    "    \n",
    "    # Define model for pruning.\n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "              initial_sparsity = initial_sparsity, final_sparsity = final_sparsity,\n",
    "              begin_step=0, end_step=end_step)\n",
    "    }\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "    \n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(loss = maestro_loss_wr(harshness, n_dur_nodes), \n",
    "                          optimizer = opt, \n",
    "                          metrics = [f1_score_mod_wr(n_dur_nodes), recall_mod_wr(n_dur_nodes), \\\n",
    "                                     precision_mod_wr(n_dur_nodes), dur_error_wr(n_dur_nodes), \\\n",
    "                                     maestro_dur_loss_wr(harshness, n_dur_nodes)])\n",
    "\n",
    "    model_for_pruning.summary()\n",
    "    \n",
    "    logdir = tempfile.mkdtemp()\n",
    "\n",
    "    mc = ModelCheckpoint('../models/' + filename, monitor = 'val_loss', mode = 'min', \\\n",
    "                                                            save_best_only = True, verbose = 1)\n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "      tfmot.sparsity.keras.PruningSummaries(log_dir = logdir),\n",
    "      mc, \n",
    "      TerminateOnNaN()\n",
    "    ]\n",
    "\n",
    "    model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n",
    "                      validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_lstm_4 ( (None, 16, 1024)          9277443   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 16, 1024)          1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_lstm_5 ( (None, 1024)              16781315  \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 1024)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_4  (None, 512)               1049090   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_5  (None, 108)               110702    \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 108)               1         \n",
      "=================================================================\n",
      "Total params: 27,218,555\n",
      "Trainable params: 13,613,676\n",
      "Non-trainable params: 13,604,879\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07691, saving model to ../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-feb6692b4220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/best_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpruned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_model_with_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-7c814fc8a57f>\u001b[0m in \u001b[0;36mprune_model_with_checkpoint\u001b[0;34m(model, filename, harshness, n_dur_nodes, batch_size, epochs, initial_sparsity, final_sparsity)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n\u001b[0;32m---> 37\u001b[0;31m                       validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     filepath, overwrite=True, options=self._options)\n\u001b[1;32m   1300\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m       \u001b[0mparam_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5')\n",
    "pruned_model = prune_model_with_checkpoint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, filename = 'best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2', harshness = 0.05, \\\n",
    "                n_dur_nodes = 20, batch_size = 512, epochs = 50, initial_sparsity = 0.5, final_sparsity = 0.8):\n",
    "    \n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    end_step = np.ceil(X_train.shape[0] / batch_size) * epochs\n",
    "    \n",
    "    # Define model for pruning.\n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "              initial_sparsity = initial_sparsity, final_sparsity = final_sparsity,\n",
    "              begin_step=0, end_step=end_step)\n",
    "    }\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "    \n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(loss = maestro_loss_wr(harshness, n_dur_nodes), \n",
    "                          optimizer = opt, \n",
    "                          metrics = [f1_score_mod_wr(n_dur_nodes), recall_mod_wr(n_dur_nodes), \\\n",
    "                                     precision_mod_wr(n_dur_nodes), dur_error_wr(n_dur_nodes), \\\n",
    "                                     maestro_dur_loss_wr(harshness, n_dur_nodes)])\n",
    "\n",
    "    #model_for_pruning.summary()\n",
    "\n",
    "    logdir = tempfile.mkdtemp()\n",
    "\n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "      tfmot.sparsity.keras.PruningSummaries(log_dir = logdir),\n",
    "      TerminateOnNaN()\n",
    "    ]\n",
    "    filepath = '../models/' + filename + '_{0}_{1}'.format(str(initial_sparsity).replace('.', 'pt'), \\\n",
    "                                         '{0:.1f}'.format(final_sparsity).replace('.', 'pt')) + '.h5'\n",
    "    # ModelCheckpoint is giving a funny error (RuntimeError: Unable to create link (name already exists), \n",
    "    # so here is my workaround:\n",
    "    print('Epoch 1/{}'.format(epochs))\n",
    "    history = model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = 1, \n",
    "                validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n",
    "    if (np.isnan(history.history['val_loss'][0])): # NaN failure in first epoch\n",
    "        return model\n",
    "    else:\n",
    "        min_val_loss = history.history['val_loss'][0]\n",
    "        print('val_loss is {a:2.5f}, saving model to {b}'.format(a = min_val_loss, b = filepath))\n",
    "        model.save(filepath, save_format = 'h5')\n",
    "        \n",
    "    for i in range(epochs - 1):\n",
    "        print('Epoch {}/{}'.format(i + 2, epochs))\n",
    "        history = model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = 1, \n",
    "                      validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n",
    "        if (np.isnan(history.history['val_loss'][0])): # NaN failure  \n",
    "            break\n",
    "        else:\n",
    "            if (history.history['val_loss'][0] < min_val_loss):\n",
    "                print('val_loss improved from {a:2.5f} to {b:2.5f}, saving model to {c}'.format(\\\n",
    "                            a = min_val_loss, b = history.history['val_loss'][0], c = filepath))\n",
    "                model.save(filepath, save_format = 'h5')\n",
    "                min_val_loss = history.history['val_loss'][0]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:200: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "50/50 - 465s - loss: 0.0584 - f1_score_mod: 0.6254 - recall_mod: 0.5019 - precision_mod: 0.8332 - dur_error: 0.1663 - maestro_dur_loss: 0.0083 - val_loss: 0.0769 - val_f1_score_mod: 0.5730 - val_recall_mod: 0.4681 - val_precision_mod: 0.7386 - val_dur_error: 0.1661 - val_maestro_dur_loss: 0.0083\n",
      "val_loss is 0.07686, saving model to ../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2_0pt5_0pt8.h5\n",
      "Epoch 2/5\n",
      "50/50 - 466s - loss: 0.0535 - f1_score_mod: 0.6722 - recall_mod: 0.5621 - precision_mod: 0.8362 - dur_error: 0.1587 - maestro_dur_loss: 0.0079 - val_loss: 0.0775 - val_f1_score_mod: 0.5849 - val_recall_mod: 0.4870 - val_precision_mod: 0.7321 - val_dur_error: 0.1651 - val_maestro_dur_loss: 0.0083\n",
      "Epoch 3/5\n",
      "50/50 - 495s - loss: 0.0779 - f1_score_mod: 0.4704 - recall_mod: 0.3395 - precision_mod: 0.7686 - dur_error: 0.2103 - maestro_dur_loss: 0.0105 - val_loss: 0.0805 - val_f1_score_mod: 0.5077 - val_recall_mod: 0.3800 - val_precision_mod: 0.7653 - val_dur_error: 0.1843 - val_maestro_dur_loss: 0.0092\n",
      "Epoch 4/5\n",
      "50/50 - 457s - loss: 0.0657 - f1_score_mod: 0.5529 - recall_mod: 0.4204 - precision_mod: 0.8086 - dur_error: 0.1766 - maestro_dur_loss: 0.0088 - val_loss: 0.0786 - val_f1_score_mod: 0.5357 - val_recall_mod: 0.4138 - val_precision_mod: 0.7601 - val_dur_error: 0.1741 - val_maestro_dur_loss: 0.0087\n",
      "Epoch 5/5\n",
      "50/50 - 440s - loss: 0.0731 - f1_score_mod: 0.4960 - recall_mod: 0.3634 - precision_mod: 0.7827 - dur_error: 0.1877 - maestro_dur_loss: 0.0094 - val_loss: 0.0805 - val_f1_score_mod: 0.5116 - val_recall_mod: 0.3859 - val_precision_mod: 0.7595 - val_dur_error: 0.1768 - val_maestro_dur_loss: 0.0088\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5')\n",
    "pruned_model = prune_model(model, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model's minimum val_loss was 0.07825, so we have actually done slightly better with 80% of the weights! However, the saved model file is still 109 MB (exactly the same as before!). What happens if I try to save just the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model.save_weights('../models/best_pruned_maestro_model_weights_ext20_2_1_1024_0pt4_mnv_2_0pt5_0pt8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the weights this file is 54.5 MB, exactly as large as the weights file from the input model here. So the 0 weights must be explicitly being saved instead of severing the connections. How to save space and time on inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now following this [tutorial](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#create_3x_smaller_models_from_pruning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5')\n",
    "pruned_model = load_model_from_file('../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2_0pt5_0pt8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2_0pt5_0pt8/assets\n"
     ]
    }
   ],
   "source": [
    "pruned_model.save('../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2_0pt5_0pt8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/ps/lfq5vmk5793cw8f0kjkpnc3m0000gn/T/tmphrtwbgzb/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "tflite_pruned_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: /var/folders/ps/lfq5vmk5793cw8f0kjkpnc3m0000gn/T/tmpjpvqf2wm.h5\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer = False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter_contrib_nbextensions\n",
      "  Downloading jupyter_contrib_nbextensions-0.5.1-py2.py3-none-any.whl (20.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.9 MB 1.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: tornado in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (6.0.3)\n",
      "Requirement already satisfied: notebook>=4.0 in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (6.0.3)\n",
      "Requirement already satisfied: traitlets>=4.1 in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (4.3.3)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (4.5.0)\n",
      "Requirement already satisfied: nbconvert>=4.2 in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (5.6.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (5.3)\n",
      "Requirement already satisfied: ipython-genutils in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/anaconda3/lib/python3.7/site-packages (from jupyter_contrib_nbextensions) (4.6.1)\n",
      "Collecting jupyter-contrib-core>=0.3.3\n",
      "  Downloading jupyter_contrib_core-0.3.3-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from jupyter-contrib-core>=0.3.3->jupyter_contrib_nbextensions) (46.0.0.post20200309)\n",
      "Collecting jupyter-highlight-selected-word>=0.1.1\n",
      "  Downloading jupyter_highlight_selected_word-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jupyter-latex-envs>=1.3.8\n",
      "  Downloading jupyter_latex_envs-1.4.6.tar.gz (861 kB)\n",
      "\u001b[K     |████████████████████████████████| 861 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython in /opt/anaconda3/lib/python3.7/site-packages (from jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (7.12.0)\n",
      "Collecting jupyter-nbextensions-configurator>=0.4.0\n",
      "  Downloading jupyter_nbextensions_configurator-0.4.1.tar.gz (479 kB)\n",
      "\u001b[K     |████████████████████████████████| 479 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: bleach in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (3.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (0.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (2.11.1)\n",
      "Requirement already satisfied: testpath in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (1.4.2)\n",
      "Requirement already satisfied: pygments in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (2.5.2)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (5.0.4)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.7/site-packages (from nbconvert>=4.2->jupyter_contrib_nbextensions) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.7/site-packages (from jinja2>=2.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (1.1.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/anaconda3/lib/python3.7/site-packages (from nbformat>=4.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (3.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (1.14.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata in /opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (3.3.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_contrib_nbextensions) (18.1.1)\n",
      "Requirement already satisfied: ipykernel in /opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_contrib_nbextensions) (5.1.4)\n",
      "Requirement already satisfied: Send2Trash in /opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_contrib_nbextensions) (1.5.0)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_contrib_nbextensions) (5.3.4)\n",
      "Requirement already satisfied: prometheus-client in /opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_contrib_nbextensions) (0.7.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_contrib_nbextensions) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from jupyter-client>=5.3.4->notebook>=4.0->jupyter_contrib_nbextensions) (2.8.1)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.7/site-packages (from traitlets>=4.1->jupyter_contrib_nbextensions) (4.4.1)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert>=4.2->jupyter_contrib_nbextensions) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=4.2->jupyter_contrib_nbextensions) (3.7.4.3)\n",
      "Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.7/site-packages (from ipykernel->notebook>=4.0->jupyter_contrib_nbextensions) (0.1.0)\n",
      "Requirement already satisfied: backcall in /opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (3.0.3)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (0.14.1)\n",
      "Requirement already satisfied: pexpect in /opt/anaconda3/lib/python3.7/site-packages (from ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (4.8.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (0.1.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from pexpect->ipython->jupyter-latex-envs>=1.3.8->jupyter_contrib_nbextensions) (0.6.0)\n",
      "Building wheels for collected packages: jupyter-latex-envs, jupyter-nbextensions-configurator\n",
      "  Building wheel for jupyter-latex-envs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jupyter-latex-envs: filename=jupyter_latex_envs-1.4.6-py2.py3-none-any.whl size=963395 sha256=75c08e4a812f96afe2f4523f223e400d06d986989ddaf183c338416664ddc35f\n",
      "  Stored in directory: /Users/gilmer/Library/Caches/pip/wheels/a0/95/26/4cf34fb92765c95fb7851fd447511594bcc3a50e504bd09af9\n",
      "  Building wheel for jupyter-nbextensions-configurator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jupyter-nbextensions-configurator: filename=jupyter_nbextensions_configurator-0.4.1-py2.py3-none-any.whl size=465825 sha256=8fed342bbda2baf70d444a3f70c65d3502b5cf20b79c070b5dff99d810a70a67\n",
      "  Stored in directory: /Users/gilmer/Library/Caches/pip/wheels/8d/c4/b5/e4b61f624036f83566580d61f24af7b73180b1361ee1ab3722\n",
      "Successfully built jupyter-latex-envs jupyter-nbextensions-configurator\n",
      "Installing collected packages: jupyter-contrib-core, jupyter-nbextensions-configurator, jupyter-latex-envs, jupyter-highlight-selected-word, jupyter-contrib-nbextensions\n",
      "Successfully installed jupyter-contrib-core-0.3.3 jupyter-contrib-nbextensions-0.5.1 jupyter-highlight-selected-word-0.2.0 jupyter-latex-envs-1.4.6 jupyter-nbextensions-configurator-0.4.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last time the next cell wrote way too many characters\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 100000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causes error and writes way too much to the screen. Can't just get a piece because the buffer size must be > 10,000 characters\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to:  /var/folders/ps/lfq5vmk5793cw8f0kjkpnc3m0000gn/T/tmp2yprf4pu.h5\n",
      "Size of model before compression: 51.96 MB\n",
      "Size of model after compression: 48.16 MB\n",
      "Saving pruned_model to:  /var/folders/ps/lfq5vmk5793cw8f0kjkpnc3m0000gn/T/tmpjmb3dqr7.h5\n",
      "Size of model before compression: 51.96 MB\n",
      "Size of model after compression: 48.16 MB\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import zipfile\n",
    "from os import path\n",
    "\n",
    "models = [model, pruned_model]\n",
    "model_strings = ['model', 'pruned_model']\n",
    "for i in range(len(models)):\n",
    "\n",
    "    _, filepath = tempfile.mkstemp(\".h5\")\n",
    "    print(\"Saving {} to: \".format(model_strings[i]), filepath)\n",
    "    models[i].save_weights(filepath)\n",
    "\n",
    "    # Zip the .h5 model file\n",
    "    _, zip3 = tempfile.mkstemp(\".zip\")\n",
    "    with zipfile.ZipFile(zip3, \"w\", compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(filepath)\n",
    "\n",
    "    print('Size of {0} before compression: {1:.2f} MB'.format(cur_model, path.getsize(filepath) / float(2 ** 20)))\n",
    "    print('Size of {0} after compression: {1:.2f} MB'.format(cur_model, path.getsize(zip3) / float(2 ** 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
