{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data we created in data_read_and_process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../train_and_val/X_train.npy')\n",
    "X_val = np.load('../train_and_val/X_val.npy')\n",
    "y_train = np.load('../train_and_val/y_train.npy')\n",
    "y_val = np.load('../train_and_val/y_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4):\n",
    "    \"\"\"Generate a keras Sequential model of the form as described in Figure 16 of\n",
    "    https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_lstm_nodes, return_sequences = True, input_shape = (16, 89,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(1, n_lstm_layers - 1):\n",
    "        model.add(LSTM(n_lstm_nodes, return_sequences = True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(n_lstm_nodes))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_lstm_nodes // 2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(n_dense_layers - 1):\n",
    "        model.add(Dense(n_lstm_nodes // 2))\n",
    "        model.add(Dropout(0.6))\n",
    "    model.add(Dense(89))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'RMSProp')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss and Metrics\n",
    "\n",
    "\\begin{equation*}\n",
    "bce\\_loss = \\frac{1}{N} (\\sum_{i=1}^{N} y_i log(p(y_i)) + (1 - y_i) log(1 - p(y_i)))\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "maestro\\_loss = 2 * Harshness \\lvert\\frac{d_{true} - d_{pred}}{d_{true} + d{_{pred}}}\\rvert\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\\begin{equation*}\n",
    "total\\_loss = MIN(2 * bce\\_loss, bce\\_loss + maestro\\_loss)\n",
    "\\end{equation*}\n",
    "\n",
    "where N = num_keys_piano, <b>Harshness</b> is a constant to be determined, and <b>d</b> gives the normalized duration. I'll call it the <b>Maestro Loss Function</b> since it pays special attention to the timing of the notes. It is usually composed of a Binary Cross Entropy Term with an additional term proportional to the relative error in duration between $d_{true}$ and $d_{pred}$. However, we limit the total_loss to be less than twice the bce_loss. We also define custom metrics, read the docstrings for their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def maestro_loss_wr(harshness): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last element (which repesents the duration). \n",
    "    The proportionality constant is the 'harshness' of the maestro in regards to\n",
    "    timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-1] * K.log(ypred[:, :-1]) + (1 - ytrue[:, :-1]) * \\\n",
    "                     K.log(1 - ypred[:, :-1]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, -1] elements will be zero\n",
    "            return bce_loss * 2     # This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "\n",
    "def precision_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified precision excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    pred_positives = K.sum(K.round(ypred[:, :-1]))\n",
    "    return true_positives / (pred_positives + K.epsilon())\n",
    "\n",
    "def recall_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified recall excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    poss_positives = K.sum(ytrue[:, :-1])\n",
    "    return true_positives / (poss_positives + K.epsilon())\n",
    "\n",
    "def f1_score_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified f1_score excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    precision = precision_mod(ytrue, ypred)\n",
    "    recall = recall_mod(ytrue, ypred)   \n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def dur_error(ytrue, ypred):\n",
    "    \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "    return 2 * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / (ytrue[:, -1] + ypred[:, -1] + \\\n",
    "                                                         K.epsilon())))\n",
    "\n",
    "def maestro_dur_loss_wr(harshness):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cols_dict(history):\n",
    "    \"\"\"return a mapping of desired column names to the corresponding columns in the\n",
    "    history dictionary (previously history.history where history is the return value\n",
    "    of model.train)\"\"\"\n",
    "    return {'maestro_loss': history['loss'], 'f1_score': history['f1_score_mod'], \\\n",
    " 'precision': history['precision_mod'], 'recall': history['recall_mod'], \\\n",
    " 'dur_error': history['dur_error'], 'dur_loss': history['maestro_dur_loss'], \\\n",
    " 'val_maestro_loss': history['val_loss'], 'val_f1_score': history['val_f1_score_mod'], \\\n",
    " 'val_precision': history['val_precision_mod'], 'val_recall': history['val_recall_mod'], \\\n",
    " 'val_dur_error': history['val_dur_error'], 'val_dur_loss': history['val_maestro_dur_loss']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a similar model to [this paper](https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972). To save time, let us use 2 LSTM layers and 1 Dense layer instead of (4 and 3 as in the paper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4, \\\n",
    "                     batch_size = 512, harshness = 0.05, lr = None, clipnorm = None, clipvalue = None, \\\n",
    "                     epochs = 150):\n",
    "    \"\"\"Train a model using the passed parameters, the data, and using the RMSprop optimizer. Write the\n",
    "    best model as a .h5 and a .csv containing columns for the training and validation custom loss and\n",
    "    metrics. Returns nothing.\"\"\"\n",
    "    model = lstm(n_lstm_layers = n_lstm_layers, n_dense_layers = n_dense_layers, \\\n",
    "                 n_lstm_nodes = n_lstm_nodes, dropout_rate = dropout_rate)\n",
    "\n",
    "    if (lr or clipnorm or clipvalue):\n",
    "        if (lr):          # It's required that the first argument to RMSprop is not None\n",
    "            opt = RMSprop(lr = lr, clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        elif (clipnorm):\n",
    "            opt = RMSprop(clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        else: # clipvalue\n",
    "            opt = RMSprop(clipvalue = clipvalue)\n",
    "    else:\n",
    "        opt = RMSprop()   # TypeError when all are None, so do this instead\n",
    "        \n",
    "    model.compile(loss = maestro_loss_wr(harshness), optimizer = opt, metrics = [f1_score_mod, recall_mod, \\\n",
    "                                                precision_mod, dur_error, maestro_dur_loss_wr(harshness)])\n",
    "    \n",
    "    filename = 'best_maestro_model_{0}_{1}_{2}_{3}'.format(n_lstm_layers, n_dense_layers, n_lstm_nodes, \\\n",
    "                                                          str(dropout_rate).replace('.', 'pt'))\n",
    "    if (lr):\n",
    "        filename += '_lr_{}'.format('%.0e' % Decimal(lr))\n",
    "    if (clipnorm):\n",
    "        filename += '_cn_{}'.format(str(clipnorm).replace('.', 'pt'))     \n",
    "    if (clipvalue):\n",
    "        filename += '_cv_{}'.format(str(clipvalue).replace('.', 'pt'))\n",
    "                                   \n",
    "    mc = ModelCheckpoint('../models/' + filename + '.h5', monitor = 'val_loss', mode = 'min', \\\n",
    "                                                         save_best_only = True, verbose = 1)\n",
    "                                   \n",
    "    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "    \n",
    "    # In most preliminary tests model training has failed at some point when the loss becomes NaN during\n",
    "    # validation\n",
    "    if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "        for key, value in history.history.items():\n",
    "            if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "                value.append(np.nan)\n",
    "                \n",
    "    df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "    df.index.name = 'Epochs'\n",
    "    df.to_csv('../model_data/' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16027, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 144s - loss: 0.2178 - f1_score_mod: 0.0147 - recall_mod: 0.0239 - precision_mod: 0.0973 - dur_error: 1.0027 - maestro_dur_loss: 0.0501 - val_loss: 0.1603 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.7351 - val_maestro_dur_loss: 0.0368\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16027 to 0.14367, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 139s - loss: 0.1623 - f1_score_mod: 0.0012 - recall_mod: 6.0225e-04 - precision_mod: 0.1845 - dur_error: 0.7081 - maestro_dur_loss: 0.0354 - val_loss: 0.1437 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5956 - val_maestro_dur_loss: 0.0298\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14367 to 0.13357, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 141s - loss: 0.1522 - f1_score_mod: 0.0157 - recall_mod: 0.0081 - precision_mod: 0.5002 - dur_error: 0.6382 - maestro_dur_loss: 0.0319 - val_loss: 0.1336 - val_f1_score_mod: 0.0342 - val_recall_mod: 0.0176 - val_precision_mod: 0.6129 - val_dur_error: 0.4772 - val_maestro_dur_loss: 0.0239\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13357\n",
      "50/50 - 137s - loss: 0.1485 - f1_score_mod: 0.0502 - recall_mod: 0.0268 - precision_mod: 0.5722 - dur_error: 0.6483 - maestro_dur_loss: 0.0324 - val_loss: 0.1539 - val_f1_score_mod: 0.0304 - val_recall_mod: 0.0155 - val_precision_mod: 0.7247 - val_dur_error: 0.8902 - val_maestro_dur_loss: 0.0445\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13357 to 0.12677, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 139s - loss: 0.1454 - f1_score_mod: 0.0593 - recall_mod: 0.0314 - precision_mod: 0.6047 - dur_error: 0.6318 - maestro_dur_loss: 0.0316 - val_loss: 0.1268 - val_f1_score_mod: 0.0499 - val_recall_mod: 0.0259 - val_precision_mod: 0.7577 - val_dur_error: 0.4284 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12677 to 0.12560, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 158s - loss: 0.1404 - f1_score_mod: 0.0752 - recall_mod: 0.0403 - precision_mod: 0.6341 - dur_error: 0.5842 - maestro_dur_loss: 0.0292 - val_loss: 0.1256 - val_f1_score_mod: 0.0660 - val_recall_mod: 0.0347 - val_precision_mod: 0.7295 - val_dur_error: 0.4365 - val_maestro_dur_loss: 0.0218\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12560\n",
      "50/50 - 148s - loss: 0.1369 - f1_score_mod: 0.0947 - recall_mod: 0.0514 - precision_mod: 0.6431 - dur_error: 0.5667 - maestro_dur_loss: 0.0283 - val_loss: 0.1334 - val_f1_score_mod: 0.1019 - val_recall_mod: 0.0553 - val_precision_mod: 0.6685 - val_dur_error: 0.6264 - val_maestro_dur_loss: 0.0313\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12560 to 0.12124, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 147s - loss: 0.1346 - f1_score_mod: 0.1140 - recall_mod: 0.0628 - precision_mod: 0.6391 - dur_error: 0.5566 - maestro_dur_loss: 0.0278 - val_loss: 0.1212 - val_f1_score_mod: 0.1112 - val_recall_mod: 0.0607 - val_precision_mod: 0.6854 - val_dur_error: 0.4089 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12124 to 0.11828, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 142s - loss: 0.1330 - f1_score_mod: 0.1241 - recall_mod: 0.0689 - precision_mod: 0.6566 - dur_error: 0.5488 - maestro_dur_loss: 0.0274 - val_loss: 0.1183 - val_f1_score_mod: 0.1258 - val_recall_mod: 0.0691 - val_precision_mod: 0.7235 - val_dur_error: 0.3883 - val_maestro_dur_loss: 0.0194\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11828\n",
      "50/50 - 142s - loss: 0.1304 - f1_score_mod: 0.1395 - recall_mod: 0.0785 - precision_mod: 0.6574 - dur_error: 0.5339 - maestro_dur_loss: 0.0267 - val_loss: 0.1219 - val_f1_score_mod: 0.1350 - val_recall_mod: 0.0745 - val_precision_mod: 0.7458 - val_dur_error: 0.4720 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11828 to 0.11470, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 139s - loss: 0.1293 - f1_score_mod: 0.1538 - recall_mod: 0.0873 - precision_mod: 0.6671 - dur_error: 0.5302 - maestro_dur_loss: 0.0265 - val_loss: 0.1147 - val_f1_score_mod: 0.1339 - val_recall_mod: 0.0736 - val_precision_mod: 0.7666 - val_dur_error: 0.3493 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11470 to 0.11431, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1279 - f1_score_mod: 0.1620 - recall_mod: 0.0924 - precision_mod: 0.6796 - dur_error: 0.5205 - maestro_dur_loss: 0.0260 - val_loss: 0.1143 - val_f1_score_mod: 0.1528 - val_recall_mod: 0.0856 - val_precision_mod: 0.7352 - val_dur_error: 0.3535 - val_maestro_dur_loss: 0.0177\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11431\n",
      "50/50 - 133s - loss: 0.1265 - f1_score_mod: 0.1722 - recall_mod: 0.0986 - precision_mod: 0.6880 - dur_error: 0.5091 - maestro_dur_loss: 0.0255 - val_loss: 0.1190 - val_f1_score_mod: 0.1621 - val_recall_mod: 0.0915 - val_precision_mod: 0.7394 - val_dur_error: 0.4441 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11431\n",
      "50/50 - 132s - loss: 0.1260 - f1_score_mod: 0.1783 - recall_mod: 0.1030 - precision_mod: 0.6801 - dur_error: 0.5066 - maestro_dur_loss: 0.0253 - val_loss: 0.1183 - val_f1_score_mod: 0.1651 - val_recall_mod: 0.0930 - val_precision_mod: 0.7493 - val_dur_error: 0.4459 - val_maestro_dur_loss: 0.0223\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11431 to 0.11221, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 133s - loss: 0.1243 - f1_score_mod: 0.1857 - recall_mod: 0.1075 - precision_mod: 0.6956 - dur_error: 0.4946 - maestro_dur_loss: 0.0247 - val_loss: 0.1122 - val_f1_score_mod: 0.1732 - val_recall_mod: 0.0979 - val_precision_mod: 0.7670 - val_dur_error: 0.3494 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11221\n",
      "50/50 - 132s - loss: 0.1232 - f1_score_mod: 0.1975 - recall_mod: 0.1155 - precision_mod: 0.6958 - dur_error: 0.4851 - maestro_dur_loss: 0.0243 - val_loss: 0.1194 - val_f1_score_mod: 0.1931 - val_recall_mod: 0.1110 - val_precision_mod: 0.7631 - val_dur_error: 0.4989 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11221\n",
      "50/50 - 133s - loss: 0.1218 - f1_score_mod: 0.2051 - recall_mod: 0.1206 - precision_mod: 0.6983 - dur_error: 0.4763 - maestro_dur_loss: 0.0238 - val_loss: 0.1219 - val_f1_score_mod: 0.2237 - val_recall_mod: 0.1338 - val_precision_mod: 0.6946 - val_dur_error: 0.5460 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11221 to 0.10794, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 133s - loss: 0.1210 - f1_score_mod: 0.2113 - recall_mod: 0.1248 - precision_mod: 0.7033 - dur_error: 0.4738 - maestro_dur_loss: 0.0237 - val_loss: 0.1079 - val_f1_score_mod: 0.2173 - val_recall_mod: 0.1283 - val_precision_mod: 0.7242 - val_dur_error: 0.3038 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10794\n",
      "50/50 - 132s - loss: 0.1197 - f1_score_mod: 0.2211 - recall_mod: 0.1316 - precision_mod: 0.7010 - dur_error: 0.4608 - maestro_dur_loss: 0.0230 - val_loss: 0.1145 - val_f1_score_mod: 0.2379 - val_recall_mod: 0.1444 - val_precision_mod: 0.6890 - val_dur_error: 0.4393 - val_maestro_dur_loss: 0.0220\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10794\n",
      "50/50 - 133s - loss: 0.1187 - f1_score_mod: 0.2306 - recall_mod: 0.1381 - precision_mod: 0.7072 - dur_error: 0.4573 - maestro_dur_loss: 0.0229 - val_loss: 0.1171 - val_f1_score_mod: 0.2353 - val_recall_mod: 0.1416 - val_precision_mod: 0.7105 - val_dur_error: 0.4868 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10794\n",
      "50/50 - 132s - loss: 0.1174 - f1_score_mod: 0.2358 - recall_mod: 0.1416 - precision_mod: 0.7124 - dur_error: 0.4462 - maestro_dur_loss: 0.0223 - val_loss: 0.1096 - val_f1_score_mod: 0.2397 - val_recall_mod: 0.1435 - val_precision_mod: 0.7419 - val_dur_error: 0.3744 - val_maestro_dur_loss: 0.0187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10794\n",
      "50/50 - 132s - loss: 0.1166 - f1_score_mod: 0.2445 - recall_mod: 0.1483 - precision_mod: 0.7097 - dur_error: 0.4476 - maestro_dur_loss: 0.0224 - val_loss: 0.1109 - val_f1_score_mod: 0.2456 - val_recall_mod: 0.1484 - val_precision_mod: 0.7209 - val_dur_error: 0.3960 - val_maestro_dur_loss: 0.0198\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10794\n",
      "50/50 - 133s - loss: 0.1163 - f1_score_mod: 0.2509 - recall_mod: 0.1524 - precision_mod: 0.7178 - dur_error: 0.4463 - maestro_dur_loss: 0.0223 - val_loss: 0.1159 - val_f1_score_mod: 0.2710 - val_recall_mod: 0.1697 - val_precision_mod: 0.6842 - val_dur_error: 0.4873 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10794\n",
      "50/50 - 133s - loss: 0.1156 - f1_score_mod: 0.2587 - recall_mod: 0.1582 - precision_mod: 0.7175 - dur_error: 0.4444 - maestro_dur_loss: 0.0222 - val_loss: 0.1138 - val_f1_score_mod: 0.2740 - val_recall_mod: 0.1719 - val_precision_mod: 0.6852 - val_dur_error: 0.4532 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10794\n",
      "50/50 - 133s - loss: 0.1140 - f1_score_mod: 0.2646 - recall_mod: 0.1627 - precision_mod: 0.7164 - dur_error: 0.4275 - maestro_dur_loss: 0.0214 - val_loss: 0.1084 - val_f1_score_mod: 0.2647 - val_recall_mod: 0.1617 - val_precision_mod: 0.7403 - val_dur_error: 0.3831 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10794\n",
      "50/50 - 133s - loss: 0.1132 - f1_score_mod: 0.2736 - recall_mod: 0.1691 - precision_mod: 0.7239 - dur_error: 0.4320 - maestro_dur_loss: 0.0216 - val_loss: 0.1136 - val_f1_score_mod: 0.2840 - val_recall_mod: 0.1782 - val_precision_mod: 0.7094 - val_dur_error: 0.4799 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10794\n",
      "50/50 - 133s - loss: 0.1130 - f1_score_mod: 0.2795 - recall_mod: 0.1739 - precision_mod: 0.7212 - dur_error: 0.4340 - maestro_dur_loss: 0.0217 - val_loss: 0.1104 - val_f1_score_mod: 0.2955 - val_recall_mod: 0.1879 - val_precision_mod: 0.7023 - val_dur_error: 0.4203 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10794\n",
      "50/50 - 133s - loss: 0.1119 - f1_score_mod: 0.2861 - recall_mod: 0.1785 - precision_mod: 0.7275 - dur_error: 0.4243 - maestro_dur_loss: 0.0212 - val_loss: 0.1082 - val_f1_score_mod: 0.3035 - val_recall_mod: 0.1945 - val_precision_mod: 0.7020 - val_dur_error: 0.3989 - val_maestro_dur_loss: 0.0199\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10794\n",
      "50/50 - 132s - loss: 0.1115 - f1_score_mod: 0.2944 - recall_mod: 0.1852 - precision_mod: 0.7247 - dur_error: 0.4262 - maestro_dur_loss: 0.0213 - val_loss: 0.1129 - val_f1_score_mod: 0.2931 - val_recall_mod: 0.1845 - val_precision_mod: 0.7265 - val_dur_error: 0.4937 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10794 to 0.10316, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 132s - loss: 0.1102 - f1_score_mod: 0.2994 - recall_mod: 0.1886 - precision_mod: 0.7314 - dur_error: 0.4204 - maestro_dur_loss: 0.0210 - val_loss: 0.1032 - val_f1_score_mod: 0.2884 - val_recall_mod: 0.1807 - val_precision_mod: 0.7299 - val_dur_error: 0.2956 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10316\n",
      "50/50 - 132s - loss: 0.1089 - f1_score_mod: 0.3068 - recall_mod: 0.1945 - precision_mod: 0.7328 - dur_error: 0.4118 - maestro_dur_loss: 0.0206 - val_loss: 0.1053 - val_f1_score_mod: 0.2983 - val_recall_mod: 0.1871 - val_precision_mod: 0.7472 - val_dur_error: 0.3599 - val_maestro_dur_loss: 0.0180\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10316 to 0.10279, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1083 - f1_score_mod: 0.3158 - recall_mod: 0.2019 - precision_mod: 0.7306 - dur_error: 0.4113 - maestro_dur_loss: 0.0206 - val_loss: 0.1028 - val_f1_score_mod: 0.2950 - val_recall_mod: 0.1840 - val_precision_mod: 0.7566 - val_dur_error: 0.3188 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.10279 to 0.10080, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 132s - loss: 0.1077 - f1_score_mod: 0.3197 - recall_mod: 0.2051 - precision_mod: 0.7280 - dur_error: 0.4115 - maestro_dur_loss: 0.0206 - val_loss: 0.1008 - val_f1_score_mod: 0.3208 - val_recall_mod: 0.2061 - val_precision_mod: 0.7367 - val_dur_error: 0.2918 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10080 to 0.09961, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 132s - loss: 0.1069 - f1_score_mod: 0.3265 - recall_mod: 0.2103 - precision_mod: 0.7346 - dur_error: 0.4076 - maestro_dur_loss: 0.0204 - val_loss: 0.0996 - val_f1_score_mod: 0.3236 - val_recall_mod: 0.2095 - val_precision_mod: 0.7198 - val_dur_error: 0.2777 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09961\n",
      "50/50 - 132s - loss: 0.1060 - f1_score_mod: 0.3320 - recall_mod: 0.2149 - precision_mod: 0.7324 - dur_error: 0.4028 - maestro_dur_loss: 0.0201 - val_loss: 0.0999 - val_f1_score_mod: 0.3236 - val_recall_mod: 0.2085 - val_precision_mod: 0.7357 - val_dur_error: 0.2906 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09961\n",
      "50/50 - 133s - loss: 0.1054 - f1_score_mod: 0.3346 - recall_mod: 0.2173 - precision_mod: 0.7335 - dur_error: 0.4039 - maestro_dur_loss: 0.0202 - val_loss: 0.1008 - val_f1_score_mod: 0.3240 - val_recall_mod: 0.2086 - val_precision_mod: 0.7359 - val_dur_error: 0.3157 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09961\n",
      "50/50 - 132s - loss: 0.1044 - f1_score_mod: 0.3462 - recall_mod: 0.2264 - precision_mod: 0.7391 - dur_error: 0.4003 - maestro_dur_loss: 0.0200 - val_loss: 0.1026 - val_f1_score_mod: 0.3218 - val_recall_mod: 0.2062 - val_precision_mod: 0.7408 - val_dur_error: 0.3661 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09961 to 0.09783, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 133s - loss: 0.1036 - f1_score_mod: 0.3508 - recall_mod: 0.2306 - precision_mod: 0.7375 - dur_error: 0.3966 - maestro_dur_loss: 0.0198 - val_loss: 0.0978 - val_f1_score_mod: 0.3556 - val_recall_mod: 0.2395 - val_precision_mod: 0.6963 - val_dur_error: 0.2696 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09783\n",
      "50/50 - 131s - loss: 0.1022 - f1_score_mod: 0.3592 - recall_mod: 0.2380 - precision_mod: 0.7371 - dur_error: 0.3890 - maestro_dur_loss: 0.0195 - val_loss: 0.0980 - val_f1_score_mod: 0.3474 - val_recall_mod: 0.2295 - val_precision_mod: 0.7238 - val_dur_error: 0.2854 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09783 to 0.09718, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 135s - loss: 0.1018 - f1_score_mod: 0.3637 - recall_mod: 0.2417 - precision_mod: 0.7371 - dur_error: 0.3912 - maestro_dur_loss: 0.0196 - val_loss: 0.0972 - val_f1_score_mod: 0.3383 - val_recall_mod: 0.2197 - val_precision_mod: 0.7433 - val_dur_error: 0.2779 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09718 to 0.09648, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 132s - loss: 0.1012 - f1_score_mod: 0.3657 - recall_mod: 0.2435 - precision_mod: 0.7394 - dur_error: 0.3889 - maestro_dur_loss: 0.0194 - val_loss: 0.0965 - val_f1_score_mod: 0.3463 - val_recall_mod: 0.2285 - val_precision_mod: 0.7207 - val_dur_error: 0.2666 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09648\n",
      "50/50 - 131s - loss: 0.1003 - f1_score_mod: 0.3742 - recall_mod: 0.2507 - precision_mod: 0.7416 - dur_error: 0.3881 - maestro_dur_loss: 0.0194 - val_loss: 0.1003 - val_f1_score_mod: 0.3552 - val_recall_mod: 0.2363 - val_precision_mod: 0.7231 - val_dur_error: 0.3500 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09648\n",
      "50/50 - 133s - loss: 0.0996 - f1_score_mod: 0.3812 - recall_mod: 0.2571 - precision_mod: 0.7404 - dur_error: 0.3868 - maestro_dur_loss: 0.0193 - val_loss: 0.1000 - val_f1_score_mod: 0.3697 - val_recall_mod: 0.2508 - val_precision_mod: 0.7089 - val_dur_error: 0.3514 - val_maestro_dur_loss: 0.0176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09648\n",
      "50/50 - 132s - loss: 0.0986 - f1_score_mod: 0.3880 - recall_mod: 0.2631 - precision_mod: 0.7420 - dur_error: 0.3807 - maestro_dur_loss: 0.0190 - val_loss: 0.0968 - val_f1_score_mod: 0.3539 - val_recall_mod: 0.2321 - val_precision_mod: 0.7494 - val_dur_error: 0.2912 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09648 to 0.09494, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.0978 - f1_score_mod: 0.3927 - recall_mod: 0.2674 - precision_mod: 0.7429 - dur_error: 0.3798 - maestro_dur_loss: 0.0190 - val_loss: 0.0949 - val_f1_score_mod: 0.3634 - val_recall_mod: 0.2417 - val_precision_mod: 0.7377 - val_dur_error: 0.2629 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 46/150\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09494\n",
      "50/50 - 133s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll refer to the above model as the 'base_model'. Looks like we get a NaN loss while the model was still improving. This seems to be case of exploding gradients. To combat this, we can try lowering the learning rate as suggested [here](https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14697, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 139s - loss: 0.2451 - f1_score_mod: 0.0284 - recall_mod: 0.0380 - precision_mod: 0.0799 - dur_error: 1.0205 - maestro_dur_loss: 0.0510 - val_loss: 0.1470 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5032 - val_maestro_dur_loss: 0.0252\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14697\n",
      "50/50 - 144s - loss: 0.1690 - f1_score_mod: 0.0061 - recall_mod: 0.0031 - precision_mod: 0.1665 - dur_error: 0.7262 - maestro_dur_loss: 0.0363 - val_loss: 0.1497 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6078 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14697 to 0.13265, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 166s - loss: 0.1562 - f1_score_mod: 0.0103 - recall_mod: 0.0052 - precision_mod: 0.3866 - dur_error: 0.6431 - maestro_dur_loss: 0.0322 - val_loss: 0.1326 - val_f1_score_mod: 4.5386e-04 - val_recall_mod: 2.2718e-04 - val_precision_mod: 0.2576 - val_dur_error: 0.4128 - val_maestro_dur_loss: 0.0206\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13265\n",
      "50/50 - 153s - loss: 0.1493 - f1_score_mod: 0.0256 - recall_mod: 0.0132 - precision_mod: 0.4965 - dur_error: 0.6072 - maestro_dur_loss: 0.0304 - val_loss: 0.1352 - val_f1_score_mod: 0.0235 - val_recall_mod: 0.0120 - val_precision_mod: 0.7213 - val_dur_error: 0.5133 - val_maestro_dur_loss: 0.0257\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13265 to 0.12797, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 152s - loss: 0.1448 - f1_score_mod: 0.0426 - recall_mod: 0.0222 - precision_mod: 0.5635 - dur_error: 0.5810 - maestro_dur_loss: 0.0291 - val_loss: 0.1280 - val_f1_score_mod: 0.0466 - val_recall_mod: 0.0242 - val_precision_mod: 0.7093 - val_dur_error: 0.4204 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12797 to 0.12493, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 137s - loss: 0.1413 - f1_score_mod: 0.0613 - recall_mod: 0.0325 - precision_mod: 0.6107 - dur_error: 0.5673 - maestro_dur_loss: 0.0284 - val_loss: 0.1249 - val_f1_score_mod: 0.0576 - val_recall_mod: 0.0301 - val_precision_mod: 0.7283 - val_dur_error: 0.3900 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12493\n",
      "50/50 - 140s - loss: 0.1381 - f1_score_mod: 0.0748 - recall_mod: 0.0399 - precision_mod: 0.6343 - dur_error: 0.5488 - maestro_dur_loss: 0.0274 - val_loss: 0.1254 - val_f1_score_mod: 0.0633 - val_recall_mod: 0.0332 - val_precision_mod: 0.7353 - val_dur_error: 0.4317 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12493 to 0.12253, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 137s - loss: 0.1361 - f1_score_mod: 0.0913 - recall_mod: 0.0493 - precision_mod: 0.6485 - dur_error: 0.5405 - maestro_dur_loss: 0.0270 - val_loss: 0.1225 - val_f1_score_mod: 0.0825 - val_recall_mod: 0.0439 - val_precision_mod: 0.7190 - val_dur_error: 0.4056 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12253 to 0.12046, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 172s - loss: 0.1340 - f1_score_mod: 0.1055 - recall_mod: 0.0576 - precision_mod: 0.6589 - dur_error: 0.5303 - maestro_dur_loss: 0.0265 - val_loss: 0.1205 - val_f1_score_mod: 0.1271 - val_recall_mod: 0.0700 - val_precision_mod: 0.7025 - val_dur_error: 0.3894 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12046 to 0.11870, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 187s - loss: 0.1324 - f1_score_mod: 0.1215 - recall_mod: 0.0671 - precision_mod: 0.6623 - dur_error: 0.5230 - maestro_dur_loss: 0.0261 - val_loss: 0.1187 - val_f1_score_mod: 0.1233 - val_recall_mod: 0.0675 - val_precision_mod: 0.7359 - val_dur_error: 0.3703 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11870\n",
      "50/50 - 184s - loss: 0.1308 - f1_score_mod: 0.1293 - recall_mod: 0.0718 - precision_mod: 0.6724 - dur_error: 0.5145 - maestro_dur_loss: 0.0257 - val_loss: 0.1258 - val_f1_score_mod: 0.1462 - val_recall_mod: 0.0818 - val_precision_mod: 0.7025 - val_dur_error: 0.5276 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11870 to 0.11678, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1293 - f1_score_mod: 0.1439 - recall_mod: 0.0808 - precision_mod: 0.6725 - dur_error: 0.5061 - maestro_dur_loss: 0.0253 - val_loss: 0.1168 - val_f1_score_mod: 0.1472 - val_recall_mod: 0.0820 - val_precision_mod: 0.7360 - val_dur_error: 0.3680 - val_maestro_dur_loss: 0.0184\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11678 to 0.11629, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1279 - f1_score_mod: 0.1538 - recall_mod: 0.0870 - precision_mod: 0.6827 - dur_error: 0.4908 - maestro_dur_loss: 0.0245 - val_loss: 0.1163 - val_f1_score_mod: 0.1469 - val_recall_mod: 0.0818 - val_precision_mod: 0.7362 - val_dur_error: 0.3736 - val_maestro_dur_loss: 0.0187\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11629\n",
      "50/50 - 184s - loss: 0.1266 - f1_score_mod: 0.1605 - recall_mod: 0.0912 - precision_mod: 0.6839 - dur_error: 0.4816 - maestro_dur_loss: 0.0241 - val_loss: 0.1177 - val_f1_score_mod: 0.1828 - val_recall_mod: 0.1055 - val_precision_mod: 0.6932 - val_dur_error: 0.4056 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11629\n",
      "50/50 - 185s - loss: 0.1253 - f1_score_mod: 0.1678 - recall_mod: 0.0958 - precision_mod: 0.6880 - dur_error: 0.4700 - maestro_dur_loss: 0.0235 - val_loss: 0.1164 - val_f1_score_mod: 0.1777 - val_recall_mod: 0.1022 - val_precision_mod: 0.6923 - val_dur_error: 0.3908 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11629 to 0.11281, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 186s - loss: 0.1242 - f1_score_mod: 0.1744 - recall_mod: 0.1004 - precision_mod: 0.6780 - dur_error: 0.4634 - maestro_dur_loss: 0.0232 - val_loss: 0.1128 - val_f1_score_mod: 0.1960 - val_recall_mod: 0.1145 - val_precision_mod: 0.6939 - val_dur_error: 0.3246 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11281\n",
      "50/50 - 185s - loss: 0.1234 - f1_score_mod: 0.1813 - recall_mod: 0.1046 - precision_mod: 0.6921 - dur_error: 0.4614 - maestro_dur_loss: 0.0231 - val_loss: 0.1187 - val_f1_score_mod: 0.1880 - val_recall_mod: 0.1084 - val_precision_mod: 0.7244 - val_dur_error: 0.4590 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11281 to 0.11086, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 189s - loss: 0.1226 - f1_score_mod: 0.1882 - recall_mod: 0.1094 - precision_mod: 0.6913 - dur_error: 0.4542 - maestro_dur_loss: 0.0227 - val_loss: 0.1109 - val_f1_score_mod: 0.1901 - val_recall_mod: 0.1097 - val_precision_mod: 0.7240 - val_dur_error: 0.3224 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11086\n",
      "50/50 - 186s - loss: 0.1216 - f1_score_mod: 0.1938 - recall_mod: 0.1132 - precision_mod: 0.6819 - dur_error: 0.4467 - maestro_dur_loss: 0.0223 - val_loss: 0.1113 - val_f1_score_mod: 0.2082 - val_recall_mod: 0.1223 - val_precision_mod: 0.7114 - val_dur_error: 0.3392 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11086\n",
      "50/50 - 186s - loss: 0.1207 - f1_score_mod: 0.2040 - recall_mod: 0.1197 - precision_mod: 0.7017 - dur_error: 0.4411 - maestro_dur_loss: 0.0221 - val_loss: 0.1114 - val_f1_score_mod: 0.2112 - val_recall_mod: 0.1242 - val_precision_mod: 0.7188 - val_dur_error: 0.3434 - val_maestro_dur_loss: 0.0172\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11086\n",
      "50/50 - 186s - loss: 0.1201 - f1_score_mod: 0.2070 - recall_mod: 0.1220 - precision_mod: 0.6986 - dur_error: 0.4388 - maestro_dur_loss: 0.0219 - val_loss: 0.1195 - val_f1_score_mod: 0.2059 - val_recall_mod: 0.1199 - val_precision_mod: 0.7418 - val_dur_error: 0.5090 - val_maestro_dur_loss: 0.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.11086\n",
      "50/50 - 186s - loss: 0.1196 - f1_score_mod: 0.2136 - recall_mod: 0.1260 - precision_mod: 0.7094 - dur_error: 0.4412 - maestro_dur_loss: 0.0221 - val_loss: 0.1123 - val_f1_score_mod: 0.2055 - val_recall_mod: 0.1190 - val_precision_mod: 0.7627 - val_dur_error: 0.3789 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.11086 to 0.10807, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 187s - loss: 0.1185 - f1_score_mod: 0.2234 - recall_mod: 0.1329 - precision_mod: 0.7121 - dur_error: 0.4327 - maestro_dur_loss: 0.0216 - val_loss: 0.1081 - val_f1_score_mod: 0.2185 - val_recall_mod: 0.1288 - val_precision_mod: 0.7301 - val_dur_error: 0.3028 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10807 to 0.10730, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 188s - loss: 0.1178 - f1_score_mod: 0.2252 - recall_mod: 0.1344 - precision_mod: 0.7041 - dur_error: 0.4272 - maestro_dur_loss: 0.0214 - val_loss: 0.1073 - val_f1_score_mod: 0.2209 - val_recall_mod: 0.1301 - val_precision_mod: 0.7461 - val_dur_error: 0.2975 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10730\n",
      "50/50 - 189s - loss: 0.1171 - f1_score_mod: 0.2313 - recall_mod: 0.1382 - precision_mod: 0.7191 - dur_error: 0.4232 - maestro_dur_loss: 0.0212 - val_loss: 0.1119 - val_f1_score_mod: 0.2519 - val_recall_mod: 0.1545 - val_precision_mod: 0.6896 - val_dur_error: 0.3903 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10730\n",
      "50/50 - 186s - loss: 0.1164 - f1_score_mod: 0.2401 - recall_mod: 0.1449 - precision_mod: 0.7110 - dur_error: 0.4202 - maestro_dur_loss: 0.0210 - val_loss: 0.1083 - val_f1_score_mod: 0.2392 - val_recall_mod: 0.1441 - val_precision_mod: 0.7146 - val_dur_error: 0.3279 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10730 to 0.10517, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1160 - f1_score_mod: 0.2402 - recall_mod: 0.1448 - precision_mod: 0.7132 - dur_error: 0.4202 - maestro_dur_loss: 0.0210 - val_loss: 0.1052 - val_f1_score_mod: 0.2224 - val_recall_mod: 0.1303 - val_precision_mod: 0.7735 - val_dur_error: 0.2799 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10517\n",
      "50/50 - 189s - loss: 0.1150 - f1_score_mod: 0.2477 - recall_mod: 0.1502 - precision_mod: 0.7136 - dur_error: 0.4134 - maestro_dur_loss: 0.0207 - val_loss: 0.1140 - val_f1_score_mod: 0.2484 - val_recall_mod: 0.1512 - val_precision_mod: 0.7080 - val_dur_error: 0.4441 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10517\n",
      "50/50 - 187s - loss: 0.1149 - f1_score_mod: 0.2541 - recall_mod: 0.1547 - precision_mod: 0.7181 - dur_error: 0.4195 - maestro_dur_loss: 0.0210 - val_loss: 0.1059 - val_f1_score_mod: 0.2626 - val_recall_mod: 0.1616 - val_precision_mod: 0.7107 - val_dur_error: 0.3053 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10517\n",
      "50/50 - 187s - loss: 0.1141 - f1_score_mod: 0.2601 - recall_mod: 0.1592 - precision_mod: 0.7199 - dur_error: 0.4122 - maestro_dur_loss: 0.0206 - val_loss: 0.1102 - val_f1_score_mod: 0.2601 - val_recall_mod: 0.1595 - val_precision_mod: 0.7158 - val_dur_error: 0.3910 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10517\n",
      "50/50 - 188s - loss: 0.1133 - f1_score_mod: 0.2649 - recall_mod: 0.1623 - precision_mod: 0.7248 - dur_error: 0.4105 - maestro_dur_loss: 0.0205 - val_loss: 0.1157 - val_f1_score_mod: 0.2787 - val_recall_mod: 0.1746 - val_precision_mod: 0.6997 - val_dur_error: 0.5073 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10517 to 0.10385, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 186s - loss: 0.1128 - f1_score_mod: 0.2716 - recall_mod: 0.1674 - precision_mod: 0.7277 - dur_error: 0.4077 - maestro_dur_loss: 0.0204 - val_loss: 0.1038 - val_f1_score_mod: 0.2702 - val_recall_mod: 0.1674 - val_precision_mod: 0.7119 - val_dur_error: 0.2775 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10385\n",
      "50/50 - 186s - loss: 0.1124 - f1_score_mod: 0.2740 - recall_mod: 0.1694 - precision_mod: 0.7233 - dur_error: 0.4099 - maestro_dur_loss: 0.0205 - val_loss: 0.1039 - val_f1_score_mod: 0.2733 - val_recall_mod: 0.1691 - val_precision_mod: 0.7220 - val_dur_error: 0.2947 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10385\n",
      "50/50 - 186s - loss: 0.1116 - f1_score_mod: 0.2775 - recall_mod: 0.1717 - precision_mod: 0.7282 - dur_error: 0.4017 - maestro_dur_loss: 0.0201 - val_loss: 0.1044 - val_f1_score_mod: 0.2728 - val_recall_mod: 0.1686 - val_precision_mod: 0.7220 - val_dur_error: 0.3059 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10385\n",
      "50/50 - 186s - loss: 0.1112 - f1_score_mod: 0.2834 - recall_mod: 0.1762 - precision_mod: 0.7305 - dur_error: 0.4028 - maestro_dur_loss: 0.0201 - val_loss: 0.1061 - val_f1_score_mod: 0.2733 - val_recall_mod: 0.1682 - val_precision_mod: 0.7409 - val_dur_error: 0.3468 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10385\n",
      "50/50 - 184s - loss: 0.1103 - f1_score_mod: 0.2889 - recall_mod: 0.1807 - precision_mod: 0.7283 - dur_error: 0.3963 - maestro_dur_loss: 0.0198 - val_loss: 0.1076 - val_f1_score_mod: 0.2769 - val_recall_mod: 0.1716 - val_precision_mod: 0.7335 - val_dur_error: 0.3771 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.10385 to 0.10202, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1103 - f1_score_mod: 0.2949 - recall_mod: 0.1853 - precision_mod: 0.7295 - dur_error: 0.4037 - maestro_dur_loss: 0.0202 - val_loss: 0.1020 - val_f1_score_mod: 0.2990 - val_recall_mod: 0.1897 - val_precision_mod: 0.7137 - val_dur_error: 0.2748 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10202 to 0.10170, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1092 - f1_score_mod: 0.3018 - recall_mod: 0.1901 - precision_mod: 0.7371 - dur_error: 0.3937 - maestro_dur_loss: 0.0197 - val_loss: 0.1017 - val_f1_score_mod: 0.2855 - val_recall_mod: 0.1766 - val_precision_mod: 0.7564 - val_dur_error: 0.2743 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.10170\n",
      "50/50 - 184s - loss: 0.1089 - f1_score_mod: 0.3047 - recall_mod: 0.1925 - precision_mod: 0.7340 - dur_error: 0.3957 - maestro_dur_loss: 0.0198 - val_loss: 0.1073 - val_f1_score_mod: 0.2907 - val_recall_mod: 0.1816 - val_precision_mod: 0.7402 - val_dur_error: 0.3888 - val_maestro_dur_loss: 0.0194\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.10170 to 0.10125, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1084 - f1_score_mod: 0.3101 - recall_mod: 0.1971 - precision_mod: 0.7325 - dur_error: 0.3916 - maestro_dur_loss: 0.0196 - val_loss: 0.1012 - val_f1_score_mod: 0.2883 - val_recall_mod: 0.1786 - val_precision_mod: 0.7544 - val_dur_error: 0.2729 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10125\n",
      "50/50 - 190s - loss: 0.1080 - f1_score_mod: 0.3124 - recall_mod: 0.1989 - precision_mod: 0.7311 - dur_error: 0.3915 - maestro_dur_loss: 0.0196 - val_loss: 0.1028 - val_f1_score_mod: 0.3076 - val_recall_mod: 0.1952 - val_precision_mod: 0.7347 - val_dur_error: 0.3176 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.10125 to 0.10058, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 186s - loss: 0.1074 - f1_score_mod: 0.3174 - recall_mod: 0.2029 - precision_mod: 0.7346 - dur_error: 0.3888 - maestro_dur_loss: 0.0194 - val_loss: 0.1006 - val_f1_score_mod: 0.3034 - val_recall_mod: 0.1907 - val_precision_mod: 0.7511 - val_dur_error: 0.2865 - val_maestro_dur_loss: 0.0143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.10058 to 0.10008, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1069 - f1_score_mod: 0.3248 - recall_mod: 0.2084 - precision_mod: 0.7397 - dur_error: 0.3887 - maestro_dur_loss: 0.0194 - val_loss: 0.1001 - val_f1_score_mod: 0.3061 - val_recall_mod: 0.1939 - val_precision_mod: 0.7343 - val_dur_error: 0.2650 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.10008\n",
      "50/50 - 185s - loss: 0.1065 - f1_score_mod: 0.3294 - recall_mod: 0.2124 - precision_mod: 0.7412 - dur_error: 0.3886 - maestro_dur_loss: 0.0194 - val_loss: 0.1007 - val_f1_score_mod: 0.3231 - val_recall_mod: 0.2114 - val_precision_mod: 0.6937 - val_dur_error: 0.2771 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.10008 to 0.09987, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.1058 - f1_score_mod: 0.3317 - recall_mod: 0.2145 - precision_mod: 0.7365 - dur_error: 0.3875 - maestro_dur_loss: 0.0194 - val_loss: 0.0999 - val_f1_score_mod: 0.3151 - val_recall_mod: 0.2005 - val_precision_mod: 0.7470 - val_dur_error: 0.2745 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09987\n",
      "50/50 - 186s - loss: 0.1050 - f1_score_mod: 0.3370 - recall_mod: 0.2182 - precision_mod: 0.7442 - dur_error: 0.3818 - maestro_dur_loss: 0.0191 - val_loss: 0.1024 - val_f1_score_mod: 0.3326 - val_recall_mod: 0.2178 - val_precision_mod: 0.7126 - val_dur_error: 0.3300 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09987 to 0.09845, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 187s - loss: 0.1047 - f1_score_mod: 0.3413 - recall_mod: 0.2221 - precision_mod: 0.7404 - dur_error: 0.3840 - maestro_dur_loss: 0.0192 - val_loss: 0.0985 - val_f1_score_mod: 0.3308 - val_recall_mod: 0.2141 - val_precision_mod: 0.7354 - val_dur_error: 0.2653 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09845\n",
      "50/50 - 188s - loss: 0.1041 - f1_score_mod: 0.3482 - recall_mod: 0.2279 - precision_mod: 0.7416 - dur_error: 0.3805 - maestro_dur_loss: 0.0190 - val_loss: 0.0991 - val_f1_score_mod: 0.3323 - val_recall_mod: 0.2154 - val_precision_mod: 0.7355 - val_dur_error: 0.2835 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09845 to 0.09835, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 187s - loss: 0.1038 - f1_score_mod: 0.3470 - recall_mod: 0.2270 - precision_mod: 0.7401 - dur_error: 0.3764 - maestro_dur_loss: 0.0188 - val_loss: 0.0983 - val_f1_score_mod: 0.3554 - val_recall_mod: 0.2390 - val_precision_mod: 0.6995 - val_dur_error: 0.2643 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09835\n",
      "50/50 - 187s - loss: 0.1032 - f1_score_mod: 0.3553 - recall_mod: 0.2337 - precision_mod: 0.7431 - dur_error: 0.3814 - maestro_dur_loss: 0.0191 - val_loss: 0.0991 - val_f1_score_mod: 0.3317 - val_recall_mod: 0.2137 - val_precision_mod: 0.7486 - val_dur_error: 0.2793 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09835 to 0.09746, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 188s - loss: 0.1024 - f1_score_mod: 0.3568 - recall_mod: 0.2352 - precision_mod: 0.7427 - dur_error: 0.3751 - maestro_dur_loss: 0.0188 - val_loss: 0.0975 - val_f1_score_mod: 0.3474 - val_recall_mod: 0.2292 - val_precision_mod: 0.7250 - val_dur_error: 0.2584 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09746\n",
      "50/50 - 187s - loss: 0.1019 - f1_score_mod: 0.3615 - recall_mod: 0.2386 - precision_mod: 0.7486 - dur_error: 0.3722 - maestro_dur_loss: 0.0186 - val_loss: 0.0980 - val_f1_score_mod: 0.3406 - val_recall_mod: 0.2210 - val_precision_mod: 0.7505 - val_dur_error: 0.2816 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09746 to 0.09719, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 186s - loss: 0.1015 - f1_score_mod: 0.3680 - recall_mod: 0.2443 - precision_mod: 0.7495 - dur_error: 0.3734 - maestro_dur_loss: 0.0187 - val_loss: 0.0972 - val_f1_score_mod: 0.3539 - val_recall_mod: 0.2362 - val_precision_mod: 0.7136 - val_dur_error: 0.2626 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09719\n",
      "50/50 - 185s - loss: 0.1011 - f1_score_mod: 0.3688 - recall_mod: 0.2455 - precision_mod: 0.7449 - dur_error: 0.3739 - maestro_dur_loss: 0.0187 - val_loss: 0.1037 - val_f1_score_mod: 0.3609 - val_recall_mod: 0.2422 - val_precision_mod: 0.7155 - val_dur_error: 0.3972 - val_maestro_dur_loss: 0.0199\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09719\n",
      "50/50 - 185s - loss: 0.1004 - f1_score_mod: 0.3760 - recall_mod: 0.2514 - precision_mod: 0.7496 - dur_error: 0.3689 - maestro_dur_loss: 0.0184 - val_loss: 0.1023 - val_f1_score_mod: 0.3652 - val_recall_mod: 0.2467 - val_precision_mod: 0.7078 - val_dur_error: 0.3664 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09719 to 0.09624, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 186s - loss: 0.1002 - f1_score_mod: 0.3771 - recall_mod: 0.2530 - precision_mod: 0.7436 - dur_error: 0.3722 - maestro_dur_loss: 0.0186 - val_loss: 0.0962 - val_f1_score_mod: 0.3603 - val_recall_mod: 0.2400 - val_precision_mod: 0.7290 - val_dur_error: 0.2583 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.09624\n",
      "50/50 - 185s - loss: 0.0995 - f1_score_mod: 0.3839 - recall_mod: 0.2589 - precision_mod: 0.7448 - dur_error: 0.3689 - maestro_dur_loss: 0.0184 - val_loss: 0.0965 - val_f1_score_mod: 0.3578 - val_recall_mod: 0.2375 - val_precision_mod: 0.7329 - val_dur_error: 0.2657 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09624 to 0.09614, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 187s - loss: 0.0988 - f1_score_mod: 0.3867 - recall_mod: 0.2612 - precision_mod: 0.7491 - dur_error: 0.3678 - maestro_dur_loss: 0.0184 - val_loss: 0.0961 - val_f1_score_mod: 0.3576 - val_recall_mod: 0.2378 - val_precision_mod: 0.7284 - val_dur_error: 0.2603 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09614\n",
      "50/50 - 185s - loss: 0.0985 - f1_score_mod: 0.3891 - recall_mod: 0.2638 - precision_mod: 0.7448 - dur_error: 0.3653 - maestro_dur_loss: 0.0183 - val_loss: 0.1027 - val_f1_score_mod: 0.3669 - val_recall_mod: 0.2482 - val_precision_mod: 0.7099 - val_dur_error: 0.3899 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09614 to 0.09563, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.0977 - f1_score_mod: 0.3962 - recall_mod: 0.2700 - precision_mod: 0.7474 - dur_error: 0.3632 - maestro_dur_loss: 0.0182 - val_loss: 0.0956 - val_f1_score_mod: 0.3723 - val_recall_mod: 0.2544 - val_precision_mod: 0.6969 - val_dur_error: 0.2519 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09563 to 0.09536, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.0975 - f1_score_mod: 0.3991 - recall_mod: 0.2723 - precision_mod: 0.7500 - dur_error: 0.3659 - maestro_dur_loss: 0.0183 - val_loss: 0.0954 - val_f1_score_mod: 0.3725 - val_recall_mod: 0.2521 - val_precision_mod: 0.7182 - val_dur_error: 0.2502 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09536\n",
      "50/50 - 186s - loss: 0.0967 - f1_score_mod: 0.4041 - recall_mod: 0.2770 - precision_mod: 0.7496 - dur_error: 0.3593 - maestro_dur_loss: 0.0180 - val_loss: 0.0958 - val_f1_score_mod: 0.3850 - val_recall_mod: 0.2673 - val_precision_mod: 0.6945 - val_dur_error: 0.2678 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.09536\n",
      "50/50 - 193s - loss: 0.0962 - f1_score_mod: 0.4071 - recall_mod: 0.2800 - precision_mod: 0.7496 - dur_error: 0.3617 - maestro_dur_loss: 0.0181 - val_loss: 0.0978 - val_f1_score_mod: 0.3827 - val_recall_mod: 0.2627 - val_precision_mod: 0.7091 - val_dur_error: 0.3131 - val_maestro_dur_loss: 0.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.09536\n",
      "50/50 - 185s - loss: 0.0960 - f1_score_mod: 0.4132 - recall_mod: 0.2858 - precision_mod: 0.7484 - dur_error: 0.3629 - maestro_dur_loss: 0.0181 - val_loss: 0.0970 - val_f1_score_mod: 0.3917 - val_recall_mod: 0.2755 - val_precision_mod: 0.6804 - val_dur_error: 0.2933 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.09536\n",
      "50/50 - 184s - loss: 0.0952 - f1_score_mod: 0.4161 - recall_mod: 0.2882 - precision_mod: 0.7525 - dur_error: 0.3582 - maestro_dur_loss: 0.0179 - val_loss: 0.0963 - val_f1_score_mod: 0.3922 - val_recall_mod: 0.2748 - val_precision_mod: 0.6877 - val_dur_error: 0.2652 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09536\n",
      "50/50 - 186s - loss: 0.0946 - f1_score_mod: 0.4229 - recall_mod: 0.2940 - precision_mod: 0.7547 - dur_error: 0.3576 - maestro_dur_loss: 0.0179 - val_loss: 0.0977 - val_f1_score_mod: 0.3882 - val_recall_mod: 0.2682 - val_precision_mod: 0.7067 - val_dur_error: 0.3230 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09536\n",
      "50/50 - 185s - loss: 0.0942 - f1_score_mod: 0.4256 - recall_mod: 0.2972 - precision_mod: 0.7513 - dur_error: 0.3574 - maestro_dur_loss: 0.0179 - val_loss: 0.0986 - val_f1_score_mod: 0.3957 - val_recall_mod: 0.2788 - val_precision_mod: 0.6851 - val_dur_error: 0.3361 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.09536 to 0.09421, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 186s - loss: 0.0937 - f1_score_mod: 0.4298 - recall_mod: 0.3011 - precision_mod: 0.7529 - dur_error: 0.3584 - maestro_dur_loss: 0.0179 - val_loss: 0.0942 - val_f1_score_mod: 0.3952 - val_recall_mod: 0.2767 - val_precision_mod: 0.6966 - val_dur_error: 0.2541 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09421\n",
      "50/50 - 186s - loss: 0.0929 - f1_score_mod: 0.4350 - recall_mod: 0.3063 - precision_mod: 0.7516 - dur_error: 0.3542 - maestro_dur_loss: 0.0177 - val_loss: 0.0962 - val_f1_score_mod: 0.3970 - val_recall_mod: 0.2795 - val_precision_mod: 0.6878 - val_dur_error: 0.2870 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09421\n",
      "50/50 - 185s - loss: 0.0926 - f1_score_mod: 0.4349 - recall_mod: 0.3068 - precision_mod: 0.7495 - dur_error: 0.3538 - maestro_dur_loss: 0.0177 - val_loss: 0.0970 - val_f1_score_mod: 0.4052 - val_recall_mod: 0.2902 - val_precision_mod: 0.6728 - val_dur_error: 0.3014 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09421 to 0.09382, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 187s - loss: 0.0918 - f1_score_mod: 0.4445 - recall_mod: 0.3157 - precision_mod: 0.7526 - dur_error: 0.3509 - maestro_dur_loss: 0.0175 - val_loss: 0.0938 - val_f1_score_mod: 0.4020 - val_recall_mod: 0.2816 - val_precision_mod: 0.7059 - val_dur_error: 0.2575 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09382 to 0.09364, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.0912 - f1_score_mod: 0.4467 - recall_mod: 0.3178 - precision_mod: 0.7536 - dur_error: 0.3461 - maestro_dur_loss: 0.0173 - val_loss: 0.0936 - val_f1_score_mod: 0.4030 - val_recall_mod: 0.2829 - val_precision_mod: 0.7038 - val_dur_error: 0.2543 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09364\n",
      "50/50 - 187s - loss: 0.0909 - f1_score_mod: 0.4506 - recall_mod: 0.3218 - precision_mod: 0.7535 - dur_error: 0.3509 - maestro_dur_loss: 0.0175 - val_loss: 0.0949 - val_f1_score_mod: 0.4135 - val_recall_mod: 0.2974 - val_precision_mod: 0.6811 - val_dur_error: 0.2787 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.09364\n",
      "50/50 - 186s - loss: 0.0903 - f1_score_mod: 0.4559 - recall_mod: 0.3275 - precision_mod: 0.7518 - dur_error: 0.3495 - maestro_dur_loss: 0.0175 - val_loss: 0.0971 - val_f1_score_mod: 0.4019 - val_recall_mod: 0.2818 - val_precision_mod: 0.7041 - val_dur_error: 0.3281 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.09364\n",
      "50/50 - 185s - loss: 0.0895 - f1_score_mod: 0.4605 - recall_mod: 0.3315 - precision_mod: 0.7560 - dur_error: 0.3467 - maestro_dur_loss: 0.0173 - val_loss: 0.0953 - val_f1_score_mod: 0.4073 - val_recall_mod: 0.2907 - val_precision_mod: 0.6840 - val_dur_error: 0.2895 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.09364 to 0.09347, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 187s - loss: 0.0891 - f1_score_mod: 0.4655 - recall_mod: 0.3376 - precision_mod: 0.7505 - dur_error: 0.3456 - maestro_dur_loss: 0.0173 - val_loss: 0.0935 - val_f1_score_mod: 0.4121 - val_recall_mod: 0.2936 - val_precision_mod: 0.6934 - val_dur_error: 0.2499 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.09347\n",
      "50/50 - 185s - loss: 0.0886 - f1_score_mod: 0.4685 - recall_mod: 0.3403 - precision_mod: 0.7532 - dur_error: 0.3468 - maestro_dur_loss: 0.0173 - val_loss: 0.0942 - val_f1_score_mod: 0.4162 - val_recall_mod: 0.3016 - val_precision_mod: 0.6728 - val_dur_error: 0.2507 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09347\n",
      "50/50 - 186s - loss: 0.0880 - f1_score_mod: 0.4725 - recall_mod: 0.3442 - precision_mod: 0.7551 - dur_error: 0.3458 - maestro_dur_loss: 0.0173 - val_loss: 0.0948 - val_f1_score_mod: 0.4174 - val_recall_mod: 0.2980 - val_precision_mod: 0.6991 - val_dur_error: 0.2892 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09347 to 0.09306, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 186s - loss: 0.0872 - f1_score_mod: 0.4800 - recall_mod: 0.3513 - precision_mod: 0.7588 - dur_error: 0.3438 - maestro_dur_loss: 0.0172 - val_loss: 0.0931 - val_f1_score_mod: 0.4228 - val_recall_mod: 0.3073 - val_precision_mod: 0.6796 - val_dur_error: 0.2505 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.09306\n",
      "50/50 - 188s - loss: 0.0868 - f1_score_mod: 0.4815 - recall_mod: 0.3540 - precision_mod: 0.7549 - dur_error: 0.3433 - maestro_dur_loss: 0.0172 - val_loss: 0.0937 - val_f1_score_mod: 0.4237 - val_recall_mod: 0.3071 - val_precision_mod: 0.6855 - val_dur_error: 0.2502 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09306\n",
      "50/50 - 186s - loss: 0.0863 - f1_score_mod: 0.4855 - recall_mod: 0.3586 - precision_mod: 0.7529 - dur_error: 0.3427 - maestro_dur_loss: 0.0171 - val_loss: 0.0948 - val_f1_score_mod: 0.4308 - val_recall_mod: 0.3179 - val_precision_mod: 0.6692 - val_dur_error: 0.2834 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.09306\n",
      "50/50 - 185s - loss: 0.0857 - f1_score_mod: 0.4922 - recall_mod: 0.3651 - precision_mod: 0.7567 - dur_error: 0.3425 - maestro_dur_loss: 0.0171 - val_loss: 0.0934 - val_f1_score_mod: 0.4338 - val_recall_mod: 0.3193 - val_precision_mod: 0.6785 - val_dur_error: 0.2514 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09306 to 0.09278, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 185s - loss: 0.0851 - f1_score_mod: 0.4971 - recall_mod: 0.3702 - precision_mod: 0.7584 - dur_error: 0.3424 - maestro_dur_loss: 0.0171 - val_loss: 0.0928 - val_f1_score_mod: 0.4320 - val_recall_mod: 0.3167 - val_precision_mod: 0.6814 - val_dur_error: 0.2493 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.09278\n",
      "50/50 - 185s - loss: 0.0843 - f1_score_mod: 0.5020 - recall_mod: 0.3754 - precision_mod: 0.7590 - dur_error: 0.3376 - maestro_dur_loss: 0.0169 - val_loss: 0.0937 - val_f1_score_mod: 0.4435 - val_recall_mod: 0.3335 - val_precision_mod: 0.6640 - val_dur_error: 0.2621 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.09278\n",
      "50/50 - 189s - loss: 0.0837 - f1_score_mod: 0.5074 - recall_mod: 0.3822 - precision_mod: 0.7562 - dur_error: 0.3379 - maestro_dur_loss: 0.0169 - val_loss: 0.0938 - val_f1_score_mod: 0.4377 - val_recall_mod: 0.3256 - val_precision_mod: 0.6700 - val_dur_error: 0.2587 - val_maestro_dur_loss: 0.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/150\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.09278\n",
      "50/50 - 185s - loss: 0.0833 - f1_score_mod: 0.5118 - recall_mod: 0.3870 - precision_mod: 0.7568 - dur_error: 0.3380 - maestro_dur_loss: 0.0169 - val_loss: 0.0930 - val_f1_score_mod: 0.4405 - val_recall_mod: 0.3290 - val_precision_mod: 0.6685 - val_dur_error: 0.2424 - val_maestro_dur_loss: 0.0121\n",
      "Epoch 87/150\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.09278\n",
      "50/50 - 185s - loss: 0.0825 - f1_score_mod: 0.5184 - recall_mod: 0.3936 - precision_mod: 0.7601 - dur_error: 0.3348 - maestro_dur_loss: 0.0167 - val_loss: 0.0949 - val_f1_score_mod: 0.4430 - val_recall_mod: 0.3320 - val_precision_mod: 0.6676 - val_dur_error: 0.2880 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 88/150\n",
      "Batch 7: Invalid loss, terminating training\n",
      "Batch 8: Invalid loss, terminating training\n",
      "Batch 9: Invalid loss, terminating training\n",
      "Batch 10: Invalid loss, terminating training\n",
      "Batch 11: Invalid loss, terminating training\n",
      "Batch 12: Invalid loss, terminating training\n",
      "Batch 13: Invalid loss, terminating training\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.09278\n",
      "50/50 - 186s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed marginally better, and was also still improving just before the NaN loss. We can also try [gradient clipping](https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/). There are two methods to accomplish this and they are controlled by the optional parameters to RMSProp (clipnorm and clipvalue). clipnorm limits the L2 norm of the gradients after each batch by rescaling them to be lower than the value set by clipnorm. clipvalue enforces a maximum/minimum value (equal to +/- clipvalue) for each gradient. In the linked blog post, reasonable values are given to be 1.0 for clipnorm and 0.5 for clipvalue. \n",
    "\n",
    "We could simply try these but it is better to get a look at the gradients for our base model to inform our decision. For this, we will need to run the base model again (using train_by_batch so I can call the following functions in between batches). Note that training by batch will slightly alter the results when compared to train (insert link). To inform the clipnorm choice, we simply need to calculate the L2-norm of the gradients after each batch. To inform the clipvalue choice, we will need to look at the minimum/maximum gradients (over the batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norm_func(model):\n",
    "    \"\"\"Returns the gradient norm of the model averaged over all gradients of all layers\"\"\"\n",
    "\n",
    "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "    summed_squares = [K.sum(K.square(g)) for g in grads]\n",
    "    norm = K.sqrt(sum(summed_squares))\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    return K.function(inputs, [norm])\n",
    "\n",
    "def get_gradient_stats_func(model):\n",
    "    \"\"\"Returns the gradient statistics of the model\"\"\"\n",
    "\n",
    "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "    means = [K.mean(g) for g in grads]\n",
    "    minimums = [K.min(g) for g in grads]\n",
    "    maximums = [K.max(g) for g in grads]\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    return K.function(inputs, [means, minimums, maximums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 2:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 3:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 4:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 5:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 6:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 7:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 8:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 9:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 10:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 11:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 12:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 13:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 14:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 15:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 16:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 17:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 18:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 19:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 20:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 21:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 22:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 23:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 24:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 25:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 26:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 27:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 28:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 29:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 30:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 31:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 32:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 33:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 34:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 35:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 36:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 37:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 38:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 39:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 40:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 41:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 42:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 43:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 44:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 45:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 46:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 47:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 48:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 49:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 50:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()   # For some reason, I need to do this to have\n",
    "                                         # access to total_loss. It's strange because \n",
    "                                         # model.run_eagerly = False even before, as it shoul5\n",
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4)\n",
    "harshness = 0.05\n",
    "model.compile(loss = maestro_loss_wr(0.05), optimizer = RMSprop(), metrics = [f1_score_mod, dur_error, maestro_dur_loss_wr(0.05)])\n",
    "\n",
    "get_gradient_norms = get_gradient_norm_func(model)\n",
    "get_gradient_stats = get_gradient_stats_func(model)\n",
    "norms_by_batch = []\n",
    "stats_by_batch = []\n",
    "batch_size = 512\n",
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    print('Epoch {}:'.format(i + 1))\n",
    "    for j in range(len(y_train) // batch_size):\n",
    "        model.train_on_batch(X_train[j * batch_size: (j + 1) * batch_size], \\\n",
    "                             y_train[j * batch_size: (j + 1) * batch_size])\n",
    "        \n",
    "        # We just need to pass placeholder arrays of shape (?, window_size, 88),\n",
    "        # (?, n_keys_piano + 1) and (n_keys_piano + 1)\n",
    "        gradient_norms = get_gradient_norms([X_train[:2], y_train[:2], np.ones(2)])\n",
    "        gradient_stats = get_gradient_stats([X_train[:2], y_train[:2], np.ones(2)])\n",
    "        mean = np.mean(gradient_stats[0])\n",
    "        minimum = min(gradient_stats[1])\n",
    "        maximum = max(gradient_stats[2])\n",
    "        print('batch {}'.format(j))\n",
    "        norms_by_batch.append(gradient_norms[0])\n",
    "        stats_by_batch.append([mean, minimum, maximum])\n",
    "        \n",
    "    model.train_on_batch(X_train[(j + 1) * batch_size:], y_train[(j + 1) * batch_size:])\n",
    "    \n",
    "    gradient_norms = get_gradient_norms([X_train[:2], y_train[:2], np.ones(2)])\n",
    "    gradient_stats = get_gradient_stats([X_train[:2], y_train[:2], np.ones(2)])\n",
    "    mean = np.mean(gradient_stats[0])\n",
    "    minimum = min(gradient_stats[1])\n",
    "    maximum = max(gradient_stats[2])\n",
    "    norms_by_batch.append(gradient_norms[0])\n",
    "    stats_by_batch.append([mean, minimum, maximum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>norm</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th>Batch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.278836</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>-0.018197</td>\n",
       "      <td>0.014304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.277867</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.023972</td>\n",
       "      <td>0.220069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.705598</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.118480</td>\n",
       "      <td>0.032690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.251942</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.025353</td>\n",
       "      <td>0.013178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.441158</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.031638</td>\n",
       "      <td>0.069337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">48</th>\n",
       "      <th>2322</th>\n",
       "      <td>0.605186</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.025268</td>\n",
       "      <td>0.083536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>0.537184</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-0.032382</td>\n",
       "      <td>0.050243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>0.538393</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.019429</td>\n",
       "      <td>0.079083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>0.568852</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.027382</td>\n",
       "      <td>0.090049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>0.597260</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.033230</td>\n",
       "      <td>0.064026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2326 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 norm      mean       min       max\n",
       "Epoch Batch                                        \n",
       "1     1      0.278836  0.000310 -0.018197  0.014304\n",
       "      2      1.277867  0.000209 -0.023972  0.220069\n",
       "      3      0.705598  0.000048 -0.118480  0.032690\n",
       "      4      0.251942  0.000034 -0.025353  0.013178\n",
       "      5      0.441158  0.000007 -0.031638  0.069337\n",
       "...               ...       ...       ...       ...\n",
       "48    2322   0.605186  0.000013 -0.025268  0.083536\n",
       "      2323   0.537184  0.000052 -0.032382  0.050243\n",
       "      2324   0.538393  0.000004 -0.019429  0.079083\n",
       "      2325   0.568852  0.000053 -0.027382  0.090049\n",
       "      2326   0.597260  0.000007 -0.033230  0.064026\n",
       "\n",
       "[2326 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the dataframes\n",
    "df_norms = pd.DataFrame(norms_by_batch)\n",
    "df_stats = pd.DataFrame(stats_by_batch)\n",
    "df_grads = pd.concat([df_norms, df_stats], axis = 1).dropna()\n",
    "df_grads.columns = ['norm', 'mean', 'min', 'max']\n",
    "batches_per_epoch = 49\n",
    "df_grads.index = pd.MultiIndex.from_tuples([((batch // batches_per_epoch) + 1, batch) \\\n",
    "                    for batch in range(1, len(df_grads) + 1)], names = ('Epoch', 'Batch'))\n",
    "df_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data so we don't have to re-run the model\n",
    "df_grads.to_csv('../model_data/gradients_2_1_512_0pt4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x65dfe06d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_grads = pd.read_csv('../model_data/gradients_2_1_512_0pt4.csv', index_col = [0, 1])\n",
    "df_grads.plot(y = 'norm', figsize = (12, 6), logy = True, xlabel = 'Epoch, Batch', \\\n",
    "             ylabel = 'L2-Norm', fontsize = 12)\n",
    "plt.axhline(1, color = 'y')\n",
    "plt.axhline(0.5, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot (note the logscale on the y-axis), we have added a line corresponding to the suggested clipnorm value of 1.0. We can see that on only two occasions in early training does the L2-norm exceed this value. It is possible that the norm for the final batch would have exploded. To get a better idea, one could re-run with a smaller batch size. However, there is no way to know how much we would have to decrease the batch size to see such an effect. Since lowering the batch size would also increase the run-time, we will halt the exploration here. Our plan is now to train models with clipnorm = 1.0 (line in <font color='yellow'>yellow</font>) and 0.5 (line in <font color='red'>red</font>). The latter should have an effect on the late-stage training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x65e600690>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_grads.plot(y = ['mean', 'min', 'max'], xlabel = 'Epoch, Batch', \\\n",
    "             ylabel = 'Gradient', figsize = (12, 6), fontsize = 12, ylim = [-.3, .3])\n",
    "plt.axhline(0.2, color = 'y')\n",
    "plt.axhline(-0.2, color = 'y')\n",
    "plt.axhline(0.1, color = 'r')\n",
    "plt.axhline(-0.1, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can see that the mean says near zero while the min is largest in early training, and the max is largest in late training (though both get pretty large). However, neither really approach the level of 0.5 (a clipvalue of 0.5 was used in the [blog post](https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/)). Perhaps such a value would delay the NaN failure in the final batch. However, from the plot, I think a clipvalue of 0.2 (line in <font color='yellow'>yellow</font>) is a better start. And then, we can reduce it to 0.1 (line in <font color='red'>red</font>) which should certainly affect the late training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25256 samples, validate on 10824 samples\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16327, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 161s - loss: 0.2239 - f1_score_mod: 0.0138 - recall_mod: 0.0227 - precision_mod: 0.0997 - dur_error: 1.0557 - maestro_dur_loss: 0.0528 - val_loss: 0.1633 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.8993 - val_maestro_dur_loss: 0.0450\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16327 to 0.15037, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 145s - loss: 0.1623 - f1_score_mod: 0.0022 - recall_mod: 0.0011 - precision_mod: 0.2901 - dur_error: 0.7105 - maestro_dur_loss: 0.0355 - val_loss: 0.1504 - val_f1_score_mod: 0.0265 - val_recall_mod: 0.0137 - val_precision_mod: 0.5551 - val_dur_error: 0.6172 - val_maestro_dur_loss: 0.0309\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15037 to 0.13731, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 160s - loss: 0.1534 - f1_score_mod: 0.0228 - recall_mod: 0.0118 - precision_mod: 0.5566 - dur_error: 0.6484 - maestro_dur_loss: 0.0324 - val_loss: 0.1373 - val_f1_score_mod: 0.0241 - val_recall_mod: 0.0123 - val_precision_mod: 0.7476 - val_dur_error: 0.5348 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13731 to 0.12738, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 148s - loss: 0.1464 - f1_score_mod: 0.0490 - recall_mod: 0.0261 - precision_mod: 0.5957 - dur_error: 0.6108 - maestro_dur_loss: 0.0305 - val_loss: 0.1274 - val_f1_score_mod: 0.0432 - val_recall_mod: 0.0223 - val_precision_mod: 0.7281 - val_dur_error: 0.4208 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12738\n",
      "25256/25256 - 148s - loss: 0.1420 - f1_score_mod: 0.0709 - recall_mod: 0.0379 - precision_mod: 0.6117 - dur_error: 0.5929 - maestro_dur_loss: 0.0296 - val_loss: 0.1365 - val_f1_score_mod: 0.1147 - val_recall_mod: 0.0640 - val_precision_mod: 0.5634 - val_dur_error: 0.5819 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12738 to 0.12309, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 149s - loss: 0.1382 - f1_score_mod: 0.0907 - recall_mod: 0.0494 - precision_mod: 0.6163 - dur_error: 0.5741 - maestro_dur_loss: 0.0287 - val_loss: 0.1231 - val_f1_score_mod: 0.1109 - val_recall_mod: 0.0606 - val_precision_mod: 0.6698 - val_dur_error: 0.4195 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12309 to 0.12043, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 140s - loss: 0.1358 - f1_score_mod: 0.1117 - recall_mod: 0.0616 - precision_mod: 0.6343 - dur_error: 0.5706 - maestro_dur_loss: 0.0285 - val_loss: 0.1204 - val_f1_score_mod: 0.1111 - val_recall_mod: 0.0603 - val_precision_mod: 0.7296 - val_dur_error: 0.3945 - val_maestro_dur_loss: 0.0197\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12043 to 0.11951, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 147s - loss: 0.1333 - f1_score_mod: 0.1299 - recall_mod: 0.0726 - precision_mod: 0.6431 - dur_error: 0.5551 - maestro_dur_loss: 0.0278 - val_loss: 0.1195 - val_f1_score_mod: 0.1187 - val_recall_mod: 0.0651 - val_precision_mod: 0.7057 - val_dur_error: 0.4040 - val_maestro_dur_loss: 0.0202\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11951\n",
      "25256/25256 - 149s - loss: 0.1313 - f1_score_mod: 0.1428 - recall_mod: 0.0803 - precision_mod: 0.6606 - dur_error: 0.5451 - maestro_dur_loss: 0.0273 - val_loss: 0.1234 - val_f1_score_mod: 0.1191 - val_recall_mod: 0.0649 - val_precision_mod: 0.7491 - val_dur_error: 0.4663 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11951 to 0.11827, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 142s - loss: 0.1296 - f1_score_mod: 0.1585 - recall_mod: 0.0901 - precision_mod: 0.6706 - dur_error: 0.5385 - maestro_dur_loss: 0.0269 - val_loss: 0.1183 - val_f1_score_mod: 0.1572 - val_recall_mod: 0.0883 - val_precision_mod: 0.7348 - val_dur_error: 0.4213 - val_maestro_dur_loss: 0.0211\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11827 to 0.11387, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 132s - loss: 0.1280 - f1_score_mod: 0.1700 - recall_mod: 0.0977 - precision_mod: 0.6741 - dur_error: 0.5248 - maestro_dur_loss: 0.0262 - val_loss: 0.1139 - val_f1_score_mod: 0.1523 - val_recall_mod: 0.0846 - val_precision_mod: 0.7741 - val_dur_error: 0.3573 - val_maestro_dur_loss: 0.0179\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11387 to 0.11348, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1262 - f1_score_mod: 0.1800 - recall_mod: 0.1040 - precision_mod: 0.6852 - dur_error: 0.5106 - maestro_dur_loss: 0.0255 - val_loss: 0.1135 - val_f1_score_mod: 0.1748 - val_recall_mod: 0.0991 - val_precision_mod: 0.7518 - val_dur_error: 0.3656 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11348 to 0.11186, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 132s - loss: 0.1244 - f1_score_mod: 0.1909 - recall_mod: 0.1113 - precision_mod: 0.6887 - dur_error: 0.4978 - maestro_dur_loss: 0.0249 - val_loss: 0.1119 - val_f1_score_mod: 0.1791 - val_recall_mod: 0.1025 - val_precision_mod: 0.7315 - val_dur_error: 0.3469 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11186 to 0.11072, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1232 - f1_score_mod: 0.2001 - recall_mod: 0.1176 - precision_mod: 0.6861 - dur_error: 0.4878 - maestro_dur_loss: 0.0244 - val_loss: 0.1107 - val_f1_score_mod: 0.2087 - val_recall_mod: 0.1222 - val_precision_mod: 0.7214 - val_dur_error: 0.3355 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11072\n",
      "25256/25256 - 132s - loss: 0.1216 - f1_score_mod: 0.2125 - recall_mod: 0.1257 - precision_mod: 0.6954 - dur_error: 0.4765 - maestro_dur_loss: 0.0238 - val_loss: 0.1128 - val_f1_score_mod: 0.1973 - val_recall_mod: 0.1138 - val_precision_mod: 0.7544 - val_dur_error: 0.3896 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11072\n",
      "25256/25256 - 132s - loss: 0.1206 - f1_score_mod: 0.2213 - recall_mod: 0.1318 - precision_mod: 0.6971 - dur_error: 0.4741 - maestro_dur_loss: 0.0237 - val_loss: 0.1114 - val_f1_score_mod: 0.2052 - val_recall_mod: 0.1197 - val_precision_mod: 0.7412 - val_dur_error: 0.3642 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11072 to 0.10784, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 132s - loss: 0.1195 - f1_score_mod: 0.2299 - recall_mod: 0.1378 - precision_mod: 0.7036 - dur_error: 0.4644 - maestro_dur_loss: 0.0232 - val_loss: 0.1078 - val_f1_score_mod: 0.2418 - val_recall_mod: 0.1459 - val_precision_mod: 0.7146 - val_dur_error: 0.3109 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10784 to 0.10695, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 137s - loss: 0.1184 - f1_score_mod: 0.2398 - recall_mod: 0.1447 - precision_mod: 0.7088 - dur_error: 0.4627 - maestro_dur_loss: 0.0231 - val_loss: 0.1070 - val_f1_score_mod: 0.2259 - val_recall_mod: 0.1333 - val_precision_mod: 0.7457 - val_dur_error: 0.3025 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10695\n",
      "25256/25256 - 153s - loss: 0.1172 - f1_score_mod: 0.2464 - recall_mod: 0.1495 - precision_mod: 0.7076 - dur_error: 0.4522 - maestro_dur_loss: 0.0226 - val_loss: 0.1086 - val_f1_score_mod: 0.2259 - val_recall_mod: 0.1327 - val_precision_mod: 0.7643 - val_dur_error: 0.3329 - val_maestro_dur_loss: 0.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10695 to 0.10474, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1163 - f1_score_mod: 0.2542 - recall_mod: 0.1551 - precision_mod: 0.7162 - dur_error: 0.4486 - maestro_dur_loss: 0.0224 - val_loss: 0.1047 - val_f1_score_mod: 0.2565 - val_recall_mod: 0.1559 - val_precision_mod: 0.7255 - val_dur_error: 0.2941 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10474\n",
      "25256/25256 - 132s - loss: 0.1151 - f1_score_mod: 0.2628 - recall_mod: 0.1612 - precision_mod: 0.7159 - dur_error: 0.4424 - maestro_dur_loss: 0.0221 - val_loss: 0.1064 - val_f1_score_mod: 0.2538 - val_recall_mod: 0.1534 - val_precision_mod: 0.7450 - val_dur_error: 0.3287 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10474\n",
      "25256/25256 - 133s - loss: 0.1142 - f1_score_mod: 0.2712 - recall_mod: 0.1676 - precision_mod: 0.7178 - dur_error: 0.4376 - maestro_dur_loss: 0.0219 - val_loss: 0.1047 - val_f1_score_mod: 0.2665 - val_recall_mod: 0.1639 - val_precision_mod: 0.7210 - val_dur_error: 0.3022 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10474\n",
      "25256/25256 - 133s - loss: 0.1134 - f1_score_mod: 0.2764 - recall_mod: 0.1716 - precision_mod: 0.7182 - dur_error: 0.4340 - maestro_dur_loss: 0.0217 - val_loss: 0.1063 - val_f1_score_mod: 0.2983 - val_recall_mod: 0.1925 - val_precision_mod: 0.6665 - val_dur_error: 0.3268 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10474\n",
      "25256/25256 - 134s - loss: 0.1126 - f1_score_mod: 0.2881 - recall_mod: 0.1805 - precision_mod: 0.7201 - dur_error: 0.4331 - maestro_dur_loss: 0.0217 - val_loss: 0.1076 - val_f1_score_mod: 0.2787 - val_recall_mod: 0.1745 - val_precision_mod: 0.7004 - val_dur_error: 0.3760 - val_maestro_dur_loss: 0.0188\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10474\n",
      "25256/25256 - 137s - loss: 0.1114 - f1_score_mod: 0.2931 - recall_mod: 0.1843 - precision_mod: 0.7213 - dur_error: 0.4291 - maestro_dur_loss: 0.0215 - val_loss: 0.1146 - val_f1_score_mod: 0.2971 - val_recall_mod: 0.1895 - val_precision_mod: 0.6935 - val_dur_error: 0.5173 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10474\n",
      "25256/25256 - 134s - loss: 0.1106 - f1_score_mod: 0.2973 - recall_mod: 0.1880 - precision_mod: 0.7181 - dur_error: 0.4227 - maestro_dur_loss: 0.0211 - val_loss: 0.1107 - val_f1_score_mod: 0.3082 - val_recall_mod: 0.1986 - val_precision_mod: 0.6935 - val_dur_error: 0.4463 - val_maestro_dur_loss: 0.0223\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10474 to 0.10103, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 132s - loss: 0.1098 - f1_score_mod: 0.3061 - recall_mod: 0.1943 - precision_mod: 0.7262 - dur_error: 0.4212 - maestro_dur_loss: 0.0211 - val_loss: 0.1010 - val_f1_score_mod: 0.2859 - val_recall_mod: 0.1775 - val_precision_mod: 0.7413 - val_dur_error: 0.2849 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10103 to 0.10085, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 171s - loss: 0.1089 - f1_score_mod: 0.3110 - recall_mod: 0.1984 - precision_mod: 0.7251 - dur_error: 0.4152 - maestro_dur_loss: 0.0208 - val_loss: 0.1009 - val_f1_score_mod: 0.2928 - val_recall_mod: 0.1811 - val_precision_mod: 0.7710 - val_dur_error: 0.2853 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10085\n",
      "25256/25256 - 155s - loss: 0.1080 - f1_score_mod: 0.3185 - recall_mod: 0.2040 - precision_mod: 0.7323 - dur_error: 0.4132 - maestro_dur_loss: 0.0207 - val_loss: 0.1048 - val_f1_score_mod: 0.3142 - val_recall_mod: 0.2001 - val_precision_mod: 0.7348 - val_dur_error: 0.3727 - val_maestro_dur_loss: 0.0186\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10085 to 0.10042, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 150s - loss: 0.1073 - f1_score_mod: 0.3250 - recall_mod: 0.2096 - precision_mod: 0.7301 - dur_error: 0.4113 - maestro_dur_loss: 0.0206 - val_loss: 0.1004 - val_f1_score_mod: 0.3156 - val_recall_mod: 0.2022 - val_precision_mod: 0.7253 - val_dur_error: 0.2971 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10042\n",
      "25256/25256 - 143s - loss: 0.1063 - f1_score_mod: 0.3346 - recall_mod: 0.2173 - precision_mod: 0.7321 - dur_error: 0.4077 - maestro_dur_loss: 0.0204 - val_loss: 0.1021 - val_f1_score_mod: 0.3189 - val_recall_mod: 0.2039 - val_precision_mod: 0.7412 - val_dur_error: 0.3428 - val_maestro_dur_loss: 0.0171\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10042 to 0.09970, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 150s - loss: 0.1054 - f1_score_mod: 0.3369 - recall_mod: 0.2194 - precision_mod: 0.7310 - dur_error: 0.4041 - maestro_dur_loss: 0.0202 - val_loss: 0.0997 - val_f1_score_mod: 0.3316 - val_recall_mod: 0.2149 - val_precision_mod: 0.7283 - val_dur_error: 0.3001 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09970 to 0.09882, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 143s - loss: 0.1045 - f1_score_mod: 0.3456 - recall_mod: 0.2264 - precision_mod: 0.7350 - dur_error: 0.4006 - maestro_dur_loss: 0.0200 - val_loss: 0.0988 - val_f1_score_mod: 0.3223 - val_recall_mod: 0.2055 - val_precision_mod: 0.7518 - val_dur_error: 0.2916 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09882\n",
      "25256/25256 - 142s - loss: 0.1039 - f1_score_mod: 0.3498 - recall_mod: 0.2299 - precision_mod: 0.7382 - dur_error: 0.4030 - maestro_dur_loss: 0.0201 - val_loss: 0.1002 - val_f1_score_mod: 0.3441 - val_recall_mod: 0.2271 - val_precision_mod: 0.7134 - val_dur_error: 0.3219 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09882 to 0.09844, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1029 - f1_score_mod: 0.3601 - recall_mod: 0.2388 - precision_mod: 0.7367 - dur_error: 0.3961 - maestro_dur_loss: 0.0198 - val_loss: 0.0984 - val_f1_score_mod: 0.3375 - val_recall_mod: 0.2188 - val_precision_mod: 0.7415 - val_dur_error: 0.2980 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09844\n",
      "25256/25256 - 132s - loss: 0.1020 - f1_score_mod: 0.3636 - recall_mod: 0.2413 - precision_mod: 0.7413 - dur_error: 0.3943 - maestro_dur_loss: 0.0197 - val_loss: 0.1005 - val_f1_score_mod: 0.3473 - val_recall_mod: 0.2270 - val_precision_mod: 0.7426 - val_dur_error: 0.3468 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09844 to 0.09599, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1016 - f1_score_mod: 0.3726 - recall_mod: 0.2493 - precision_mod: 0.7420 - dur_error: 0.3953 - maestro_dur_loss: 0.0198 - val_loss: 0.0960 - val_f1_score_mod: 0.3568 - val_recall_mod: 0.2386 - val_precision_mod: 0.7115 - val_dur_error: 0.2638 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09599\n",
      "25256/25256 - 132s - loss: 0.1003 - f1_score_mod: 0.3759 - recall_mod: 0.2524 - precision_mod: 0.7383 - dur_error: 0.3880 - maestro_dur_loss: 0.0194 - val_loss: 0.0965 - val_f1_score_mod: 0.3543 - val_recall_mod: 0.2319 - val_precision_mod: 0.7553 - val_dur_error: 0.2742 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09599\n",
      "25256/25256 - 134s - loss: 0.0997 - f1_score_mod: 0.3821 - recall_mod: 0.2580 - precision_mod: 0.7394 - dur_error: 0.3908 - maestro_dur_loss: 0.0195 - val_loss: 0.1032 - val_f1_score_mod: 0.3797 - val_recall_mod: 0.2594 - val_precision_mod: 0.7098 - val_dur_error: 0.4199 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09599 to 0.09570, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0991 - f1_score_mod: 0.3844 - recall_mod: 0.2603 - precision_mod: 0.7390 - dur_error: 0.3863 - maestro_dur_loss: 0.0193 - val_loss: 0.0957 - val_f1_score_mod: 0.3706 - val_recall_mod: 0.2512 - val_precision_mod: 0.7127 - val_dur_error: 0.2688 - val_maestro_dur_loss: 0.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09570 to 0.09532, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0982 - f1_score_mod: 0.3955 - recall_mod: 0.2701 - precision_mod: 0.7418 - dur_error: 0.3862 - maestro_dur_loss: 0.0193 - val_loss: 0.0953 - val_f1_score_mod: 0.3782 - val_recall_mod: 0.2566 - val_precision_mod: 0.7212 - val_dur_error: 0.2789 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09532\n",
      "25256/25256 - 133s - loss: 0.0979 - f1_score_mod: 0.3980 - recall_mod: 0.2723 - precision_mod: 0.7427 - dur_error: 0.3858 - maestro_dur_loss: 0.0193 - val_loss: 0.0984 - val_f1_score_mod: 0.3797 - val_recall_mod: 0.2586 - val_precision_mod: 0.7176 - val_dur_error: 0.3403 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09532\n",
      "25256/25256 - 135s - loss: 0.0966 - f1_score_mod: 0.4041 - recall_mod: 0.2777 - precision_mod: 0.7447 - dur_error: 0.3770 - maestro_dur_loss: 0.0188 - val_loss: 0.0983 - val_f1_score_mod: 0.3728 - val_recall_mod: 0.2489 - val_precision_mod: 0.7456 - val_dur_error: 0.3503 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09532\n",
      "25256/25256 - 193s - loss: 0.0962 - f1_score_mod: 0.4090 - recall_mod: 0.2826 - precision_mod: 0.7423 - dur_error: 0.3821 - maestro_dur_loss: 0.0191 - val_loss: 0.0979 - val_f1_score_mod: 0.3866 - val_recall_mod: 0.2665 - val_precision_mod: 0.7081 - val_dur_error: 0.3347 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09532\n",
      "25256/25256 - 181s - loss: 0.0950 - f1_score_mod: 0.4175 - recall_mod: 0.2901 - precision_mod: 0.7473 - dur_error: 0.3749 - maestro_dur_loss: 0.0187 - val_loss: 0.0985 - val_f1_score_mod: 0.3916 - val_recall_mod: 0.2707 - val_precision_mod: 0.7109 - val_dur_error: 0.3588 - val_maestro_dur_loss: 0.0179\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09532\n",
      "25256/25256 - 134s - loss: 0.0942 - f1_score_mod: 0.4221 - recall_mod: 0.2945 - precision_mod: 0.7467 - dur_error: 0.3756 - maestro_dur_loss: 0.0188 - val_loss: 0.0998 - val_f1_score_mod: 0.3926 - val_recall_mod: 0.2727 - val_precision_mod: 0.7079 - val_dur_error: 0.3912 - val_maestro_dur_loss: 0.0196\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09532 to 0.09277, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25256/25256 - 132s - loss: 0.0935 - f1_score_mod: 0.4277 - recall_mod: 0.3007 - precision_mod: 0.7416 - dur_error: 0.3728 - maestro_dur_loss: 0.0186 - val_loss: 0.0928 - val_f1_score_mod: 0.3939 - val_recall_mod: 0.2724 - val_precision_mod: 0.7151 - val_dur_error: 0.2565 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09277\n",
      "25256/25256 - 132s - loss: 0.0925 - f1_score_mod: 0.4360 - recall_mod: 0.3087 - precision_mod: 0.7445 - dur_error: 0.3689 - maestro_dur_loss: 0.0184 - val_loss: 0.1010 - val_f1_score_mod: 0.3928 - val_recall_mod: 0.2715 - val_precision_mod: 0.7119 - val_dur_error: 0.4151 - val_maestro_dur_loss: 0.0208\n",
      "Epoch 49/150\n",
      "Batch 18: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9728/25256 - 43s - loss: nan - f1_score_mod: 0.4536 - recall_mod: 0.3239 - precision_mod: 0.7582 - dur_error: 0.3584 - maestro_dur_loss: 0.0179\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipnorm = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed about as well on the validation set as the model with half the default learning rate, in fewer epochs. However, it failed at a higher training loss. Let's now try with clipnorm = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25256 samples, validate on 10824 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15157, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 135s - loss: 0.2211 - f1_score_mod: 0.0137 - recall_mod: 0.0218 - precision_mod: 0.1212 - dur_error: 1.0112 - maestro_dur_loss: 0.0506 - val_loss: 0.1516 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6820 - val_maestro_dur_loss: 0.0341\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15157 to 0.14176, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 134s - loss: 0.1616 - f1_score_mod: 0.0021 - recall_mod: 0.0011 - precision_mod: 0.2882 - dur_error: 0.7003 - maestro_dur_loss: 0.0350 - val_loss: 0.1418 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5354 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14176\n",
      "25256/25256 - 133s - loss: 0.1513 - f1_score_mod: 0.0216 - recall_mod: 0.0111 - precision_mod: 0.5527 - dur_error: 0.6375 - maestro_dur_loss: 0.0319 - val_loss: 0.1418 - val_f1_score_mod: 0.0278 - val_recall_mod: 0.0142 - val_precision_mod: 0.7539 - val_dur_error: 0.6264 - val_maestro_dur_loss: 0.0313\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14176 to 0.12835, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 133s - loss: 0.1448 - f1_score_mod: 0.0530 - recall_mod: 0.0280 - precision_mod: 0.6104 - dur_error: 0.6068 - maestro_dur_loss: 0.0303 - val_loss: 0.1284 - val_f1_score_mod: 0.0547 - val_recall_mod: 0.0286 - val_precision_mod: 0.6633 - val_dur_error: 0.4428 - val_maestro_dur_loss: 0.0221\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12835\n",
      "25256/25256 - 132s - loss: 0.1404 - f1_score_mod: 0.0800 - recall_mod: 0.0431 - precision_mod: 0.6219 - dur_error: 0.5848 - maestro_dur_loss: 0.0292 - val_loss: 0.1309 - val_f1_score_mod: 0.1011 - val_recall_mod: 0.0549 - val_precision_mod: 0.6837 - val_dur_error: 0.5518 - val_maestro_dur_loss: 0.0276\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12835\n",
      "25256/25256 - 132s - loss: 0.1370 - f1_score_mod: 0.1065 - recall_mod: 0.0586 - precision_mod: 0.6348 - dur_error: 0.5754 - maestro_dur_loss: 0.0288 - val_loss: 0.1299 - val_f1_score_mod: 0.1427 - val_recall_mod: 0.0801 - val_precision_mod: 0.6581 - val_dur_error: 0.5482 - val_maestro_dur_loss: 0.0274\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12835\n",
      "25256/25256 - 133s - loss: 0.1369 - f1_score_mod: 0.1249 - recall_mod: 0.0704 - precision_mod: 0.6064 - dur_error: 0.5946 - maestro_dur_loss: 0.0297 - val_loss: 0.1319 - val_f1_score_mod: 0.1049 - val_recall_mod: 0.0573 - val_precision_mod: 0.6368 - val_dur_error: 0.4937 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12835 to 0.12236, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1357 - f1_score_mod: 0.1240 - recall_mod: 0.0690 - precision_mod: 0.6347 - dur_error: 0.5724 - maestro_dur_loss: 0.0286 - val_loss: 0.1224 - val_f1_score_mod: 0.1493 - val_recall_mod: 0.0840 - val_precision_mod: 0.6761 - val_dur_error: 0.4423 - val_maestro_dur_loss: 0.0221\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12236 to 0.12107, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1315 - f1_score_mod: 0.1386 - recall_mod: 0.0777 - precision_mod: 0.6680 - dur_error: 0.5409 - maestro_dur_loss: 0.0270 - val_loss: 0.1211 - val_f1_score_mod: 0.1490 - val_recall_mod: 0.0839 - val_precision_mod: 0.6895 - val_dur_error: 0.4548 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12107\n",
      "25256/25256 - 131s - loss: 0.1296 - f1_score_mod: 0.1538 - recall_mod: 0.0873 - precision_mod: 0.6692 - dur_error: 0.5276 - maestro_dur_loss: 0.0264 - val_loss: 0.1214 - val_f1_score_mod: 0.1680 - val_recall_mod: 0.0960 - val_precision_mod: 0.6822 - val_dur_error: 0.4699 - val_maestro_dur_loss: 0.0235\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12107 to 0.11965, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1279 - f1_score_mod: 0.1646 - recall_mod: 0.0939 - precision_mod: 0.6818 - dur_error: 0.5152 - maestro_dur_loss: 0.0258 - val_loss: 0.1197 - val_f1_score_mod: 0.1868 - val_recall_mod: 0.1082 - val_precision_mod: 0.6907 - val_dur_error: 0.4536 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11965 to 0.11574, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 133s - loss: 0.1263 - f1_score_mod: 0.1755 - recall_mod: 0.1011 - precision_mod: 0.6799 - dur_error: 0.5052 - maestro_dur_loss: 0.0253 - val_loss: 0.1157 - val_f1_score_mod: 0.1990 - val_recall_mod: 0.1176 - val_precision_mod: 0.6585 - val_dur_error: 0.3925 - val_maestro_dur_loss: 0.0196\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11574 to 0.11417, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1252 - f1_score_mod: 0.1886 - recall_mod: 0.1100 - precision_mod: 0.6805 - dur_error: 0.5034 - maestro_dur_loss: 0.0252 - val_loss: 0.1142 - val_f1_score_mod: 0.1896 - val_recall_mod: 0.1093 - val_precision_mod: 0.7244 - val_dur_error: 0.3610 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11417 to 0.11013, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 133s - loss: 0.1239 - f1_score_mod: 0.1926 - recall_mod: 0.1126 - precision_mod: 0.6792 - dur_error: 0.4873 - maestro_dur_loss: 0.0244 - val_loss: 0.1101 - val_f1_score_mod: 0.1961 - val_recall_mod: 0.1130 - val_precision_mod: 0.7492 - val_dur_error: 0.3243 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11013 to 0.10999, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 134s - loss: 0.1220 - f1_score_mod: 0.2031 - recall_mod: 0.1193 - precision_mod: 0.6928 - dur_error: 0.4741 - maestro_dur_loss: 0.0237 - val_loss: 0.1100 - val_f1_score_mod: 0.2085 - val_recall_mod: 0.1222 - val_precision_mod: 0.7163 - val_dur_error: 0.3223 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.10999 to 0.10852, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 133s - loss: 0.1213 - f1_score_mod: 0.2128 - recall_mod: 0.1260 - precision_mod: 0.6939 - dur_error: 0.4752 - maestro_dur_loss: 0.0238 - val_loss: 0.1085 - val_f1_score_mod: 0.2203 - val_recall_mod: 0.1304 - val_precision_mod: 0.7201 - val_dur_error: 0.3125 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10852\n",
      "25256/25256 - 133s - loss: 0.1200 - f1_score_mod: 0.2223 - recall_mod: 0.1326 - precision_mod: 0.7010 - dur_error: 0.4666 - maestro_dur_loss: 0.0233 - val_loss: 0.1091 - val_f1_score_mod: 0.2136 - val_recall_mod: 0.1257 - val_precision_mod: 0.7191 - val_dur_error: 0.3228 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10852\n",
      "25256/25256 - 132s - loss: 0.1188 - f1_score_mod: 0.2346 - recall_mod: 0.1411 - precision_mod: 0.7040 - dur_error: 0.4601 - maestro_dur_loss: 0.0230 - val_loss: 0.1091 - val_f1_score_mod: 0.2242 - val_recall_mod: 0.1323 - val_precision_mod: 0.7421 - val_dur_error: 0.3508 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.10852 to 0.10797, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1177 - f1_score_mod: 0.2399 - recall_mod: 0.1447 - precision_mod: 0.7129 - dur_error: 0.4516 - maestro_dur_loss: 0.0226 - val_loss: 0.1080 - val_f1_score_mod: 0.2375 - val_recall_mod: 0.1426 - val_precision_mod: 0.7195 - val_dur_error: 0.3291 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10797 to 0.10736, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 133s - loss: 0.1164 - f1_score_mod: 0.2476 - recall_mod: 0.1503 - precision_mod: 0.7102 - dur_error: 0.4434 - maestro_dur_loss: 0.0222 - val_loss: 0.1074 - val_f1_score_mod: 0.2437 - val_recall_mod: 0.1463 - val_precision_mod: 0.7427 - val_dur_error: 0.3372 - val_maestro_dur_loss: 0.0169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10736\n",
      "25256/25256 - 132s - loss: 0.1153 - f1_score_mod: 0.2558 - recall_mod: 0.1560 - precision_mod: 0.7171 - dur_error: 0.4384 - maestro_dur_loss: 0.0219 - val_loss: 0.1086 - val_f1_score_mod: 0.2634 - val_recall_mod: 0.1615 - val_precision_mod: 0.7187 - val_dur_error: 0.3604 - val_maestro_dur_loss: 0.0180\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10736 to 0.10434, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1145 - f1_score_mod: 0.2641 - recall_mod: 0.1622 - precision_mod: 0.7178 - dur_error: 0.4376 - maestro_dur_loss: 0.0219 - val_loss: 0.1043 - val_f1_score_mod: 0.2683 - val_recall_mod: 0.1646 - val_precision_mod: 0.7312 - val_dur_error: 0.3008 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10434 to 0.10330, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1135 - f1_score_mod: 0.2743 - recall_mod: 0.1702 - precision_mod: 0.7182 - dur_error: 0.4321 - maestro_dur_loss: 0.0216 - val_loss: 0.1033 - val_f1_score_mod: 0.2804 - val_recall_mod: 0.1756 - val_precision_mod: 0.7016 - val_dur_error: 0.2879 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10330 to 0.10272, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1125 - f1_score_mod: 0.2831 - recall_mod: 0.1765 - precision_mod: 0.7211 - dur_error: 0.4275 - maestro_dur_loss: 0.0214 - val_loss: 0.1027 - val_f1_score_mod: 0.2710 - val_recall_mod: 0.1661 - val_precision_mod: 0.7427 - val_dur_error: 0.2861 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10272\n",
      "25256/25256 - 133s - loss: 0.1117 - f1_score_mod: 0.2864 - recall_mod: 0.1789 - precision_mod: 0.7230 - dur_error: 0.4270 - maestro_dur_loss: 0.0214 - val_loss: 0.1037 - val_f1_score_mod: 0.2773 - val_recall_mod: 0.1700 - val_precision_mod: 0.7589 - val_dur_error: 0.3144 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10272\n",
      "25256/25256 - 132s - loss: 0.1105 - f1_score_mod: 0.2957 - recall_mod: 0.1857 - precision_mod: 0.7302 - dur_error: 0.4185 - maestro_dur_loss: 0.0209 - val_loss: 0.1075 - val_f1_score_mod: 0.2900 - val_recall_mod: 0.1820 - val_precision_mod: 0.7199 - val_dur_error: 0.3865 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10272\n",
      "25256/25256 - 132s - loss: 0.1095 - f1_score_mod: 0.3049 - recall_mod: 0.1935 - precision_mod: 0.7259 - dur_error: 0.4136 - maestro_dur_loss: 0.0207 - val_loss: 0.1056 - val_f1_score_mod: 0.2956 - val_recall_mod: 0.1875 - val_precision_mod: 0.7042 - val_dur_error: 0.3566 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10272 to 0.10153, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1089 - f1_score_mod: 0.3091 - recall_mod: 0.1966 - precision_mod: 0.7277 - dur_error: 0.4163 - maestro_dur_loss: 0.0208 - val_loss: 0.1015 - val_f1_score_mod: 0.3135 - val_recall_mod: 0.2008 - val_precision_mod: 0.7195 - val_dur_error: 0.2967 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10153 to 0.10124, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1079 - f1_score_mod: 0.3123 - recall_mod: 0.1993 - precision_mod: 0.7258 - dur_error: 0.4106 - maestro_dur_loss: 0.0205 - val_loss: 0.1012 - val_f1_score_mod: 0.3093 - val_recall_mod: 0.1963 - val_precision_mod: 0.7313 - val_dur_error: 0.3008 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10124 to 0.10116, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1069 - f1_score_mod: 0.3256 - recall_mod: 0.2097 - precision_mod: 0.7321 - dur_error: 0.4086 - maestro_dur_loss: 0.0204 - val_loss: 0.1012 - val_f1_score_mod: 0.3074 - val_recall_mod: 0.1943 - val_precision_mod: 0.7390 - val_dur_error: 0.3017 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10116 to 0.10087, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1062 - f1_score_mod: 0.3307 - recall_mod: 0.2139 - precision_mod: 0.7363 - dur_error: 0.4064 - maestro_dur_loss: 0.0203 - val_loss: 0.1009 - val_f1_score_mod: 0.3158 - val_recall_mod: 0.2021 - val_precision_mod: 0.7259 - val_dur_error: 0.3081 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10087\n",
      "25256/25256 - 132s - loss: 0.1053 - f1_score_mod: 0.3417 - recall_mod: 0.2231 - precision_mod: 0.7341 - dur_error: 0.4043 - maestro_dur_loss: 0.0202 - val_loss: 0.1044 - val_f1_score_mod: 0.3194 - val_recall_mod: 0.2056 - val_precision_mod: 0.7203 - val_dur_error: 0.3864 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10087\n",
      "25256/25256 - 131s - loss: 0.1043 - f1_score_mod: 0.3471 - recall_mod: 0.2275 - precision_mod: 0.7359 - dur_error: 0.3991 - maestro_dur_loss: 0.0200 - val_loss: 0.1047 - val_f1_score_mod: 0.3379 - val_recall_mod: 0.2212 - val_precision_mod: 0.7189 - val_dur_error: 0.4022 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10087 to 0.09799, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 133s - loss: 0.1035 - f1_score_mod: 0.3525 - recall_mod: 0.2323 - precision_mod: 0.7348 - dur_error: 0.3998 - maestro_dur_loss: 0.0200 - val_loss: 0.0980 - val_f1_score_mod: 0.3356 - val_recall_mod: 0.2184 - val_precision_mod: 0.7308 - val_dur_error: 0.2768 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09799\n",
      "25256/25256 - 132s - loss: 0.1024 - f1_score_mod: 0.3575 - recall_mod: 0.2364 - precision_mod: 0.7370 - dur_error: 0.3943 - maestro_dur_loss: 0.0197 - val_loss: 0.1047 - val_f1_score_mod: 0.3431 - val_recall_mod: 0.2248 - val_precision_mod: 0.7280 - val_dur_error: 0.4224 - val_maestro_dur_loss: 0.0211\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09799 to 0.09798, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.1018 - f1_score_mod: 0.3660 - recall_mod: 0.2440 - precision_mod: 0.7366 - dur_error: 0.3972 - maestro_dur_loss: 0.0199 - val_loss: 0.0980 - val_f1_score_mod: 0.3464 - val_recall_mod: 0.2270 - val_precision_mod: 0.7384 - val_dur_error: 0.3014 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09798\n",
      "25256/25256 - 132s - loss: 0.1010 - f1_score_mod: 0.3709 - recall_mod: 0.2480 - precision_mod: 0.7393 - dur_error: 0.3920 - maestro_dur_loss: 0.0196 - val_loss: 0.1012 - val_f1_score_mod: 0.3716 - val_recall_mod: 0.2540 - val_precision_mod: 0.6941 - val_dur_error: 0.3582 - val_maestro_dur_loss: 0.0179\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09798\n",
      "25256/25256 - 131s - loss: 0.1001 - f1_score_mod: 0.3785 - recall_mod: 0.2550 - precision_mod: 0.7375 - dur_error: 0.3887 - maestro_dur_loss: 0.0194 - val_loss: 0.0986 - val_f1_score_mod: 0.3591 - val_recall_mod: 0.2380 - val_precision_mod: 0.7390 - val_dur_error: 0.3287 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09798\n",
      "25256/25256 - 131s - loss: 0.0990 - f1_score_mod: 0.3831 - recall_mod: 0.2589 - precision_mod: 0.7408 - dur_error: 0.3851 - maestro_dur_loss: 0.0193 - val_loss: 0.1015 - val_f1_score_mod: 0.3651 - val_recall_mod: 0.2467 - val_precision_mod: 0.7099 - val_dur_error: 0.3745 - val_maestro_dur_loss: 0.0187\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09798 to 0.09485, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25256/25256 - 132s - loss: 0.0984 - f1_score_mod: 0.3905 - recall_mod: 0.2661 - precision_mod: 0.7377 - dur_error: 0.3854 - maestro_dur_loss: 0.0193 - val_loss: 0.0949 - val_f1_score_mod: 0.3619 - val_recall_mod: 0.2416 - val_precision_mod: 0.7273 - val_dur_error: 0.2624 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 41/150\n",
      "Batch 44: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23040/25256 - 105s - loss: nan - f1_score_mod: 0.3976 - recall_mod: 0.2718 - precision_mod: 0.7434 - dur_error: 0.3811 - maestro_dur_loss: 0.0191\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipnorm = 0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Surprisingly, this model failed earlier than the last one. This is possibly due to the randomized weight initialization performed by keras. Let's now try specifying clipvalue = 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25256 samples, validate on 10824 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14660, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 135s - loss: 0.2179 - f1_score_mod: 0.0149 - recall_mod: 0.0237 - precision_mod: 0.1051 - dur_error: 0.9675 - maestro_dur_loss: 0.0484 - val_loss: 0.1466 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5633 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14660 to 0.14057, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1613 - f1_score_mod: 0.0052 - recall_mod: 0.0027 - precision_mod: 0.2944 - dur_error: 0.6947 - maestro_dur_loss: 0.0347 - val_loss: 0.1406 - val_f1_score_mod: 0.0052 - val_recall_mod: 0.0026 - val_precision_mod: 0.6562 - val_dur_error: 0.5456 - val_maestro_dur_loss: 0.0273\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14057 to 0.13080, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 134s - loss: 0.1519 - f1_score_mod: 0.0304 - recall_mod: 0.0163 - precision_mod: 0.5493 - dur_error: 0.6341 - maestro_dur_loss: 0.0317 - val_loss: 0.1308 - val_f1_score_mod: 0.0240 - val_recall_mod: 0.0123 - val_precision_mod: 0.6030 - val_dur_error: 0.4315 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13080\n",
      "25256/25256 - 133s - loss: 0.1500 - f1_score_mod: 0.0455 - recall_mod: 0.0240 - precision_mod: 0.5708 - dur_error: 0.6580 - maestro_dur_loss: 0.0329 - val_loss: 0.1381 - val_f1_score_mod: 0.0244 - val_recall_mod: 0.0126 - val_precision_mod: 0.6445 - val_dur_error: 0.6063 - val_maestro_dur_loss: 0.0303\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13080 to 0.12806, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1425 - f1_score_mod: 0.0606 - recall_mod: 0.0320 - precision_mod: 0.6405 - dur_error: 0.5928 - maestro_dur_loss: 0.0296 - val_loss: 0.1281 - val_f1_score_mod: 0.0563 - val_recall_mod: 0.0294 - val_precision_mod: 0.7183 - val_dur_error: 0.4721 - val_maestro_dur_loss: 0.0236\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12806 to 0.12717, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 131s - loss: 0.1390 - f1_score_mod: 0.0848 - recall_mod: 0.0457 - precision_mod: 0.6406 - dur_error: 0.5761 - maestro_dur_loss: 0.0288 - val_loss: 0.1272 - val_f1_score_mod: 0.0868 - val_recall_mod: 0.0465 - val_precision_mod: 0.6773 - val_dur_error: 0.4897 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12717 to 0.12611, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 131s - loss: 0.1368 - f1_score_mod: 0.1005 - recall_mod: 0.0549 - precision_mod: 0.6341 - dur_error: 0.5678 - maestro_dur_loss: 0.0284 - val_loss: 0.1261 - val_f1_score_mod: 0.0704 - val_recall_mod: 0.0371 - val_precision_mod: 0.7111 - val_dur_error: 0.4004 - val_maestro_dur_loss: 0.0200\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12611\n",
      "25256/25256 - 131s - loss: 0.1345 - f1_score_mod: 0.1154 - recall_mod: 0.0637 - precision_mod: 0.6525 - dur_error: 0.5532 - maestro_dur_loss: 0.0277 - val_loss: 0.1294 - val_f1_score_mod: 0.1440 - val_recall_mod: 0.0815 - val_precision_mod: 0.6375 - val_dur_error: 0.5552 - val_maestro_dur_loss: 0.0278\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12611 to 0.12576, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 131s - loss: 0.1336 - f1_score_mod: 0.1300 - recall_mod: 0.0726 - precision_mod: 0.6604 - dur_error: 0.5557 - maestro_dur_loss: 0.0278 - val_loss: 0.1258 - val_f1_score_mod: 0.1587 - val_recall_mod: 0.0904 - val_precision_mod: 0.6605 - val_dur_error: 0.5129 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12576 to 0.12285, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 131s - loss: 0.1312 - f1_score_mod: 0.1411 - recall_mod: 0.0793 - precision_mod: 0.6680 - dur_error: 0.5405 - maestro_dur_loss: 0.0270 - val_loss: 0.1228 - val_f1_score_mod: 0.1521 - val_recall_mod: 0.0854 - val_precision_mod: 0.7117 - val_dur_error: 0.4918 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12285 to 0.11951, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 134s - loss: 0.1294 - f1_score_mod: 0.1563 - recall_mod: 0.0888 - precision_mod: 0.6762 - dur_error: 0.5291 - maestro_dur_loss: 0.0265 - val_loss: 0.1195 - val_f1_score_mod: 0.1674 - val_recall_mod: 0.0951 - val_precision_mod: 0.7066 - val_dur_error: 0.4259 - val_maestro_dur_loss: 0.0213\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11951 to 0.11843, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 133s - loss: 0.1282 - f1_score_mod: 0.1633 - recall_mod: 0.0933 - precision_mod: 0.6721 - dur_error: 0.5183 - maestro_dur_loss: 0.0259 - val_loss: 0.1184 - val_f1_score_mod: 0.1741 - val_recall_mod: 0.0994 - val_precision_mod: 0.7104 - val_dur_error: 0.4309 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11843 to 0.11449, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1263 - f1_score_mod: 0.1722 - recall_mod: 0.0990 - precision_mod: 0.6854 - dur_error: 0.5004 - maestro_dur_loss: 0.0250 - val_loss: 0.1145 - val_f1_score_mod: 0.1857 - val_recall_mod: 0.1080 - val_precision_mod: 0.6752 - val_dur_error: 0.3710 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11449\n",
      "25256/25256 - 134s - loss: 0.1254 - f1_score_mod: 0.1807 - recall_mod: 0.1043 - precision_mod: 0.6849 - dur_error: 0.4981 - maestro_dur_loss: 0.0249 - val_loss: 0.1216 - val_f1_score_mod: 0.1914 - val_recall_mod: 0.1113 - val_precision_mod: 0.6951 - val_dur_error: 0.5102 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11449\n",
      "25256/25256 - 133s - loss: 0.1241 - f1_score_mod: 0.1911 - recall_mod: 0.1114 - precision_mod: 0.6820 - dur_error: 0.4892 - maestro_dur_loss: 0.0245 - val_loss: 0.1200 - val_f1_score_mod: 0.1759 - val_recall_mod: 0.1004 - val_precision_mod: 0.7355 - val_dur_error: 0.5029 - val_maestro_dur_loss: 0.0251\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11449 to 0.11427, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1231 - f1_score_mod: 0.1974 - recall_mod: 0.1155 - precision_mod: 0.6976 - dur_error: 0.4861 - maestro_dur_loss: 0.0243 - val_loss: 0.1143 - val_f1_score_mod: 0.2074 - val_recall_mod: 0.1225 - val_precision_mod: 0.6941 - val_dur_error: 0.3965 - val_maestro_dur_loss: 0.0198\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11427\n",
      "25256/25256 - 132s - loss: 0.1217 - f1_score_mod: 0.2041 - recall_mod: 0.1199 - precision_mod: 0.6968 - dur_error: 0.4761 - maestro_dur_loss: 0.0238 - val_loss: 0.1144 - val_f1_score_mod: 0.1963 - val_recall_mod: 0.1130 - val_precision_mod: 0.7710 - val_dur_error: 0.3973 - val_maestro_dur_loss: 0.0199\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11427 to 0.10954, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 133s - loss: 0.1206 - f1_score_mod: 0.2144 - recall_mod: 0.1267 - precision_mod: 0.7061 - dur_error: 0.4681 - maestro_dur_loss: 0.0234 - val_loss: 0.1095 - val_f1_score_mod: 0.2156 - val_recall_mod: 0.1268 - val_precision_mod: 0.7351 - val_dur_error: 0.3304 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10954\n",
      "25256/25256 - 132s - loss: 0.1197 - f1_score_mod: 0.2229 - recall_mod: 0.1330 - precision_mod: 0.7017 - dur_error: 0.4642 - maestro_dur_loss: 0.0232 - val_loss: 0.1137 - val_f1_score_mod: 0.2036 - val_recall_mod: 0.1172 - val_precision_mod: 0.7894 - val_dur_error: 0.4120 - val_maestro_dur_loss: 0.0206\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10954 to 0.10767, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1185 - f1_score_mod: 0.2333 - recall_mod: 0.1397 - precision_mod: 0.7139 - dur_error: 0.4544 - maestro_dur_loss: 0.0227 - val_loss: 0.1077 - val_f1_score_mod: 0.2262 - val_recall_mod: 0.1340 - val_precision_mod: 0.7415 - val_dur_error: 0.3156 - val_maestro_dur_loss: 0.0158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10767\n",
      "25256/25256 - 131s - loss: 0.1176 - f1_score_mod: 0.2373 - recall_mod: 0.1428 - precision_mod: 0.7118 - dur_error: 0.4486 - maestro_dur_loss: 0.0224 - val_loss: 0.1102 - val_f1_score_mod: 0.2364 - val_recall_mod: 0.1414 - val_precision_mod: 0.7314 - val_dur_error: 0.3700 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10767 to 0.10514, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1167 - f1_score_mod: 0.2456 - recall_mod: 0.1489 - precision_mod: 0.7123 - dur_error: 0.4455 - maestro_dur_loss: 0.0223 - val_loss: 0.1051 - val_f1_score_mod: 0.2462 - val_recall_mod: 0.1483 - val_precision_mod: 0.7321 - val_dur_error: 0.2862 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10514\n",
      "25256/25256 - 131s - loss: 0.1155 - f1_score_mod: 0.2533 - recall_mod: 0.1544 - precision_mod: 0.7143 - dur_error: 0.4391 - maestro_dur_loss: 0.0220 - val_loss: 0.1088 - val_f1_score_mod: 0.2624 - val_recall_mod: 0.1606 - val_precision_mod: 0.7260 - val_dur_error: 0.3690 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10514\n",
      "25256/25256 - 132s - loss: 0.1150 - f1_score_mod: 0.2637 - recall_mod: 0.1618 - precision_mod: 0.7209 - dur_error: 0.4420 - maestro_dur_loss: 0.0221 - val_loss: 0.1074 - val_f1_score_mod: 0.2639 - val_recall_mod: 0.1626 - val_precision_mod: 0.7163 - val_dur_error: 0.3413 - val_maestro_dur_loss: 0.0171\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.10514 to 0.10506, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1141 - f1_score_mod: 0.2706 - recall_mod: 0.1670 - precision_mod: 0.7201 - dur_error: 0.4363 - maestro_dur_loss: 0.0218 - val_loss: 0.1051 - val_f1_score_mod: 0.2593 - val_recall_mod: 0.1582 - val_precision_mod: 0.7303 - val_dur_error: 0.3038 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10506\n",
      "25256/25256 - 131s - loss: 0.1130 - f1_score_mod: 0.2746 - recall_mod: 0.1701 - precision_mod: 0.7193 - dur_error: 0.4283 - maestro_dur_loss: 0.0214 - val_loss: 0.1057 - val_f1_score_mod: 0.2757 - val_recall_mod: 0.1700 - val_precision_mod: 0.7333 - val_dur_error: 0.3373 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10506 to 0.10319, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1119 - f1_score_mod: 0.2848 - recall_mod: 0.1778 - precision_mod: 0.7241 - dur_error: 0.4249 - maestro_dur_loss: 0.0212 - val_loss: 0.1032 - val_f1_score_mod: 0.2809 - val_recall_mod: 0.1737 - val_precision_mod: 0.7445 - val_dur_error: 0.2924 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10319 to 0.10192, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1109 - f1_score_mod: 0.2921 - recall_mod: 0.1831 - precision_mod: 0.7276 - dur_error: 0.4188 - maestro_dur_loss: 0.0209 - val_loss: 0.1019 - val_f1_score_mod: 0.2905 - val_recall_mod: 0.1820 - val_precision_mod: 0.7303 - val_dur_error: 0.2793 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10192 to 0.10136, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1102 - f1_score_mod: 0.2962 - recall_mod: 0.1864 - precision_mod: 0.7277 - dur_error: 0.4196 - maestro_dur_loss: 0.0210 - val_loss: 0.1014 - val_f1_score_mod: 0.2900 - val_recall_mod: 0.1811 - val_precision_mod: 0.7365 - val_dur_error: 0.2861 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10136\n",
      "25256/25256 - 131s - loss: 0.1096 - f1_score_mod: 0.3026 - recall_mod: 0.1916 - precision_mod: 0.7261 - dur_error: 0.4168 - maestro_dur_loss: 0.0208 - val_loss: 0.1035 - val_f1_score_mod: 0.3028 - val_recall_mod: 0.1910 - val_precision_mod: 0.7342 - val_dur_error: 0.3368 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10136\n",
      "25256/25256 - 131s - loss: 0.1085 - f1_score_mod: 0.3101 - recall_mod: 0.1971 - precision_mod: 0.7312 - dur_error: 0.4134 - maestro_dur_loss: 0.0207 - val_loss: 0.1056 - val_f1_score_mod: 0.3201 - val_recall_mod: 0.2072 - val_precision_mod: 0.7070 - val_dur_error: 0.3785 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10136\n",
      "25256/25256 - 132s - loss: 0.1073 - f1_score_mod: 0.3211 - recall_mod: 0.2060 - precision_mod: 0.7317 - dur_error: 0.4081 - maestro_dur_loss: 0.0204 - val_loss: 0.1098 - val_f1_score_mod: 0.3292 - val_recall_mod: 0.2156 - val_precision_mod: 0.7018 - val_dur_error: 0.4632 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10136\n",
      "25256/25256 - 140s - loss: 0.1068 - f1_score_mod: 0.3272 - recall_mod: 0.2110 - precision_mod: 0.7337 - dur_error: 0.4086 - maestro_dur_loss: 0.0204 - val_loss: 0.1025 - val_f1_score_mod: 0.3303 - val_recall_mod: 0.2142 - val_precision_mod: 0.7275 - val_dur_error: 0.3432 - val_maestro_dur_loss: 0.0172\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10136\n",
      "25256/25256 - 136s - loss: 0.1061 - f1_score_mod: 0.3358 - recall_mod: 0.2178 - precision_mod: 0.7377 - dur_error: 0.4069 - maestro_dur_loss: 0.0203 - val_loss: 0.1064 - val_f1_score_mod: 0.3308 - val_recall_mod: 0.2138 - val_precision_mod: 0.7340 - val_dur_error: 0.4260 - val_maestro_dur_loss: 0.0213\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.10136 to 0.09849, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1050 - f1_score_mod: 0.3391 - recall_mod: 0.2207 - precision_mod: 0.7359 - dur_error: 0.4002 - maestro_dur_loss: 0.0200 - val_loss: 0.0985 - val_f1_score_mod: 0.3312 - val_recall_mod: 0.2140 - val_precision_mod: 0.7385 - val_dur_error: 0.2792 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09849\n",
      "25256/25256 - 132s - loss: 0.1044 - f1_score_mod: 0.3452 - recall_mod: 0.2259 - precision_mod: 0.7358 - dur_error: 0.3996 - maestro_dur_loss: 0.0200 - val_loss: 0.1032 - val_f1_score_mod: 0.3499 - val_recall_mod: 0.2316 - val_precision_mod: 0.7172 - val_dur_error: 0.3734 - val_maestro_dur_loss: 0.0187\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09849 to 0.09815, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1034 - f1_score_mod: 0.3529 - recall_mod: 0.2323 - precision_mod: 0.7385 - dur_error: 0.3953 - maestro_dur_loss: 0.0198 - val_loss: 0.0981 - val_f1_score_mod: 0.3418 - val_recall_mod: 0.2233 - val_precision_mod: 0.7307 - val_dur_error: 0.2804 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09815\n",
      "25256/25256 - 131s - loss: 0.1026 - f1_score_mod: 0.3565 - recall_mod: 0.2354 - precision_mod: 0.7385 - dur_error: 0.3925 - maestro_dur_loss: 0.0196 - val_loss: 0.0992 - val_f1_score_mod: 0.3350 - val_recall_mod: 0.2165 - val_precision_mod: 0.7447 - val_dur_error: 0.3131 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09815 to 0.09722, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.1013 - f1_score_mod: 0.3675 - recall_mod: 0.2445 - precision_mod: 0.7425 - dur_error: 0.3875 - maestro_dur_loss: 0.0194 - val_loss: 0.0972 - val_f1_score_mod: 0.3510 - val_recall_mod: 0.2323 - val_precision_mod: 0.7216 - val_dur_error: 0.2759 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09722 to 0.09711, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 133s - loss: 0.1011 - f1_score_mod: 0.3720 - recall_mod: 0.2490 - precision_mod: 0.7390 - dur_error: 0.3921 - maestro_dur_loss: 0.0196 - val_loss: 0.0971 - val_f1_score_mod: 0.3541 - val_recall_mod: 0.2308 - val_precision_mod: 0.7650 - val_dur_error: 0.2883 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09711\n",
      "25256/25256 - 132s - loss: 0.1001 - f1_score_mod: 0.3800 - recall_mod: 0.2550 - precision_mod: 0.7491 - dur_error: 0.3868 - maestro_dur_loss: 0.0193 - val_loss: 0.0993 - val_f1_score_mod: 0.3648 - val_recall_mod: 0.2440 - val_precision_mod: 0.7306 - val_dur_error: 0.3344 - val_maestro_dur_loss: 0.0167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09711 to 0.09568, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 133s - loss: 0.0996 - f1_score_mod: 0.3811 - recall_mod: 0.2573 - precision_mod: 0.7401 - dur_error: 0.3859 - maestro_dur_loss: 0.0193 - val_loss: 0.0957 - val_f1_score_mod: 0.3727 - val_recall_mod: 0.2528 - val_precision_mod: 0.7142 - val_dur_error: 0.2645 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09568 to 0.09476, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25256/25256 - 132s - loss: 0.0986 - f1_score_mod: 0.3897 - recall_mod: 0.2645 - precision_mod: 0.7416 - dur_error: 0.3828 - maestro_dur_loss: 0.0191 - val_loss: 0.0948 - val_f1_score_mod: 0.3662 - val_recall_mod: 0.2432 - val_precision_mod: 0.7473 - val_dur_error: 0.2635 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09476\n",
      "25256/25256 - 135s - loss: 0.0976 - f1_score_mod: 0.3957 - recall_mod: 0.2697 - precision_mod: 0.7458 - dur_error: 0.3796 - maestro_dur_loss: 0.0190 - val_loss: 0.0949 - val_f1_score_mod: 0.3789 - val_recall_mod: 0.2570 - val_precision_mod: 0.7252 - val_dur_error: 0.2642 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09476\n",
      "25256/25256 - 133s - loss: 0.0965 - f1_score_mod: 0.4004 - recall_mod: 0.2741 - precision_mod: 0.7466 - dur_error: 0.3745 - maestro_dur_loss: 0.0187 - val_loss: 0.0959 - val_f1_score_mod: 0.3808 - val_recall_mod: 0.2594 - val_precision_mod: 0.7194 - val_dur_error: 0.2841 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09476\n",
      "25256/25256 - 133s - loss: 0.0962 - f1_score_mod: 0.4061 - recall_mod: 0.2798 - precision_mod: 0.7437 - dur_error: 0.3766 - maestro_dur_loss: 0.0188 - val_loss: 0.0990 - val_f1_score_mod: 0.3885 - val_recall_mod: 0.2687 - val_precision_mod: 0.7042 - val_dur_error: 0.3490 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09476\n",
      "25256/25256 - 134s - loss: 0.0954 - f1_score_mod: 0.4113 - recall_mod: 0.2847 - precision_mod: 0.7425 - dur_error: 0.3723 - maestro_dur_loss: 0.0186 - val_loss: 0.0955 - val_f1_score_mod: 0.3863 - val_recall_mod: 0.2652 - val_precision_mod: 0.7148 - val_dur_error: 0.2958 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09476\n",
      "25256/25256 - 134s - loss: 0.0946 - f1_score_mod: 0.4180 - recall_mod: 0.2905 - precision_mod: 0.7479 - dur_error: 0.3730 - maestro_dur_loss: 0.0186 - val_loss: 0.0955 - val_f1_score_mod: 0.3901 - val_recall_mod: 0.2694 - val_precision_mod: 0.7089 - val_dur_error: 0.2949 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09476\n",
      "25256/25256 - 134s - loss: 0.0934 - f1_score_mod: 0.4250 - recall_mod: 0.2982 - precision_mod: 0.7420 - dur_error: 0.3694 - maestro_dur_loss: 0.0185 - val_loss: 0.0976 - val_f1_score_mod: 0.3959 - val_recall_mod: 0.2755 - val_precision_mod: 0.7098 - val_dur_error: 0.3444 - val_maestro_dur_loss: 0.0172\n",
      "Epoch 50/150\n",
      "Batch 22: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11776/25256 - 52s - loss: nan - f1_score_mod: 0.4305 - recall_mod: 0.3029 - precision_mod: 0.7457 - dur_error: 0.3677 - maestro_dur_loss: 0.0184\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipvalue = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than the base model, but not as effective as clipnorm = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25256 samples, validate on 10824 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15061, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 137s - loss: 0.2190 - f1_score_mod: 0.0156 - recall_mod: 0.0250 - precision_mod: 0.1081 - dur_error: 0.9839 - maestro_dur_loss: 0.0492 - val_loss: 0.1506 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6511 - val_maestro_dur_loss: 0.0326\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15061 to 0.14876, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 134s - loss: 0.1617 - f1_score_mod: 0.0030 - recall_mod: 0.0015 - precision_mod: 0.2526 - dur_error: 0.6876 - maestro_dur_loss: 0.0344 - val_loss: 0.1488 - val_f1_score_mod: 0.0064 - val_recall_mod: 0.0032 - val_precision_mod: 0.6569 - val_dur_error: 0.6038 - val_maestro_dur_loss: 0.0302\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14876 to 0.13092, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 133s - loss: 0.1521 - f1_score_mod: 0.0225 - recall_mod: 0.0117 - precision_mod: 0.5532 - dur_error: 0.6260 - maestro_dur_loss: 0.0313 - val_loss: 0.1309 - val_f1_score_mod: 0.0166 - val_recall_mod: 0.0084 - val_precision_mod: 0.8243 - val_dur_error: 0.4300 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13092 to 0.12991, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1459 - f1_score_mod: 0.0517 - recall_mod: 0.0272 - precision_mod: 0.6378 - dur_error: 0.6095 - maestro_dur_loss: 0.0305 - val_loss: 0.1299 - val_f1_score_mod: 0.0489 - val_recall_mod: 0.0257 - val_precision_mod: 0.6417 - val_dur_error: 0.4520 - val_maestro_dur_loss: 0.0226\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12991\n",
      "25256/25256 - 133s - loss: 0.1436 - f1_score_mod: 0.0679 - recall_mod: 0.0362 - precision_mod: 0.6098 - dur_error: 0.6070 - maestro_dur_loss: 0.0303 - val_loss: 0.1333 - val_f1_score_mod: 0.0478 - val_recall_mod: 0.0248 - val_precision_mod: 0.6853 - val_dur_error: 0.5156 - val_maestro_dur_loss: 0.0258\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12991\n",
      "25256/25256 - 133s - loss: 0.1392 - f1_score_mod: 0.0864 - recall_mod: 0.0466 - precision_mod: 0.6324 - dur_error: 0.5759 - maestro_dur_loss: 0.0288 - val_loss: 0.1303 - val_f1_score_mod: 0.1058 - val_recall_mod: 0.0574 - val_precision_mod: 0.6939 - val_dur_error: 0.5371 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12991 to 0.12666, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1365 - f1_score_mod: 0.1055 - recall_mod: 0.0579 - precision_mod: 0.6339 - dur_error: 0.5672 - maestro_dur_loss: 0.0284 - val_loss: 0.1267 - val_f1_score_mod: 0.1013 - val_recall_mod: 0.0545 - val_precision_mod: 0.7493 - val_dur_error: 0.5072 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12666 to 0.12488, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1350 - f1_score_mod: 0.1168 - recall_mod: 0.0646 - precision_mod: 0.6402 - dur_error: 0.5603 - maestro_dur_loss: 0.0280 - val_loss: 0.1249 - val_f1_score_mod: 0.1087 - val_recall_mod: 0.0587 - val_precision_mod: 0.7550 - val_dur_error: 0.4875 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12488\n",
      "25256/25256 - 132s - loss: 0.1333 - f1_score_mod: 0.1295 - recall_mod: 0.0724 - precision_mod: 0.6549 - dur_error: 0.5520 - maestro_dur_loss: 0.0276 - val_loss: 0.1292 - val_f1_score_mod: 0.1335 - val_recall_mod: 0.0741 - val_precision_mod: 0.6856 - val_dur_error: 0.5660 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12488\n",
      "25256/25256 - 133s - loss: 0.1320 - f1_score_mod: 0.1381 - recall_mod: 0.0775 - precision_mod: 0.6647 - dur_error: 0.5446 - maestro_dur_loss: 0.0272 - val_loss: 0.1295 - val_f1_score_mod: 0.1589 - val_recall_mod: 0.0905 - val_precision_mod: 0.6584 - val_dur_error: 0.5834 - val_maestro_dur_loss: 0.0292\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12488 to 0.11779, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1306 - f1_score_mod: 0.1517 - recall_mod: 0.0861 - precision_mod: 0.6587 - dur_error: 0.5411 - maestro_dur_loss: 0.0271 - val_loss: 0.1178 - val_f1_score_mod: 0.1299 - val_recall_mod: 0.0711 - val_precision_mod: 0.7777 - val_dur_error: 0.4056 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11779\n",
      "25256/25256 - 132s - loss: 0.1287 - f1_score_mod: 0.1644 - recall_mod: 0.0938 - precision_mod: 0.6806 - dur_error: 0.5249 - maestro_dur_loss: 0.0262 - val_loss: 0.1217 - val_f1_score_mod: 0.1487 - val_recall_mod: 0.0826 - val_precision_mod: 0.7508 - val_dur_error: 0.4896 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11779\n",
      "25256/25256 - 132s - loss: 0.1271 - f1_score_mod: 0.1708 - recall_mod: 0.0981 - precision_mod: 0.6782 - dur_error: 0.5116 - maestro_dur_loss: 0.0256 - val_loss: 0.1200 - val_f1_score_mod: 0.1679 - val_recall_mod: 0.0950 - val_precision_mod: 0.7420 - val_dur_error: 0.4688 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11779 to 0.11537, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1261 - f1_score_mod: 0.1810 - recall_mod: 0.1048 - precision_mod: 0.6864 - dur_error: 0.5101 - maestro_dur_loss: 0.0255 - val_loss: 0.1154 - val_f1_score_mod: 0.1793 - val_recall_mod: 0.1024 - val_precision_mod: 0.7561 - val_dur_error: 0.4020 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11537 to 0.11512, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1242 - f1_score_mod: 0.1914 - recall_mod: 0.1114 - precision_mod: 0.6947 - dur_error: 0.4905 - maestro_dur_loss: 0.0245 - val_loss: 0.1151 - val_f1_score_mod: 0.1878 - val_recall_mod: 0.1081 - val_precision_mod: 0.7299 - val_dur_error: 0.4011 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11512 to 0.11189, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1233 - f1_score_mod: 0.1991 - recall_mod: 0.1169 - precision_mod: 0.6892 - dur_error: 0.4859 - maestro_dur_loss: 0.0243 - val_loss: 0.1119 - val_f1_score_mod: 0.1833 - val_recall_mod: 0.1048 - val_precision_mod: 0.7473 - val_dur_error: 0.3325 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11189 to 0.10884, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1212 - f1_score_mod: 0.2076 - recall_mod: 0.1222 - precision_mod: 0.7047 - dur_error: 0.4618 - maestro_dur_loss: 0.0231 - val_loss: 0.1088 - val_f1_score_mod: 0.2228 - val_recall_mod: 0.1329 - val_precision_mod: 0.6947 - val_dur_error: 0.3068 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10884\n",
      "25256/25256 - 135s - loss: 0.1210 - f1_score_mod: 0.2137 - recall_mod: 0.1264 - precision_mod: 0.7040 - dur_error: 0.4720 - maestro_dur_loss: 0.0236 - val_loss: 0.1163 - val_f1_score_mod: 0.2197 - val_recall_mod: 0.1310 - val_precision_mod: 0.6889 - val_dur_error: 0.4482 - val_maestro_dur_loss: 0.0224\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10884\n",
      "25256/25256 - 133s - loss: 0.1197 - f1_score_mod: 0.2220 - recall_mod: 0.1322 - precision_mod: 0.6998 - dur_error: 0.4580 - maestro_dur_loss: 0.0229 - val_loss: 0.1111 - val_f1_score_mod: 0.2178 - val_recall_mod: 0.1285 - val_precision_mod: 0.7213 - val_dur_error: 0.3664 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10884\n",
      "25256/25256 - 132s - loss: 0.1189 - f1_score_mod: 0.2255 - recall_mod: 0.1345 - precision_mod: 0.7102 - dur_error: 0.4556 - maestro_dur_loss: 0.0228 - val_loss: 0.1126 - val_f1_score_mod: 0.2424 - val_recall_mod: 0.1472 - val_precision_mod: 0.6899 - val_dur_error: 0.4018 - val_maestro_dur_loss: 0.0201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10884\n",
      "25256/25256 - 132s - loss: 0.1181 - f1_score_mod: 0.2329 - recall_mod: 0.1398 - precision_mod: 0.7064 - dur_error: 0.4514 - maestro_dur_loss: 0.0226 - val_loss: 0.1132 - val_f1_score_mod: 0.2402 - val_recall_mod: 0.1452 - val_precision_mod: 0.7025 - val_dur_error: 0.4088 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10884\n",
      "25256/25256 - 133s - loss: 0.1179 - f1_score_mod: 0.2363 - recall_mod: 0.1425 - precision_mod: 0.7079 - dur_error: 0.4531 - maestro_dur_loss: 0.0227 - val_loss: 0.1136 - val_f1_score_mod: 0.2499 - val_recall_mod: 0.1523 - val_precision_mod: 0.7016 - val_dur_error: 0.4439 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10884\n",
      "25256/25256 - 133s - loss: 0.1167 - f1_score_mod: 0.2471 - recall_mod: 0.1497 - precision_mod: 0.7156 - dur_error: 0.4477 - maestro_dur_loss: 0.0224 - val_loss: 0.1148 - val_f1_score_mod: 0.2605 - val_recall_mod: 0.1595 - val_precision_mod: 0.7185 - val_dur_error: 0.4744 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10884 to 0.10448, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1159 - f1_score_mod: 0.2546 - recall_mod: 0.1550 - precision_mod: 0.7207 - dur_error: 0.4434 - maestro_dur_loss: 0.0222 - val_loss: 0.1045 - val_f1_score_mod: 0.2565 - val_recall_mod: 0.1561 - val_precision_mod: 0.7225 - val_dur_error: 0.2878 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10448\n",
      "25256/25256 - 132s - loss: 0.1147 - f1_score_mod: 0.2615 - recall_mod: 0.1602 - precision_mod: 0.7196 - dur_error: 0.4343 - maestro_dur_loss: 0.0217 - val_loss: 0.1046 - val_f1_score_mod: 0.2696 - val_recall_mod: 0.1666 - val_precision_mod: 0.7190 - val_dur_error: 0.2927 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10448\n",
      "25256/25256 - 132s - loss: 0.1140 - f1_score_mod: 0.2697 - recall_mod: 0.1661 - precision_mod: 0.7247 - dur_error: 0.4341 - maestro_dur_loss: 0.0217 - val_loss: 0.1055 - val_f1_score_mod: 0.2728 - val_recall_mod: 0.1687 - val_precision_mod: 0.7226 - val_dur_error: 0.3192 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10448 to 0.10327, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 132s - loss: 0.1131 - f1_score_mod: 0.2751 - recall_mod: 0.1701 - precision_mod: 0.7234 - dur_error: 0.4301 - maestro_dur_loss: 0.0215 - val_loss: 0.1033 - val_f1_score_mod: 0.2668 - val_recall_mod: 0.1637 - val_precision_mod: 0.7252 - val_dur_error: 0.2817 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10327\n",
      "25256/25256 - 132s - loss: 0.1121 - f1_score_mod: 0.2834 - recall_mod: 0.1765 - precision_mod: 0.7270 - dur_error: 0.4257 - maestro_dur_loss: 0.0213 - val_loss: 0.1033 - val_f1_score_mod: 0.2660 - val_recall_mod: 0.1632 - val_precision_mod: 0.7374 - val_dur_error: 0.3039 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10327\n",
      "25256/25256 - 132s - loss: 0.1112 - f1_score_mod: 0.2891 - recall_mod: 0.1810 - precision_mod: 0.7266 - dur_error: 0.4206 - maestro_dur_loss: 0.0210 - val_loss: 0.1105 - val_f1_score_mod: 0.2794 - val_recall_mod: 0.1725 - val_precision_mod: 0.7414 - val_dur_error: 0.4492 - val_maestro_dur_loss: 0.0225\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10327 to 0.10219, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 133s - loss: 0.1105 - f1_score_mod: 0.2966 - recall_mod: 0.1867 - precision_mod: 0.7259 - dur_error: 0.4195 - maestro_dur_loss: 0.0210 - val_loss: 0.1022 - val_f1_score_mod: 0.2797 - val_recall_mod: 0.1726 - val_precision_mod: 0.7418 - val_dur_error: 0.2916 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10219 to 0.10018, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 134s - loss: 0.1098 - f1_score_mod: 0.3030 - recall_mod: 0.1915 - precision_mod: 0.7320 - dur_error: 0.4178 - maestro_dur_loss: 0.0209 - val_loss: 0.1002 - val_f1_score_mod: 0.2870 - val_recall_mod: 0.1774 - val_precision_mod: 0.7596 - val_dur_error: 0.2709 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10018\n",
      "25256/25256 - 133s - loss: 0.1087 - f1_score_mod: 0.3108 - recall_mod: 0.1976 - precision_mod: 0.7339 - dur_error: 0.4129 - maestro_dur_loss: 0.0206 - val_loss: 0.1039 - val_f1_score_mod: 0.3152 - val_recall_mod: 0.2014 - val_precision_mod: 0.7304 - val_dur_error: 0.3451 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10018\n",
      "25256/25256 - 133s - loss: 0.1080 - f1_score_mod: 0.3179 - recall_mod: 0.2034 - precision_mod: 0.7341 - dur_error: 0.4132 - maestro_dur_loss: 0.0207 - val_loss: 0.1019 - val_f1_score_mod: 0.3164 - val_recall_mod: 0.2041 - val_precision_mod: 0.7083 - val_dur_error: 0.3183 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10018\n",
      "25256/25256 - 133s - loss: 0.1070 - f1_score_mod: 0.3252 - recall_mod: 0.2094 - precision_mod: 0.7328 - dur_error: 0.4065 - maestro_dur_loss: 0.0203 - val_loss: 0.1019 - val_f1_score_mod: 0.3111 - val_recall_mod: 0.1987 - val_precision_mod: 0.7243 - val_dur_error: 0.3185 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10018\n",
      "25256/25256 - 134s - loss: 0.1062 - f1_score_mod: 0.3315 - recall_mod: 0.2146 - precision_mod: 0.7348 - dur_error: 0.4051 - maestro_dur_loss: 0.0203 - val_loss: 0.1008 - val_f1_score_mod: 0.3357 - val_recall_mod: 0.2205 - val_precision_mod: 0.7075 - val_dur_error: 0.3058 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10018\n",
      "25256/25256 - 133s - loss: 0.1051 - f1_score_mod: 0.3370 - recall_mod: 0.2187 - precision_mod: 0.7397 - dur_error: 0.3972 - maestro_dur_loss: 0.0199 - val_loss: 0.1031 - val_f1_score_mod: 0.3240 - val_recall_mod: 0.2087 - val_precision_mod: 0.7311 - val_dur_error: 0.3609 - val_maestro_dur_loss: 0.0180\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.10018 to 0.09860, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 134s - loss: 0.1045 - f1_score_mod: 0.3459 - recall_mod: 0.2263 - precision_mod: 0.7382 - dur_error: 0.3996 - maestro_dur_loss: 0.0200 - val_loss: 0.0986 - val_f1_score_mod: 0.3319 - val_recall_mod: 0.2156 - val_precision_mod: 0.7257 - val_dur_error: 0.2803 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09860 to 0.09821, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 133s - loss: 0.1036 - f1_score_mod: 0.3508 - recall_mod: 0.2300 - precision_mod: 0.7423 - dur_error: 0.3940 - maestro_dur_loss: 0.0197 - val_loss: 0.0982 - val_f1_score_mod: 0.3308 - val_recall_mod: 0.2128 - val_precision_mod: 0.7470 - val_dur_error: 0.2746 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09821\n",
      "25256/25256 - 133s - loss: 0.1031 - f1_score_mod: 0.3550 - recall_mod: 0.2339 - precision_mod: 0.7394 - dur_error: 0.3951 - maestro_dur_loss: 0.0198 - val_loss: 0.1036 - val_f1_score_mod: 0.3462 - val_recall_mod: 0.2277 - val_precision_mod: 0.7252 - val_dur_error: 0.3931 - val_maestro_dur_loss: 0.0197\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09821\n",
      "25256/25256 - 133s - loss: 0.1023 - f1_score_mod: 0.3621 - recall_mod: 0.2405 - precision_mod: 0.7362 - dur_error: 0.3950 - maestro_dur_loss: 0.0197 - val_loss: 0.0988 - val_f1_score_mod: 0.3496 - val_recall_mod: 0.2301 - val_precision_mod: 0.7326 - val_dur_error: 0.3015 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09821 to 0.09641, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25256/25256 - 134s - loss: 0.1013 - f1_score_mod: 0.3671 - recall_mod: 0.2448 - precision_mod: 0.7383 - dur_error: 0.3888 - maestro_dur_loss: 0.0194 - val_loss: 0.0964 - val_f1_score_mod: 0.3522 - val_recall_mod: 0.2329 - val_precision_mod: 0.7247 - val_dur_error: 0.2664 - val_maestro_dur_loss: 0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "Batch 22: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11776/25256 - 54s - loss: nan - f1_score_mod: 0.3739 - recall_mod: 0.2507 - precision_mod: 0.7398 - dur_error: 0.3887 - maestro_dur_loss: 0.0194\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipvalue = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, not as effective in delaying failure as clipvalue = 0.2. The best results were achieved with clipnorm = 1.0 and lr = 5e-4. Let's now combine these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out visualize_performance.ipynb (Figures 1 and 2) for a comparison of the model training between those run so far (excluding the clipvalue runs since it appears setting clipnorm is a better approach.\n",
    "Next, let's combine the our best clipnorm value (0.5) with an lr of 5e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25256 samples, validate on 10824 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14153, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 142s - loss: 0.2409 - f1_score_mod: 0.0244 - recall_mod: 0.0316 - precision_mod: 0.0844 - dur_error: 1.0069 - maestro_dur_loss: 0.0503 - val_loss: 0.1415 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4365 - val_maestro_dur_loss: 0.0218\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14153\n",
      "25256/25256 - 135s - loss: 0.1682 - f1_score_mod: 0.0046 - recall_mod: 0.0023 - precision_mod: 0.2031 - dur_error: 0.7270 - maestro_dur_loss: 0.0363 - val_loss: 0.1430 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5235 - val_maestro_dur_loss: 0.0262\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14153 to 0.13727, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 137s - loss: 0.1563 - f1_score_mod: 0.0079 - recall_mod: 0.0040 - precision_mod: 0.4024 - dur_error: 0.6421 - maestro_dur_loss: 0.0321 - val_loss: 0.1373 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5084 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13727\n",
      "25256/25256 - 136s - loss: 0.1493 - f1_score_mod: 0.0253 - recall_mod: 0.0130 - precision_mod: 0.5184 - dur_error: 0.6049 - maestro_dur_loss: 0.0302 - val_loss: 0.1382 - val_f1_score_mod: 0.0344 - val_recall_mod: 0.0177 - val_precision_mod: 0.6361 - val_dur_error: 0.5604 - val_maestro_dur_loss: 0.0280\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13727 to 0.13674, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1451 - f1_score_mod: 0.0446 - recall_mod: 0.0234 - precision_mod: 0.5592 - dur_error: 0.5865 - maestro_dur_loss: 0.0293 - val_loss: 0.1367 - val_f1_score_mod: 0.0369 - val_recall_mod: 0.0190 - val_precision_mod: 0.7817 - val_dur_error: 0.5860 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13674 to 0.12742, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1417 - f1_score_mod: 0.0635 - recall_mod: 0.0337 - precision_mod: 0.6182 - dur_error: 0.5706 - maestro_dur_loss: 0.0285 - val_loss: 0.1274 - val_f1_score_mod: 0.0703 - val_recall_mod: 0.0371 - val_precision_mod: 0.6846 - val_dur_error: 0.4406 - val_maestro_dur_loss: 0.0220\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12742 to 0.12252, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1386 - f1_score_mod: 0.0792 - recall_mod: 0.0425 - precision_mod: 0.6244 - dur_error: 0.5513 - maestro_dur_loss: 0.0276 - val_loss: 0.1225 - val_f1_score_mod: 0.0779 - val_recall_mod: 0.0412 - val_precision_mod: 0.7428 - val_dur_error: 0.3831 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12252 to 0.12108, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1361 - f1_score_mod: 0.0909 - recall_mod: 0.0493 - precision_mod: 0.6353 - dur_error: 0.5406 - maestro_dur_loss: 0.0270 - val_loss: 0.1211 - val_f1_score_mod: 0.0905 - val_recall_mod: 0.0484 - val_precision_mod: 0.7406 - val_dur_error: 0.3818 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12108\n",
      "25256/25256 - 134s - loss: 0.1343 - f1_score_mod: 0.1073 - recall_mod: 0.0588 - precision_mod: 0.6431 - dur_error: 0.5307 - maestro_dur_loss: 0.0265 - val_loss: 0.1255 - val_f1_score_mod: 0.1297 - val_recall_mod: 0.0721 - val_precision_mod: 0.6690 - val_dur_error: 0.4814 - val_maestro_dur_loss: 0.0241\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12108\n",
      "25256/25256 - 135s - loss: 0.1325 - f1_score_mod: 0.1212 - recall_mod: 0.0671 - precision_mod: 0.6556 - dur_error: 0.5242 - maestro_dur_loss: 0.0262 - val_loss: 0.1216 - val_f1_score_mod: 0.1417 - val_recall_mod: 0.0793 - val_precision_mod: 0.6740 - val_dur_error: 0.4284 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12108 to 0.11976, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1312 - f1_score_mod: 0.1291 - recall_mod: 0.0718 - precision_mod: 0.6563 - dur_error: 0.5198 - maestro_dur_loss: 0.0260 - val_loss: 0.1198 - val_f1_score_mod: 0.1493 - val_recall_mod: 0.0836 - val_precision_mod: 0.7136 - val_dur_error: 0.4068 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11976\n",
      "25256/25256 - 134s - loss: 0.1294 - f1_score_mod: 0.1396 - recall_mod: 0.0784 - precision_mod: 0.6575 - dur_error: 0.5068 - maestro_dur_loss: 0.0253 - val_loss: 0.1283 - val_f1_score_mod: 0.1516 - val_recall_mod: 0.0850 - val_precision_mod: 0.7108 - val_dur_error: 0.5945 - val_maestro_dur_loss: 0.0297\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11976 to 0.11525, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1279 - f1_score_mod: 0.1542 - recall_mod: 0.0874 - precision_mod: 0.6733 - dur_error: 0.4954 - maestro_dur_loss: 0.0248 - val_loss: 0.1152 - val_f1_score_mod: 0.1512 - val_recall_mod: 0.0847 - val_precision_mod: 0.7380 - val_dur_error: 0.3567 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11525 to 0.11313, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1266 - f1_score_mod: 0.1613 - recall_mod: 0.0918 - precision_mod: 0.6751 - dur_error: 0.4829 - maestro_dur_loss: 0.0241 - val_loss: 0.1131 - val_f1_score_mod: 0.1729 - val_recall_mod: 0.0983 - val_precision_mod: 0.7263 - val_dur_error: 0.3349 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11313\n",
      "25256/25256 - 134s - loss: 0.1253 - f1_score_mod: 0.1701 - recall_mod: 0.0977 - precision_mod: 0.6732 - dur_error: 0.4716 - maestro_dur_loss: 0.0236 - val_loss: 0.1181 - val_f1_score_mod: 0.1778 - val_recall_mod: 0.1022 - val_precision_mod: 0.6980 - val_dur_error: 0.4324 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11313 to 0.11112, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1243 - f1_score_mod: 0.1763 - recall_mod: 0.1015 - precision_mod: 0.6843 - dur_error: 0.4681 - maestro_dur_loss: 0.0234 - val_loss: 0.1111 - val_f1_score_mod: 0.1655 - val_recall_mod: 0.0937 - val_precision_mod: 0.7305 - val_dur_error: 0.3134 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11112\n",
      "25256/25256 - 133s - loss: 0.1231 - f1_score_mod: 0.1832 - recall_mod: 0.1059 - precision_mod: 0.6942 - dur_error: 0.4584 - maestro_dur_loss: 0.0229 - val_loss: 0.1115 - val_f1_score_mod: 0.1828 - val_recall_mod: 0.1043 - val_precision_mod: 0.7438 - val_dur_error: 0.3307 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11112 to 0.10944, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1224 - f1_score_mod: 0.1903 - recall_mod: 0.1106 - precision_mod: 0.6903 - dur_error: 0.4539 - maestro_dur_loss: 0.0227 - val_loss: 0.1094 - val_f1_score_mod: 0.1969 - val_recall_mod: 0.1142 - val_precision_mod: 0.7242 - val_dur_error: 0.2951 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10944\n",
      "25256/25256 - 137s - loss: 0.1212 - f1_score_mod: 0.1992 - recall_mod: 0.1164 - precision_mod: 0.6982 - dur_error: 0.4471 - maestro_dur_loss: 0.0224 - val_loss: 0.1129 - val_f1_score_mod: 0.1953 - val_recall_mod: 0.1127 - val_precision_mod: 0.7405 - val_dur_error: 0.3762 - val_maestro_dur_loss: 0.0188\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10944 to 0.10892, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1206 - f1_score_mod: 0.2034 - recall_mod: 0.1193 - precision_mod: 0.6996 - dur_error: 0.4432 - maestro_dur_loss: 0.0222 - val_loss: 0.1089 - val_f1_score_mod: 0.2049 - val_recall_mod: 0.1188 - val_precision_mod: 0.7586 - val_dur_error: 0.2989 - val_maestro_dur_loss: 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10892\n",
      "25256/25256 - 132s - loss: 0.1195 - f1_score_mod: 0.2112 - recall_mod: 0.1244 - precision_mod: 0.7061 - dur_error: 0.4347 - maestro_dur_loss: 0.0217 - val_loss: 0.1146 - val_f1_score_mod: 0.2087 - val_recall_mod: 0.1232 - val_precision_mod: 0.6986 - val_dur_error: 0.4138 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10892 to 0.10828, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1192 - f1_score_mod: 0.2169 - recall_mod: 0.1286 - precision_mod: 0.7015 - dur_error: 0.4375 - maestro_dur_loss: 0.0219 - val_loss: 0.1083 - val_f1_score_mod: 0.2303 - val_recall_mod: 0.1376 - val_precision_mod: 0.7296 - val_dur_error: 0.3095 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10828\n",
      "25256/25256 - 133s - loss: 0.1182 - f1_score_mod: 0.2227 - recall_mod: 0.1322 - precision_mod: 0.7096 - dur_error: 0.4312 - maestro_dur_loss: 0.0216 - val_loss: 0.1103 - val_f1_score_mod: 0.2137 - val_recall_mod: 0.1251 - val_precision_mod: 0.7543 - val_dur_error: 0.3340 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10828 to 0.10691, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1175 - f1_score_mod: 0.2296 - recall_mod: 0.1377 - precision_mod: 0.7027 - dur_error: 0.4267 - maestro_dur_loss: 0.0213 - val_loss: 0.1069 - val_f1_score_mod: 0.2340 - val_recall_mod: 0.1402 - val_precision_mod: 0.7183 - val_dur_error: 0.2938 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10691\n",
      "25256/25256 - 134s - loss: 0.1171 - f1_score_mod: 0.2358 - recall_mod: 0.1416 - precision_mod: 0.7112 - dur_error: 0.4287 - maestro_dur_loss: 0.0214 - val_loss: 0.1072 - val_f1_score_mod: 0.2197 - val_recall_mod: 0.1292 - val_precision_mod: 0.7436 - val_dur_error: 0.3117 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10691\n",
      "25256/25256 - 134s - loss: 0.1163 - f1_score_mod: 0.2387 - recall_mod: 0.1438 - precision_mod: 0.7134 - dur_error: 0.4223 - maestro_dur_loss: 0.0211 - val_loss: 0.1148 - val_f1_score_mod: 0.2534 - val_recall_mod: 0.1550 - val_precision_mod: 0.7082 - val_dur_error: 0.4624 - val_maestro_dur_loss: 0.0231\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10691 to 0.10575, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1158 - f1_score_mod: 0.2472 - recall_mod: 0.1498 - precision_mod: 0.7137 - dur_error: 0.4211 - maestro_dur_loss: 0.0211 - val_loss: 0.1058 - val_f1_score_mod: 0.2560 - val_recall_mod: 0.1566 - val_precision_mod: 0.7031 - val_dur_error: 0.2913 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10575\n",
      "25256/25256 - 133s - loss: 0.1151 - f1_score_mod: 0.2496 - recall_mod: 0.1516 - precision_mod: 0.7170 - dur_error: 0.4153 - maestro_dur_loss: 0.0208 - val_loss: 0.1079 - val_f1_score_mod: 0.2544 - val_recall_mod: 0.1544 - val_precision_mod: 0.7330 - val_dur_error: 0.3407 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10575 to 0.10485, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1144 - f1_score_mod: 0.2577 - recall_mod: 0.1573 - precision_mod: 0.7180 - dur_error: 0.4143 - maestro_dur_loss: 0.0207 - val_loss: 0.1048 - val_f1_score_mod: 0.2671 - val_recall_mod: 0.1647 - val_precision_mod: 0.7144 - val_dur_error: 0.2861 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10485\n",
      "25256/25256 - 133s - loss: 0.1140 - f1_score_mod: 0.2621 - recall_mod: 0.1605 - precision_mod: 0.7216 - dur_error: 0.4115 - maestro_dur_loss: 0.0206 - val_loss: 0.1075 - val_f1_score_mod: 0.2653 - val_recall_mod: 0.1640 - val_precision_mod: 0.6992 - val_dur_error: 0.3416 - val_maestro_dur_loss: 0.0171\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10485 to 0.10459, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1135 - f1_score_mod: 0.2670 - recall_mod: 0.1640 - precision_mod: 0.7246 - dur_error: 0.4121 - maestro_dur_loss: 0.0206 - val_loss: 0.1046 - val_f1_score_mod: 0.2723 - val_recall_mod: 0.1682 - val_precision_mod: 0.7182 - val_dur_error: 0.2929 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10459\n",
      "25256/25256 - 133s - loss: 0.1125 - f1_score_mod: 0.2735 - recall_mod: 0.1687 - precision_mod: 0.7285 - dur_error: 0.4072 - maestro_dur_loss: 0.0204 - val_loss: 0.1067 - val_f1_score_mod: 0.2824 - val_recall_mod: 0.1766 - val_precision_mod: 0.7132 - val_dur_error: 0.3472 - val_maestro_dur_loss: 0.0174\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10459\n",
      "25256/25256 - 133s - loss: 0.1120 - f1_score_mod: 0.2796 - recall_mod: 0.1734 - precision_mod: 0.7277 - dur_error: 0.4043 - maestro_dur_loss: 0.0202 - val_loss: 0.1061 - val_f1_score_mod: 0.2727 - val_recall_mod: 0.1685 - val_precision_mod: 0.7272 - val_dur_error: 0.3428 - val_maestro_dur_loss: 0.0171\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10459 to 0.10353, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1113 - f1_score_mod: 0.2818 - recall_mod: 0.1755 - precision_mod: 0.7235 - dur_error: 0.4005 - maestro_dur_loss: 0.0200 - val_loss: 0.1035 - val_f1_score_mod: 0.2639 - val_recall_mod: 0.1605 - val_precision_mod: 0.7491 - val_dur_error: 0.2885 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.10353 to 0.10229, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1111 - f1_score_mod: 0.2863 - recall_mod: 0.1787 - precision_mod: 0.7255 - dur_error: 0.4045 - maestro_dur_loss: 0.0202 - val_loss: 0.1023 - val_f1_score_mod: 0.2822 - val_recall_mod: 0.1745 - val_precision_mod: 0.7447 - val_dur_error: 0.2779 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10229\n",
      "25256/25256 - 134s - loss: 0.1102 - f1_score_mod: 0.2948 - recall_mod: 0.1849 - precision_mod: 0.7340 - dur_error: 0.3970 - maestro_dur_loss: 0.0199 - val_loss: 0.1043 - val_f1_score_mod: 0.2938 - val_recall_mod: 0.1850 - val_precision_mod: 0.7221 - val_dur_error: 0.3164 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10229\n",
      "25256/25256 - 133s - loss: 0.1098 - f1_score_mod: 0.2983 - recall_mod: 0.1874 - precision_mod: 0.7336 - dur_error: 0.3996 - maestro_dur_loss: 0.0200 - val_loss: 0.1025 - val_f1_score_mod: 0.3036 - val_recall_mod: 0.1937 - val_precision_mod: 0.7088 - val_dur_error: 0.2879 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.10229\n",
      "25256/25256 - 134s - loss: 0.1089 - f1_score_mod: 0.3017 - recall_mod: 0.1904 - precision_mod: 0.7308 - dur_error: 0.3908 - maestro_dur_loss: 0.0195 - val_loss: 0.1032 - val_f1_score_mod: 0.2926 - val_recall_mod: 0.1824 - val_precision_mod: 0.7448 - val_dur_error: 0.3117 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.10229 to 0.10149, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1087 - f1_score_mod: 0.3055 - recall_mod: 0.1933 - precision_mod: 0.7335 - dur_error: 0.3931 - maestro_dur_loss: 0.0197 - val_loss: 0.1015 - val_f1_score_mod: 0.3074 - val_recall_mod: 0.1944 - val_precision_mod: 0.7389 - val_dur_error: 0.2841 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10149\n",
      "25256/25256 - 134s - loss: 0.1081 - f1_score_mod: 0.3112 - recall_mod: 0.1976 - precision_mod: 0.7381 - dur_error: 0.3931 - maestro_dur_loss: 0.0197 - val_loss: 0.1022 - val_f1_score_mod: 0.3009 - val_recall_mod: 0.1904 - val_precision_mod: 0.7255 - val_dur_error: 0.2968 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10149\n",
      "25256/25256 - 135s - loss: 0.1075 - f1_score_mod: 0.3161 - recall_mod: 0.2017 - precision_mod: 0.7358 - dur_error: 0.3901 - maestro_dur_loss: 0.0195 - val_loss: 0.1016 - val_f1_score_mod: 0.3167 - val_recall_mod: 0.2038 - val_precision_mod: 0.7137 - val_dur_error: 0.2974 - val_maestro_dur_loss: 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.10149 to 0.10030, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1068 - f1_score_mod: 0.3245 - recall_mod: 0.2085 - precision_mod: 0.7350 - dur_error: 0.3874 - maestro_dur_loss: 0.0194 - val_loss: 0.1003 - val_f1_score_mod: 0.3145 - val_recall_mod: 0.2009 - val_precision_mod: 0.7321 - val_dur_error: 0.2791 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.10030 to 0.10011, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1066 - f1_score_mod: 0.3284 - recall_mod: 0.2114 - precision_mod: 0.7412 - dur_error: 0.3897 - maestro_dur_loss: 0.0195 - val_loss: 0.1001 - val_f1_score_mod: 0.3302 - val_recall_mod: 0.2159 - val_precision_mod: 0.7064 - val_dur_error: 0.2736 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.10011\n",
      "25256/25256 - 134s - loss: 0.1059 - f1_score_mod: 0.3319 - recall_mod: 0.2149 - precision_mod: 0.7338 - dur_error: 0.3875 - maestro_dur_loss: 0.0194 - val_loss: 0.1005 - val_f1_score_mod: 0.3169 - val_recall_mod: 0.2020 - val_precision_mod: 0.7400 - val_dur_error: 0.2842 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.10011 to 0.09934, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1053 - f1_score_mod: 0.3336 - recall_mod: 0.2158 - precision_mod: 0.7400 - dur_error: 0.3836 - maestro_dur_loss: 0.0192 - val_loss: 0.0993 - val_f1_score_mod: 0.3276 - val_recall_mod: 0.2106 - val_precision_mod: 0.7423 - val_dur_error: 0.2723 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09934\n",
      "25256/25256 - 134s - loss: 0.1047 - f1_score_mod: 0.3396 - recall_mod: 0.2208 - precision_mod: 0.7405 - dur_error: 0.3830 - maestro_dur_loss: 0.0192 - val_loss: 0.0999 - val_f1_score_mod: 0.3216 - val_recall_mod: 0.2050 - val_precision_mod: 0.7499 - val_dur_error: 0.2877 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09934\n",
      "25256/25256 - 134s - loss: 0.1042 - f1_score_mod: 0.3434 - recall_mod: 0.2238 - precision_mod: 0.7420 - dur_error: 0.3794 - maestro_dur_loss: 0.0190 - val_loss: 0.1005 - val_f1_score_mod: 0.3547 - val_recall_mod: 0.2409 - val_precision_mod: 0.6793 - val_dur_error: 0.2925 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09934 to 0.09885, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1038 - f1_score_mod: 0.3499 - recall_mod: 0.2296 - precision_mod: 0.7402 - dur_error: 0.3830 - maestro_dur_loss: 0.0191 - val_loss: 0.0988 - val_f1_score_mod: 0.3507 - val_recall_mod: 0.2338 - val_precision_mod: 0.7042 - val_dur_error: 0.2732 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09885 to 0.09738, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1032 - f1_score_mod: 0.3547 - recall_mod: 0.2336 - precision_mod: 0.7403 - dur_error: 0.3769 - maestro_dur_loss: 0.0188 - val_loss: 0.0974 - val_f1_score_mod: 0.3373 - val_recall_mod: 0.2196 - val_precision_mod: 0.7334 - val_dur_error: 0.2545 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09738\n",
      "25256/25256 - 133s - loss: 0.1025 - f1_score_mod: 0.3582 - recall_mod: 0.2361 - precision_mod: 0.7457 - dur_error: 0.3755 - maestro_dur_loss: 0.0188 - val_loss: 0.1048 - val_f1_score_mod: 0.3585 - val_recall_mod: 0.2436 - val_precision_mod: 0.6823 - val_dur_error: 0.3939 - val_maestro_dur_loss: 0.0197\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09738\n",
      "25256/25256 - 138s - loss: 0.1021 - f1_score_mod: 0.3634 - recall_mod: 0.2408 - precision_mod: 0.7435 - dur_error: 0.3746 - maestro_dur_loss: 0.0187 - val_loss: 0.1040 - val_f1_score_mod: 0.3523 - val_recall_mod: 0.2340 - val_precision_mod: 0.7161 - val_dur_error: 0.3913 - val_maestro_dur_loss: 0.0196\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09738 to 0.09695, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1013 - f1_score_mod: 0.3665 - recall_mod: 0.2433 - precision_mod: 0.7458 - dur_error: 0.3721 - maestro_dur_loss: 0.0186 - val_loss: 0.0970 - val_f1_score_mod: 0.3454 - val_recall_mod: 0.2258 - val_precision_mod: 0.7390 - val_dur_error: 0.2558 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09695\n",
      "25256/25256 - 133s - loss: 0.1010 - f1_score_mod: 0.3741 - recall_mod: 0.2497 - precision_mod: 0.7489 - dur_error: 0.3717 - maestro_dur_loss: 0.0186 - val_loss: 0.0977 - val_f1_score_mod: 0.3563 - val_recall_mod: 0.2373 - val_precision_mod: 0.7178 - val_dur_error: 0.2710 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09695 to 0.09651, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1005 - f1_score_mod: 0.3769 - recall_mod: 0.2522 - precision_mod: 0.7487 - dur_error: 0.3721 - maestro_dur_loss: 0.0186 - val_loss: 0.0965 - val_f1_score_mod: 0.3625 - val_recall_mod: 0.2423 - val_precision_mod: 0.7252 - val_dur_error: 0.2596 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09651\n",
      "25256/25256 - 133s - loss: 0.1000 - f1_score_mod: 0.3808 - recall_mod: 0.2556 - precision_mod: 0.7493 - dur_error: 0.3700 - maestro_dur_loss: 0.0185 - val_loss: 0.0970 - val_f1_score_mod: 0.3623 - val_recall_mod: 0.2444 - val_precision_mod: 0.7025 - val_dur_error: 0.2704 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09651\n",
      "25256/25256 - 133s - loss: 0.0994 - f1_score_mod: 0.3838 - recall_mod: 0.2588 - precision_mod: 0.7460 - dur_error: 0.3676 - maestro_dur_loss: 0.0184 - val_loss: 0.0988 - val_f1_score_mod: 0.3773 - val_recall_mod: 0.2602 - val_precision_mod: 0.6889 - val_dur_error: 0.3102 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09651 to 0.09643, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 132s - loss: 0.0988 - f1_score_mod: 0.3850 - recall_mod: 0.2601 - precision_mod: 0.7453 - dur_error: 0.3653 - maestro_dur_loss: 0.0183 - val_loss: 0.0964 - val_f1_score_mod: 0.3709 - val_recall_mod: 0.2519 - val_precision_mod: 0.7060 - val_dur_error: 0.2634 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.09643\n",
      "25256/25256 - 132s - loss: 0.0985 - f1_score_mod: 0.3917 - recall_mod: 0.2653 - precision_mod: 0.7521 - dur_error: 0.3664 - maestro_dur_loss: 0.0183 - val_loss: 0.0977 - val_f1_score_mod: 0.3715 - val_recall_mod: 0.2502 - val_precision_mod: 0.7242 - val_dur_error: 0.2924 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09643 to 0.09570, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0977 - f1_score_mod: 0.3956 - recall_mod: 0.2693 - precision_mod: 0.7494 - dur_error: 0.3627 - maestro_dur_loss: 0.0181 - val_loss: 0.0957 - val_f1_score_mod: 0.3605 - val_recall_mod: 0.2388 - val_precision_mod: 0.7407 - val_dur_error: 0.2556 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09570\n",
      "25256/25256 - 133s - loss: 0.0975 - f1_score_mod: 0.4009 - recall_mod: 0.2737 - precision_mod: 0.7534 - dur_error: 0.3648 - maestro_dur_loss: 0.0182 - val_loss: 0.0960 - val_f1_score_mod: 0.3822 - val_recall_mod: 0.2615 - val_precision_mod: 0.7113 - val_dur_error: 0.2665 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09570 to 0.09529, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0969 - f1_score_mod: 0.4036 - recall_mod: 0.2768 - precision_mod: 0.7474 - dur_error: 0.3630 - maestro_dur_loss: 0.0182 - val_loss: 0.0953 - val_f1_score_mod: 0.3809 - val_recall_mod: 0.2595 - val_precision_mod: 0.7195 - val_dur_error: 0.2567 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 62/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09529\n",
      "25256/25256 - 133s - loss: 0.0960 - f1_score_mod: 0.4081 - recall_mod: 0.2797 - precision_mod: 0.7566 - dur_error: 0.3591 - maestro_dur_loss: 0.0180 - val_loss: 0.0983 - val_f1_score_mod: 0.3937 - val_recall_mod: 0.2750 - val_precision_mod: 0.6949 - val_dur_error: 0.3245 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.09529\n",
      "25256/25256 - 133s - loss: 0.0957 - f1_score_mod: 0.4127 - recall_mod: 0.2849 - precision_mod: 0.7523 - dur_error: 0.3592 - maestro_dur_loss: 0.0180 - val_loss: 0.0954 - val_f1_score_mod: 0.3871 - val_recall_mod: 0.2672 - val_precision_mod: 0.7060 - val_dur_error: 0.2600 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.09529\n",
      "25256/25256 - 135s - loss: 0.0951 - f1_score_mod: 0.4176 - recall_mod: 0.2894 - precision_mod: 0.7520 - dur_error: 0.3575 - maestro_dur_loss: 0.0179 - val_loss: 0.0986 - val_f1_score_mod: 0.3871 - val_recall_mod: 0.2676 - val_precision_mod: 0.7009 - val_dur_error: 0.3298 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.09529\n",
      "25256/25256 - 136s - loss: 0.0946 - f1_score_mod: 0.4229 - recall_mod: 0.2946 - precision_mod: 0.7504 - dur_error: 0.3573 - maestro_dur_loss: 0.0179 - val_loss: 0.0995 - val_f1_score_mod: 0.3853 - val_recall_mod: 0.2673 - val_precision_mod: 0.6924 - val_dur_error: 0.3506 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09529\n",
      "25256/25256 - 134s - loss: 0.0942 - f1_score_mod: 0.4238 - recall_mod: 0.2956 - precision_mod: 0.7508 - dur_error: 0.3559 - maestro_dur_loss: 0.0178 - val_loss: 0.0976 - val_f1_score_mod: 0.3856 - val_recall_mod: 0.2643 - val_precision_mod: 0.7164 - val_dur_error: 0.3204 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09529 to 0.09457, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0936 - f1_score_mod: 0.4287 - recall_mod: 0.3000 - precision_mod: 0.7527 - dur_error: 0.3553 - maestro_dur_loss: 0.0178 - val_loss: 0.0946 - val_f1_score_mod: 0.3884 - val_recall_mod: 0.2686 - val_precision_mod: 0.7044 - val_dur_error: 0.2577 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09457\n",
      "25256/25256 - 135s - loss: 0.0933 - f1_score_mod: 0.4318 - recall_mod: 0.3035 - precision_mod: 0.7503 - dur_error: 0.3570 - maestro_dur_loss: 0.0179 - val_loss: 0.0967 - val_f1_score_mod: 0.4045 - val_recall_mod: 0.2844 - val_precision_mod: 0.7018 - val_dur_error: 0.3124 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09457 to 0.09394, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0925 - f1_score_mod: 0.4366 - recall_mod: 0.3083 - precision_mod: 0.7504 - dur_error: 0.3528 - maestro_dur_loss: 0.0176 - val_loss: 0.0939 - val_f1_score_mod: 0.4079 - val_recall_mod: 0.2892 - val_precision_mod: 0.6948 - val_dur_error: 0.2543 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09394\n",
      "25256/25256 - 133s - loss: 0.0918 - f1_score_mod: 0.4460 - recall_mod: 0.3168 - precision_mod: 0.7556 - dur_error: 0.3546 - maestro_dur_loss: 0.0177 - val_loss: 0.0944 - val_f1_score_mod: 0.4028 - val_recall_mod: 0.2830 - val_precision_mod: 0.7014 - val_dur_error: 0.2613 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09394\n",
      "25256/25256 - 133s - loss: 0.0913 - f1_score_mod: 0.4476 - recall_mod: 0.3183 - precision_mod: 0.7557 - dur_error: 0.3492 - maestro_dur_loss: 0.0175 - val_loss: 0.0970 - val_f1_score_mod: 0.4150 - val_recall_mod: 0.2957 - val_precision_mod: 0.7002 - val_dur_error: 0.3127 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09394\n",
      "25256/25256 - 133s - loss: 0.0908 - f1_score_mod: 0.4537 - recall_mod: 0.3242 - precision_mod: 0.7570 - dur_error: 0.3489 - maestro_dur_loss: 0.0174 - val_loss: 0.0943 - val_f1_score_mod: 0.4142 - val_recall_mod: 0.2964 - val_precision_mod: 0.6907 - val_dur_error: 0.2604 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09394\n",
      "25256/25256 - 133s - loss: 0.0902 - f1_score_mod: 0.4567 - recall_mod: 0.3277 - precision_mod: 0.7550 - dur_error: 0.3496 - maestro_dur_loss: 0.0175 - val_loss: 0.0943 - val_f1_score_mod: 0.4116 - val_recall_mod: 0.2949 - val_precision_mod: 0.6827 - val_dur_error: 0.2653 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.09394\n",
      "25256/25256 - 133s - loss: 0.0895 - f1_score_mod: 0.4625 - recall_mod: 0.3339 - precision_mod: 0.7540 - dur_error: 0.3468 - maestro_dur_loss: 0.0173 - val_loss: 0.0960 - val_f1_score_mod: 0.4231 - val_recall_mod: 0.3091 - val_precision_mod: 0.6728 - val_dur_error: 0.2935 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09394 to 0.09324, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0893 - f1_score_mod: 0.4673 - recall_mod: 0.3385 - precision_mod: 0.7565 - dur_error: 0.3493 - maestro_dur_loss: 0.0175 - val_loss: 0.0932 - val_f1_score_mod: 0.4238 - val_recall_mod: 0.3080 - val_precision_mod: 0.6808 - val_dur_error: 0.2463 - val_maestro_dur_loss: 0.0123\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09324\n",
      "25256/25256 - 134s - loss: 0.0884 - f1_score_mod: 0.4696 - recall_mod: 0.3415 - precision_mod: 0.7540 - dur_error: 0.3434 - maestro_dur_loss: 0.0172 - val_loss: 0.0981 - val_f1_score_mod: 0.4226 - val_recall_mod: 0.3135 - val_precision_mod: 0.6500 - val_dur_error: 0.3439 - val_maestro_dur_loss: 0.0172\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.09324\n",
      "25256/25256 - 134s - loss: 0.0880 - f1_score_mod: 0.4737 - recall_mod: 0.3453 - precision_mod: 0.7555 - dur_error: 0.3448 - maestro_dur_loss: 0.0172 - val_loss: 0.0976 - val_f1_score_mod: 0.4222 - val_recall_mod: 0.3053 - val_precision_mod: 0.6859 - val_dur_error: 0.3333 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09324\n",
      "25256/25256 - 133s - loss: 0.0874 - f1_score_mod: 0.4782 - recall_mod: 0.3499 - precision_mod: 0.7568 - dur_error: 0.3434 - maestro_dur_loss: 0.0172 - val_loss: 0.0949 - val_f1_score_mod: 0.4227 - val_recall_mod: 0.3097 - val_precision_mod: 0.6693 - val_dur_error: 0.2816 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.09324\n",
      "25256/25256 - 133s - loss: 0.0867 - f1_score_mod: 0.4843 - recall_mod: 0.3561 - precision_mod: 0.7582 - dur_error: 0.3430 - maestro_dur_loss: 0.0172 - val_loss: 0.0949 - val_f1_score_mod: 0.4254 - val_recall_mod: 0.3115 - val_precision_mod: 0.6717 - val_dur_error: 0.2707 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09324 to 0.09275, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0860 - f1_score_mod: 0.4882 - recall_mod: 0.3602 - precision_mod: 0.7587 - dur_error: 0.3400 - maestro_dur_loss: 0.0170 - val_loss: 0.0927 - val_f1_score_mod: 0.4319 - val_recall_mod: 0.3165 - val_precision_mod: 0.6823 - val_dur_error: 0.2444 - val_maestro_dur_loss: 0.0122\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09275\n",
      "25256/25256 - 133s - loss: 0.0854 - f1_score_mod: 0.4933 - recall_mod: 0.3663 - precision_mod: 0.7566 - dur_error: 0.3400 - maestro_dur_loss: 0.0170 - val_loss: 0.0948 - val_f1_score_mod: 0.4379 - val_recall_mod: 0.3218 - val_precision_mod: 0.6871 - val_dur_error: 0.2839 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.09275 to 0.09242, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 141s - loss: 0.0849 - f1_score_mod: 0.4989 - recall_mod: 0.3713 - precision_mod: 0.7615 - dur_error: 0.3368 - maestro_dur_loss: 0.0168 - val_loss: 0.0924 - val_f1_score_mod: 0.4312 - val_recall_mod: 0.3167 - val_precision_mod: 0.6764 - val_dur_error: 0.2445 - val_maestro_dur_loss: 0.0122\n",
      "Epoch 83/150\n",
      "Batch 44: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23040/25256 - 107s - loss: nan - f1_score_mod: 0.5022 - recall_mod: 0.3764 - precision_mod: 0.7558 - dur_error: 0.3390 - maestro_dur_loss: 0.0170\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This produced the best model yet, but barely. As a final tuning step, I will vary the dropout_rate because I have a hunch that reducing dropout_rate may make the training more stable and perhaps delay model failure by NaN loss. For a visual comparison of performance with models using each setting on its own, see Figures 3 and 4 from visualize_performance.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25256 samples, validate on 10824 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14012, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 138s - loss: 0.2305 - f1_score_mod: 0.0192 - recall_mod: 0.0292 - precision_mod: 0.0944 - dur_error: 0.9799 - maestro_dur_loss: 0.0490 - val_loss: 0.1401 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4328 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14012\n",
      "25256/25256 - 133s - loss: 0.1631 - f1_score_mod: 0.0032 - recall_mod: 0.0016 - precision_mod: 0.2294 - dur_error: 0.6816 - maestro_dur_loss: 0.0341 - val_loss: 0.1433 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5495 - val_maestro_dur_loss: 0.0275\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14012\n",
      "25256/25256 - 133s - loss: 0.1528 - f1_score_mod: 0.0096 - recall_mod: 0.0049 - precision_mod: 0.3964 - dur_error: 0.6169 - maestro_dur_loss: 0.0308 - val_loss: 0.1428 - val_f1_score_mod: 0.0177 - val_recall_mod: 0.0090 - val_precision_mod: 0.6230 - val_dur_error: 0.6003 - val_maestro_dur_loss: 0.0300\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14012 to 0.13447, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1463 - f1_score_mod: 0.0274 - recall_mod: 0.0142 - precision_mod: 0.5224 - dur_error: 0.5769 - maestro_dur_loss: 0.0288 - val_loss: 0.1345 - val_f1_score_mod: 0.0450 - val_recall_mod: 0.0235 - val_precision_mod: 0.5733 - val_dur_error: 0.5084 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13447 to 0.13043, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1416 - f1_score_mod: 0.0543 - recall_mod: 0.0287 - precision_mod: 0.6030 - dur_error: 0.5565 - maestro_dur_loss: 0.0278 - val_loss: 0.1304 - val_f1_score_mod: 0.0635 - val_recall_mod: 0.0334 - val_precision_mod: 0.6872 - val_dur_error: 0.4892 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13043 to 0.12907, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1378 - f1_score_mod: 0.0776 - recall_mod: 0.0415 - precision_mod: 0.6260 - dur_error: 0.5370 - maestro_dur_loss: 0.0268 - val_loss: 0.1291 - val_f1_score_mod: 0.1031 - val_recall_mod: 0.0563 - val_precision_mod: 0.6221 - val_dur_error: 0.4867 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12907 to 0.12744, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 137s - loss: 0.1352 - f1_score_mod: 0.0963 - recall_mod: 0.0523 - precision_mod: 0.6388 - dur_error: 0.5288 - maestro_dur_loss: 0.0264 - val_loss: 0.1274 - val_f1_score_mod: 0.0953 - val_recall_mod: 0.0512 - val_precision_mod: 0.7212 - val_dur_error: 0.4984 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12744 to 0.11983, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1327 - f1_score_mod: 0.1125 - recall_mod: 0.0620 - precision_mod: 0.6332 - dur_error: 0.5134 - maestro_dur_loss: 0.0257 - val_loss: 0.1198 - val_f1_score_mod: 0.1335 - val_recall_mod: 0.0745 - val_precision_mod: 0.6618 - val_dur_error: 0.3810 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11983 to 0.11795, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1309 - f1_score_mod: 0.1318 - recall_mod: 0.0735 - precision_mod: 0.6603 - dur_error: 0.5089 - maestro_dur_loss: 0.0254 - val_loss: 0.1180 - val_f1_score_mod: 0.1265 - val_recall_mod: 0.0693 - val_precision_mod: 0.7385 - val_dur_error: 0.3667 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11795 to 0.11723, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 136s - loss: 0.1339 - f1_score_mod: 0.1377 - recall_mod: 0.0772 - precision_mod: 0.6621 - dur_error: 0.5750 - maestro_dur_loss: 0.0287 - val_loss: 0.1172 - val_f1_score_mod: 0.1266 - val_recall_mod: 0.0695 - val_precision_mod: 0.7262 - val_dur_error: 0.3614 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11723\n",
      "25256/25256 - 136s - loss: 0.1277 - f1_score_mod: 0.1543 - recall_mod: 0.0874 - precision_mod: 0.6677 - dur_error: 0.4917 - maestro_dur_loss: 0.0246 - val_loss: 0.1199 - val_f1_score_mod: 0.1356 - val_recall_mod: 0.0745 - val_precision_mod: 0.7670 - val_dur_error: 0.4205 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11723\n",
      "25256/25256 - 137s - loss: 0.1298 - f1_score_mod: 0.1613 - recall_mod: 0.0917 - precision_mod: 0.6838 - dur_error: 0.5376 - maestro_dur_loss: 0.0269 - val_loss: 0.1233 - val_f1_score_mod: 0.1429 - val_recall_mod: 0.0793 - val_precision_mod: 0.7554 - val_dur_error: 0.4948 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11723\n",
      "25256/25256 - 135s - loss: 0.1300 - f1_score_mod: 0.1549 - recall_mod: 0.0878 - precision_mod: 0.6760 - dur_error: 0.5127 - maestro_dur_loss: 0.0256 - val_loss: 0.1209 - val_f1_score_mod: 0.1536 - val_recall_mod: 0.0863 - val_precision_mod: 0.7124 - val_dur_error: 0.4281 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11723 to 0.11582, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1277 - f1_score_mod: 0.1600 - recall_mod: 0.0910 - precision_mod: 0.6787 - dur_error: 0.4911 - maestro_dur_loss: 0.0246 - val_loss: 0.1158 - val_f1_score_mod: 0.1521 - val_recall_mod: 0.0853 - val_precision_mod: 0.7207 - val_dur_error: 0.3552 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11582\n",
      "25256/25256 - 134s - loss: 0.1255 - f1_score_mod: 0.1641 - recall_mod: 0.0934 - precision_mod: 0.6921 - dur_error: 0.4733 - maestro_dur_loss: 0.0237 - val_loss: 0.1183 - val_f1_score_mod: 0.1804 - val_recall_mod: 0.1037 - val_precision_mod: 0.6989 - val_dur_error: 0.4277 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11582\n",
      "25256/25256 - 133s - loss: 0.1242 - f1_score_mod: 0.1756 - recall_mod: 0.1010 - precision_mod: 0.6877 - dur_error: 0.4643 - maestro_dur_loss: 0.0232 - val_loss: 0.1179 - val_f1_score_mod: 0.1796 - val_recall_mod: 0.1030 - val_precision_mod: 0.7093 - val_dur_error: 0.4190 - val_maestro_dur_loss: 0.0209\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11582 to 0.11327, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1222 - f1_score_mod: 0.1819 - recall_mod: 0.1050 - precision_mod: 0.6889 - dur_error: 0.4439 - maestro_dur_loss: 0.0222 - val_loss: 0.1133 - val_f1_score_mod: 0.2008 - val_recall_mod: 0.1175 - val_precision_mod: 0.6991 - val_dur_error: 0.3524 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11327\n",
      "25256/25256 - 135s - loss: 0.1213 - f1_score_mod: 0.1921 - recall_mod: 0.1118 - precision_mod: 0.6940 - dur_error: 0.4367 - maestro_dur_loss: 0.0218 - val_loss: 0.1158 - val_f1_score_mod: 0.1848 - val_recall_mod: 0.1062 - val_precision_mod: 0.7189 - val_dur_error: 0.4105 - val_maestro_dur_loss: 0.0205\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11327 to 0.11153, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 135s - loss: 0.1207 - f1_score_mod: 0.1994 - recall_mod: 0.1166 - precision_mod: 0.6993 - dur_error: 0.4355 - maestro_dur_loss: 0.0218 - val_loss: 0.1115 - val_f1_score_mod: 0.1722 - val_recall_mod: 0.0974 - val_precision_mod: 0.7445 - val_dur_error: 0.3206 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11153\n",
      "25256/25256 - 133s - loss: 0.1195 - f1_score_mod: 0.2055 - recall_mod: 0.1205 - precision_mod: 0.7069 - dur_error: 0.4254 - maestro_dur_loss: 0.0213 - val_loss: 0.1208 - val_f1_score_mod: 0.2121 - val_recall_mod: 0.1258 - val_precision_mod: 0.6797 - val_dur_error: 0.5126 - val_maestro_dur_loss: 0.0256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11153\n",
      "25256/25256 - 133s - loss: 0.1188 - f1_score_mod: 0.2153 - recall_mod: 0.1273 - precision_mod: 0.7082 - dur_error: 0.4286 - maestro_dur_loss: 0.0214 - val_loss: 0.1162 - val_f1_score_mod: 0.2012 - val_recall_mod: 0.1172 - val_precision_mod: 0.7150 - val_dur_error: 0.4416 - val_maestro_dur_loss: 0.0221\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11153 to 0.11119, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1180 - f1_score_mod: 0.2199 - recall_mod: 0.1306 - precision_mod: 0.7061 - dur_error: 0.4255 - maestro_dur_loss: 0.0213 - val_loss: 0.1112 - val_f1_score_mod: 0.2259 - val_recall_mod: 0.1353 - val_precision_mod: 0.6979 - val_dur_error: 0.3553 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.11119\n",
      "25256/25256 - 134s - loss: 0.1168 - f1_score_mod: 0.2304 - recall_mod: 0.1376 - precision_mod: 0.7169 - dur_error: 0.4162 - maestro_dur_loss: 0.0208 - val_loss: 0.1125 - val_f1_score_mod: 0.2414 - val_recall_mod: 0.1470 - val_precision_mod: 0.6848 - val_dur_error: 0.3702 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11119\n",
      "25256/25256 - 134s - loss: 0.1158 - f1_score_mod: 0.2353 - recall_mod: 0.1412 - precision_mod: 0.7207 - dur_error: 0.4111 - maestro_dur_loss: 0.0206 - val_loss: 0.1125 - val_f1_score_mod: 0.2214 - val_recall_mod: 0.1306 - val_precision_mod: 0.7296 - val_dur_error: 0.3891 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.11119 to 0.10679, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1151 - f1_score_mod: 0.2436 - recall_mod: 0.1468 - precision_mod: 0.7220 - dur_error: 0.4071 - maestro_dur_loss: 0.0204 - val_loss: 0.1068 - val_f1_score_mod: 0.2380 - val_recall_mod: 0.1435 - val_precision_mod: 0.7061 - val_dur_error: 0.2916 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10679 to 0.10665, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1145 - f1_score_mod: 0.2485 - recall_mod: 0.1506 - precision_mod: 0.7181 - dur_error: 0.4047 - maestro_dur_loss: 0.0202 - val_loss: 0.1067 - val_f1_score_mod: 0.2392 - val_recall_mod: 0.1433 - val_precision_mod: 0.7332 - val_dur_error: 0.2980 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10665\n",
      "25256/25256 - 133s - loss: 0.1131 - f1_score_mod: 0.2567 - recall_mod: 0.1561 - precision_mod: 0.7266 - dur_error: 0.3967 - maestro_dur_loss: 0.0198 - val_loss: 0.1091 - val_f1_score_mod: 0.2525 - val_recall_mod: 0.1538 - val_precision_mod: 0.7098 - val_dur_error: 0.3468 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10665 to 0.10649, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1127 - f1_score_mod: 0.2621 - recall_mod: 0.1603 - precision_mod: 0.7269 - dur_error: 0.3971 - maestro_dur_loss: 0.0199 - val_loss: 0.1065 - val_f1_score_mod: 0.2570 - val_recall_mod: 0.1563 - val_precision_mod: 0.7303 - val_dur_error: 0.3157 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10649\n",
      "25256/25256 - 133s - loss: 0.1119 - f1_score_mod: 0.2709 - recall_mod: 0.1664 - precision_mod: 0.7343 - dur_error: 0.3940 - maestro_dur_loss: 0.0197 - val_loss: 0.1091 - val_f1_score_mod: 0.2695 - val_recall_mod: 0.1680 - val_precision_mod: 0.6860 - val_dur_error: 0.3651 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10649\n",
      "25256/25256 - 133s - loss: 0.1114 - f1_score_mod: 0.2770 - recall_mod: 0.1713 - precision_mod: 0.7296 - dur_error: 0.3942 - maestro_dur_loss: 0.0197 - val_loss: 0.1073 - val_f1_score_mod: 0.2623 - val_recall_mod: 0.1603 - val_precision_mod: 0.7288 - val_dur_error: 0.3436 - val_maestro_dur_loss: 0.0172\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10649 to 0.10365, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 137s - loss: 0.1104 - f1_score_mod: 0.2808 - recall_mod: 0.1738 - precision_mod: 0.7344 - dur_error: 0.3866 - maestro_dur_loss: 0.0193 - val_loss: 0.1037 - val_f1_score_mod: 0.2770 - val_recall_mod: 0.1714 - val_precision_mod: 0.7241 - val_dur_error: 0.2803 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10365\n",
      "25256/25256 - 133s - loss: 0.1095 - f1_score_mod: 0.2891 - recall_mod: 0.1802 - precision_mod: 0.7350 - dur_error: 0.3823 - maestro_dur_loss: 0.0191 - val_loss: 0.1042 - val_f1_score_mod: 0.2871 - val_recall_mod: 0.1818 - val_precision_mod: 0.6887 - val_dur_error: 0.2877 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10365\n",
      "25256/25256 - 134s - loss: 0.1090 - f1_score_mod: 0.2976 - recall_mod: 0.1865 - precision_mod: 0.7415 - dur_error: 0.3805 - maestro_dur_loss: 0.0190 - val_loss: 0.1081 - val_f1_score_mod: 0.2837 - val_recall_mod: 0.1775 - val_precision_mod: 0.7172 - val_dur_error: 0.3863 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10365 to 0.10340, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1083 - f1_score_mod: 0.3009 - recall_mod: 0.1894 - precision_mod: 0.7353 - dur_error: 0.3796 - maestro_dur_loss: 0.0190 - val_loss: 0.1034 - val_f1_score_mod: 0.2836 - val_recall_mod: 0.1765 - val_precision_mod: 0.7264 - val_dur_error: 0.2995 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10340\n",
      "25256/25256 - 133s - loss: 0.1075 - f1_score_mod: 0.3055 - recall_mod: 0.1929 - precision_mod: 0.7387 - dur_error: 0.3762 - maestro_dur_loss: 0.0188 - val_loss: 0.1041 - val_f1_score_mod: 0.2883 - val_recall_mod: 0.1805 - val_precision_mod: 0.7187 - val_dur_error: 0.3137 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.10340 to 0.10199, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.1067 - f1_score_mod: 0.3151 - recall_mod: 0.2003 - precision_mod: 0.7431 - dur_error: 0.3758 - maestro_dur_loss: 0.0188 - val_loss: 0.1020 - val_f1_score_mod: 0.3189 - val_recall_mod: 0.2077 - val_precision_mod: 0.6922 - val_dur_error: 0.2763 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10199\n",
      "25256/25256 - 134s - loss: 0.1061 - f1_score_mod: 0.3182 - recall_mod: 0.2029 - precision_mod: 0.7419 - dur_error: 0.3744 - maestro_dur_loss: 0.0187 - val_loss: 0.1028 - val_f1_score_mod: 0.2982 - val_recall_mod: 0.1871 - val_precision_mod: 0.7401 - val_dur_error: 0.3016 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10199 to 0.10055, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1054 - f1_score_mod: 0.3275 - recall_mod: 0.2102 - precision_mod: 0.7460 - dur_error: 0.3697 - maestro_dur_loss: 0.0185 - val_loss: 0.1006 - val_f1_score_mod: 0.3195 - val_recall_mod: 0.2054 - val_precision_mod: 0.7256 - val_dur_error: 0.2710 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.10055\n",
      "25256/25256 - 134s - loss: 0.1047 - f1_score_mod: 0.3341 - recall_mod: 0.2158 - precision_mod: 0.7457 - dur_error: 0.3664 - maestro_dur_loss: 0.0183 - val_loss: 0.1017 - val_f1_score_mod: 0.3198 - val_recall_mod: 0.2058 - val_precision_mod: 0.7239 - val_dur_error: 0.2966 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10055\n",
      "25256/25256 - 133s - loss: 0.1042 - f1_score_mod: 0.3394 - recall_mod: 0.2202 - precision_mod: 0.7467 - dur_error: 0.3666 - maestro_dur_loss: 0.0183 - val_loss: 0.1059 - val_f1_score_mod: 0.3244 - val_recall_mod: 0.2108 - val_precision_mod: 0.7079 - val_dur_error: 0.3909 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10055\n",
      "25256/25256 - 133s - loss: 0.1033 - f1_score_mod: 0.3441 - recall_mod: 0.2234 - precision_mod: 0.7534 - dur_error: 0.3640 - maestro_dur_loss: 0.0182 - val_loss: 0.1045 - val_f1_score_mod: 0.3413 - val_recall_mod: 0.2277 - val_precision_mod: 0.6849 - val_dur_error: 0.3530 - val_maestro_dur_loss: 0.0176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.10055\n",
      "25256/25256 - 134s - loss: 0.1029 - f1_score_mod: 0.3504 - recall_mod: 0.2293 - precision_mod: 0.7460 - dur_error: 0.3647 - maestro_dur_loss: 0.0182 - val_loss: 0.1068 - val_f1_score_mod: 0.3404 - val_recall_mod: 0.2249 - val_precision_mod: 0.7053 - val_dur_error: 0.4112 - val_maestro_dur_loss: 0.0206\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.10055 to 0.09943, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1021 - f1_score_mod: 0.3566 - recall_mod: 0.2339 - precision_mod: 0.7529 - dur_error: 0.3623 - maestro_dur_loss: 0.0181 - val_loss: 0.0994 - val_f1_score_mod: 0.3451 - val_recall_mod: 0.2275 - val_precision_mod: 0.7196 - val_dur_error: 0.2749 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09943\n",
      "25256/25256 - 134s - loss: 0.1014 - f1_score_mod: 0.3605 - recall_mod: 0.2374 - precision_mod: 0.7512 - dur_error: 0.3587 - maestro_dur_loss: 0.0179 - val_loss: 0.1026 - val_f1_score_mod: 0.3499 - val_recall_mod: 0.2338 - val_precision_mod: 0.6985 - val_dur_error: 0.3478 - val_maestro_dur_loss: 0.0174\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09943\n",
      "25256/25256 - 133s - loss: 0.1007 - f1_score_mod: 0.3652 - recall_mod: 0.2417 - precision_mod: 0.7512 - dur_error: 0.3563 - maestro_dur_loss: 0.0178 - val_loss: 0.0997 - val_f1_score_mod: 0.3477 - val_recall_mod: 0.2301 - val_precision_mod: 0.7139 - val_dur_error: 0.2898 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09943 to 0.09905, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1001 - f1_score_mod: 0.3732 - recall_mod: 0.2490 - precision_mod: 0.7485 - dur_error: 0.3551 - maestro_dur_loss: 0.0178 - val_loss: 0.0991 - val_f1_score_mod: 0.3390 - val_recall_mod: 0.2215 - val_precision_mod: 0.7278 - val_dur_error: 0.2832 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09905 to 0.09814, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0995 - f1_score_mod: 0.3769 - recall_mod: 0.2518 - precision_mod: 0.7537 - dur_error: 0.3564 - maestro_dur_loss: 0.0178 - val_loss: 0.0981 - val_f1_score_mod: 0.3489 - val_recall_mod: 0.2294 - val_precision_mod: 0.7315 - val_dur_error: 0.2730 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09814\n",
      "25256/25256 - 133s - loss: 0.0989 - f1_score_mod: 0.3834 - recall_mod: 0.2576 - precision_mod: 0.7524 - dur_error: 0.3562 - maestro_dur_loss: 0.0178 - val_loss: 0.0984 - val_f1_score_mod: 0.3458 - val_recall_mod: 0.2268 - val_precision_mod: 0.7313 - val_dur_error: 0.2723 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09814\n",
      "25256/25256 - 133s - loss: 0.0984 - f1_score_mod: 0.3871 - recall_mod: 0.2610 - precision_mod: 0.7540 - dur_error: 0.3550 - maestro_dur_loss: 0.0177 - val_loss: 0.1001 - val_f1_score_mod: 0.3628 - val_recall_mod: 0.2437 - val_precision_mod: 0.7120 - val_dur_error: 0.3265 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09814\n",
      "25256/25256 - 134s - loss: 0.0974 - f1_score_mod: 0.3950 - recall_mod: 0.2677 - precision_mod: 0.7553 - dur_error: 0.3506 - maestro_dur_loss: 0.0175 - val_loss: 0.0991 - val_f1_score_mod: 0.3586 - val_recall_mod: 0.2415 - val_precision_mod: 0.6991 - val_dur_error: 0.3051 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09814 to 0.09623, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0966 - f1_score_mod: 0.3982 - recall_mod: 0.2701 - precision_mod: 0.7604 - dur_error: 0.3444 - maestro_dur_loss: 0.0172 - val_loss: 0.0962 - val_f1_score_mod: 0.3645 - val_recall_mod: 0.2445 - val_precision_mod: 0.7182 - val_dur_error: 0.2624 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09623\n",
      "25256/25256 - 134s - loss: 0.0964 - f1_score_mod: 0.4024 - recall_mod: 0.2750 - precision_mod: 0.7528 - dur_error: 0.3500 - maestro_dur_loss: 0.0175 - val_loss: 0.0971 - val_f1_score_mod: 0.3681 - val_recall_mod: 0.2484 - val_precision_mod: 0.7164 - val_dur_error: 0.2745 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09623\n",
      "25256/25256 - 133s - loss: 0.0957 - f1_score_mod: 0.4073 - recall_mod: 0.2792 - precision_mod: 0.7559 - dur_error: 0.3475 - maestro_dur_loss: 0.0174 - val_loss: 0.0965 - val_f1_score_mod: 0.3798 - val_recall_mod: 0.2590 - val_precision_mod: 0.7152 - val_dur_error: 0.2739 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09623\n",
      "25256/25256 - 133s - loss: 0.0947 - f1_score_mod: 0.4170 - recall_mod: 0.2881 - precision_mod: 0.7572 - dur_error: 0.3440 - maestro_dur_loss: 0.0172 - val_loss: 0.1020 - val_f1_score_mod: 0.3793 - val_recall_mod: 0.2623 - val_precision_mod: 0.6885 - val_dur_error: 0.3767 - val_maestro_dur_loss: 0.0188\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09623 to 0.09569, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0945 - f1_score_mod: 0.4179 - recall_mod: 0.2890 - precision_mod: 0.7573 - dur_error: 0.3447 - maestro_dur_loss: 0.0172 - val_loss: 0.0957 - val_f1_score_mod: 0.3839 - val_recall_mod: 0.2646 - val_precision_mod: 0.7021 - val_dur_error: 0.2636 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09569\n",
      "25256/25256 - 133s - loss: 0.0935 - f1_score_mod: 0.4258 - recall_mod: 0.2960 - precision_mod: 0.7609 - dur_error: 0.3435 - maestro_dur_loss: 0.0172 - val_loss: 0.0970 - val_f1_score_mod: 0.3895 - val_recall_mod: 0.2740 - val_precision_mod: 0.6773 - val_dur_error: 0.2825 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09569 to 0.09565, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 136s - loss: 0.0931 - f1_score_mod: 0.4279 - recall_mod: 0.2989 - precision_mod: 0.7567 - dur_error: 0.3416 - maestro_dur_loss: 0.0171 - val_loss: 0.0956 - val_f1_score_mod: 0.3897 - val_recall_mod: 0.2699 - val_precision_mod: 0.7046 - val_dur_error: 0.2639 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.09565\n",
      "25256/25256 - 134s - loss: 0.0921 - f1_score_mod: 0.4369 - recall_mod: 0.3069 - precision_mod: 0.7617 - dur_error: 0.3366 - maestro_dur_loss: 0.0168 - val_loss: 0.0968 - val_f1_score_mod: 0.3954 - val_recall_mod: 0.2757 - val_precision_mod: 0.7029 - val_dur_error: 0.2988 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09565 to 0.09523, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0915 - f1_score_mod: 0.4394 - recall_mod: 0.3094 - precision_mod: 0.7605 - dur_error: 0.3366 - maestro_dur_loss: 0.0168 - val_loss: 0.0952 - val_f1_score_mod: 0.3949 - val_recall_mod: 0.2780 - val_precision_mod: 0.6832 - val_dur_error: 0.2617 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09523\n",
      "25256/25256 - 134s - loss: 0.0910 - f1_score_mod: 0.4456 - recall_mod: 0.3156 - precision_mod: 0.7597 - dur_error: 0.3379 - maestro_dur_loss: 0.0169 - val_loss: 0.0953 - val_f1_score_mod: 0.4017 - val_recall_mod: 0.2828 - val_precision_mod: 0.6952 - val_dur_error: 0.2612 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09523 to 0.09438, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0903 - f1_score_mod: 0.4494 - recall_mod: 0.3193 - precision_mod: 0.7607 - dur_error: 0.3365 - maestro_dur_loss: 0.0168 - val_loss: 0.0944 - val_f1_score_mod: 0.3971 - val_recall_mod: 0.2796 - val_precision_mod: 0.6872 - val_dur_error: 0.2564 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09438\n",
      "25256/25256 - 134s - loss: 0.0897 - f1_score_mod: 0.4571 - recall_mod: 0.3265 - precision_mod: 0.7639 - dur_error: 0.3357 - maestro_dur_loss: 0.0168 - val_loss: 0.0956 - val_f1_score_mod: 0.4019 - val_recall_mod: 0.2837 - val_precision_mod: 0.6927 - val_dur_error: 0.2781 - val_maestro_dur_loss: 0.0139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09438 to 0.09389, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.0887 - f1_score_mod: 0.4641 - recall_mod: 0.3338 - precision_mod: 0.7629 - dur_error: 0.3314 - maestro_dur_loss: 0.0166 - val_loss: 0.0939 - val_f1_score_mod: 0.4094 - val_recall_mod: 0.2931 - val_precision_mod: 0.6810 - val_dur_error: 0.2496 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.09389\n",
      "25256/25256 - 133s - loss: 0.0883 - f1_score_mod: 0.4658 - recall_mod: 0.3357 - precision_mod: 0.7619 - dur_error: 0.3317 - maestro_dur_loss: 0.0166 - val_loss: 0.0949 - val_f1_score_mod: 0.4156 - val_recall_mod: 0.2977 - val_precision_mod: 0.6949 - val_dur_error: 0.2798 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09389 to 0.09333, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0875 - f1_score_mod: 0.4704 - recall_mod: 0.3407 - precision_mod: 0.7614 - dur_error: 0.3288 - maestro_dur_loss: 0.0164 - val_loss: 0.0933 - val_f1_score_mod: 0.4039 - val_recall_mod: 0.2846 - val_precision_mod: 0.6974 - val_dur_error: 0.2510 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09333\n",
      "25256/25256 - 134s - loss: 0.0871 - f1_score_mod: 0.4766 - recall_mod: 0.3471 - precision_mod: 0.7624 - dur_error: 0.3344 - maestro_dur_loss: 0.0167 - val_loss: 0.0945 - val_f1_score_mod: 0.4039 - val_recall_mod: 0.2870 - val_precision_mod: 0.6862 - val_dur_error: 0.2563 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09333\n",
      "25256/25256 - 133s - loss: 0.0864 - f1_score_mod: 0.4821 - recall_mod: 0.3526 - precision_mod: 0.7637 - dur_error: 0.3280 - maestro_dur_loss: 0.0164 - val_loss: 0.0958 - val_f1_score_mod: 0.4224 - val_recall_mod: 0.3078 - val_precision_mod: 0.6757 - val_dur_error: 0.2933 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09333\n",
      "25256/25256 - 134s - loss: 0.0854 - f1_score_mod: 0.4897 - recall_mod: 0.3603 - precision_mod: 0.7655 - dur_error: 0.3260 - maestro_dur_loss: 0.0163 - val_loss: 0.0939 - val_f1_score_mod: 0.4269 - val_recall_mod: 0.3105 - val_precision_mod: 0.6888 - val_dur_error: 0.2664 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09333\n",
      "25256/25256 - 134s - loss: 0.0849 - f1_score_mod: 0.4948 - recall_mod: 0.3660 - precision_mod: 0.7652 - dur_error: 0.3261 - maestro_dur_loss: 0.0163 - val_loss: 0.0957 - val_f1_score_mod: 0.4295 - val_recall_mod: 0.3139 - val_precision_mod: 0.6825 - val_dur_error: 0.3001 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09333 to 0.09281, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 133s - loss: 0.0840 - f1_score_mod: 0.5017 - recall_mod: 0.3732 - precision_mod: 0.7673 - dur_error: 0.3261 - maestro_dur_loss: 0.0163 - val_loss: 0.0928 - val_f1_score_mod: 0.4284 - val_recall_mod: 0.3115 - val_precision_mod: 0.6881 - val_dur_error: 0.2510 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0834 - f1_score_mod: 0.5054 - recall_mod: 0.3772 - precision_mod: 0.7665 - dur_error: 0.3244 - maestro_dur_loss: 0.0162 - val_loss: 0.0946 - val_f1_score_mod: 0.4327 - val_recall_mod: 0.3181 - val_precision_mod: 0.6770 - val_dur_error: 0.2813 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0828 - f1_score_mod: 0.5104 - recall_mod: 0.3826 - precision_mod: 0.7676 - dur_error: 0.3217 - maestro_dur_loss: 0.0161 - val_loss: 0.0956 - val_f1_score_mod: 0.4470 - val_recall_mod: 0.3381 - val_precision_mod: 0.6614 - val_dur_error: 0.3073 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0820 - f1_score_mod: 0.5162 - recall_mod: 0.3892 - precision_mod: 0.7674 - dur_error: 0.3218 - maestro_dur_loss: 0.0161 - val_loss: 0.0931 - val_f1_score_mod: 0.4335 - val_recall_mod: 0.3205 - val_precision_mod: 0.6722 - val_dur_error: 0.2481 - val_maestro_dur_loss: 0.0124\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0811 - f1_score_mod: 0.5224 - recall_mod: 0.3962 - precision_mod: 0.7681 - dur_error: 0.3166 - maestro_dur_loss: 0.0158 - val_loss: 0.0962 - val_f1_score_mod: 0.4473 - val_recall_mod: 0.3363 - val_precision_mod: 0.6687 - val_dur_error: 0.3154 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0806 - f1_score_mod: 0.5273 - recall_mod: 0.4018 - precision_mod: 0.7683 - dur_error: 0.3196 - maestro_dur_loss: 0.0160 - val_loss: 0.0932 - val_f1_score_mod: 0.4421 - val_recall_mod: 0.3284 - val_precision_mod: 0.6774 - val_dur_error: 0.2561 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0797 - f1_score_mod: 0.5348 - recall_mod: 0.4098 - precision_mod: 0.7711 - dur_error: 0.3175 - maestro_dur_loss: 0.0159 - val_loss: 0.0942 - val_f1_score_mod: 0.4423 - val_recall_mod: 0.3333 - val_precision_mod: 0.6579 - val_dur_error: 0.2686 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0792 - f1_score_mod: 0.5378 - recall_mod: 0.4134 - precision_mod: 0.7706 - dur_error: 0.3173 - maestro_dur_loss: 0.0159 - val_loss: 0.0928 - val_f1_score_mod: 0.4504 - val_recall_mod: 0.3426 - val_precision_mod: 0.6588 - val_dur_error: 0.2559 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09281\n",
      "25256/25256 - 135s - loss: 0.0786 - f1_score_mod: 0.5428 - recall_mod: 0.4192 - precision_mod: 0.7714 - dur_error: 0.3174 - maestro_dur_loss: 0.0159 - val_loss: 0.0984 - val_f1_score_mod: 0.4535 - val_recall_mod: 0.3518 - val_precision_mod: 0.6397 - val_dur_error: 0.3517 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.09281\n",
      "25256/25256 - 135s - loss: 0.0777 - f1_score_mod: 0.5515 - recall_mod: 0.4291 - precision_mod: 0.7726 - dur_error: 0.3143 - maestro_dur_loss: 0.0157 - val_loss: 0.0958 - val_f1_score_mod: 0.4583 - val_recall_mod: 0.3534 - val_precision_mod: 0.6539 - val_dur_error: 0.3061 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0769 - f1_score_mod: 0.5550 - recall_mod: 0.4331 - precision_mod: 0.7734 - dur_error: 0.3125 - maestro_dur_loss: 0.0156 - val_loss: 0.0947 - val_f1_score_mod: 0.4669 - val_recall_mod: 0.3659 - val_precision_mod: 0.6460 - val_dur_error: 0.2784 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0763 - f1_score_mod: 0.5616 - recall_mod: 0.4412 - precision_mod: 0.7733 - dur_error: 0.3127 - maestro_dur_loss: 0.0156 - val_loss: 0.0940 - val_f1_score_mod: 0.4600 - val_recall_mod: 0.3538 - val_precision_mod: 0.6586 - val_dur_error: 0.2677 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.09281\n",
      "25256/25256 - 135s - loss: 0.0755 - f1_score_mod: 0.5662 - recall_mod: 0.4462 - precision_mod: 0.7756 - dur_error: 0.3106 - maestro_dur_loss: 0.0155 - val_loss: 0.0990 - val_f1_score_mod: 0.4667 - val_recall_mod: 0.3651 - val_precision_mod: 0.6472 - val_dur_error: 0.3656 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.09281\n",
      "25256/25256 - 135s - loss: 0.0748 - f1_score_mod: 0.5725 - recall_mod: 0.4545 - precision_mod: 0.7742 - dur_error: 0.3108 - maestro_dur_loss: 0.0155 - val_loss: 0.0956 - val_f1_score_mod: 0.4615 - val_recall_mod: 0.3598 - val_precision_mod: 0.6445 - val_dur_error: 0.2856 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.09281\n",
      "25256/25256 - 138s - loss: 0.0739 - f1_score_mod: 0.5784 - recall_mod: 0.4606 - precision_mod: 0.7780 - dur_error: 0.3086 - maestro_dur_loss: 0.0154 - val_loss: 0.0979 - val_f1_score_mod: 0.4750 - val_recall_mod: 0.3763 - val_precision_mod: 0.6451 - val_dur_error: 0.3353 - val_maestro_dur_loss: 0.0168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0730 - f1_score_mod: 0.5869 - recall_mod: 0.4701 - precision_mod: 0.7817 - dur_error: 0.3059 - maestro_dur_loss: 0.0153 - val_loss: 0.0941 - val_f1_score_mod: 0.4699 - val_recall_mod: 0.3688 - val_precision_mod: 0.6482 - val_dur_error: 0.2536 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 86/150\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.09281\n",
      "25256/25256 - 134s - loss: 0.0727 - f1_score_mod: 0.5904 - recall_mod: 0.4754 - precision_mod: 0.7797 - dur_error: 0.3055 - maestro_dur_loss: 0.0153 - val_loss: 0.0983 - val_f1_score_mod: 0.4758 - val_recall_mod: 0.3760 - val_precision_mod: 0.6486 - val_dur_error: 0.3536 - val_maestro_dur_loss: 0.0177\n",
      "Epoch 87/150\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.09281\n",
      "25256/25256 - 133s - loss: 0.0721 - f1_score_mod: 0.5957 - recall_mod: 0.4815 - precision_mod: 0.7816 - dur_error: 0.3092 - maestro_dur_loss: 0.0155 - val_loss: 0.0954 - val_f1_score_mod: 0.4797 - val_recall_mod: 0.3805 - val_precision_mod: 0.6495 - val_dur_error: 0.2737 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 88/150\n",
      "Batch 24: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12800/25256 - 57s - loss: nan - f1_score_mod: 0.6052 - recall_mod: 0.4934 - precision_mod: 0.7831 - dur_error: 0.3060 - maestro_dur_loss: 0.0153\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 1.0, dropout_rate = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It certainly does delay failure until the training loss is significantly lower. However, since dropout is designed to combat overfitting, lowering it increases overfitting causing the validation loss to remain relatively high. It now makes sense to increase dropout_rate since lowering it did not have a favorable effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25256 samples, validate on 10824 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16498, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 178s - loss: 0.2584 - f1_score_mod: 0.0313 - recall_mod: 0.0438 - precision_mod: 0.0693 - dur_error: 1.0616 - maestro_dur_loss: 0.0531 - val_loss: 0.1650 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.9011 - val_maestro_dur_loss: 0.0451\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16498 to 0.15863, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 172s - loss: 0.1751 - f1_score_mod: 0.0075 - recall_mod: 0.0039 - precision_mod: 0.1424 - dur_error: 0.7806 - maestro_dur_loss: 0.0390 - val_loss: 0.1586 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5895 - val_maestro_dur_loss: 0.0295\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15863 to 0.13546, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 170s - loss: 0.1614 - f1_score_mod: 0.0056 - recall_mod: 0.0028 - precision_mod: 0.2764 - dur_error: 0.6815 - maestro_dur_loss: 0.0341 - val_loss: 0.1355 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4418 - val_maestro_dur_loss: 0.0221\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13546 to 0.13469, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 170s - loss: 0.1535 - f1_score_mod: 0.0183 - recall_mod: 0.0094 - precision_mod: 0.4630 - dur_error: 0.6340 - maestro_dur_loss: 0.0317 - val_loss: 0.1347 - val_f1_score_mod: 0.0129 - val_recall_mod: 0.0065 - val_precision_mod: 0.7702 - val_dur_error: 0.4886 - val_maestro_dur_loss: 0.0244\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13469 to 0.12932, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 174s - loss: 0.1487 - f1_score_mod: 0.0277 - recall_mod: 0.0143 - precision_mod: 0.5415 - dur_error: 0.6081 - maestro_dur_loss: 0.0304 - val_loss: 0.1293 - val_f1_score_mod: 0.0242 - val_recall_mod: 0.0123 - val_precision_mod: 0.7229 - val_dur_error: 0.4196 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12932 to 0.12769, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 170s - loss: 0.1451 - f1_score_mod: 0.0437 - recall_mod: 0.0228 - precision_mod: 0.5709 - dur_error: 0.5897 - maestro_dur_loss: 0.0295 - val_loss: 0.1277 - val_f1_score_mod: 0.0460 - val_recall_mod: 0.0238 - val_precision_mod: 0.7222 - val_dur_error: 0.4183 - val_maestro_dur_loss: 0.0209\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12769\n",
      "25256/25256 - 168s - loss: 0.1425 - f1_score_mod: 0.0561 - recall_mod: 0.0295 - precision_mod: 0.5912 - dur_error: 0.5783 - maestro_dur_loss: 0.0289 - val_loss: 0.1314 - val_f1_score_mod: 0.0575 - val_recall_mod: 0.0301 - val_precision_mod: 0.7119 - val_dur_error: 0.5179 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12769 to 0.12258, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 168s - loss: 0.1403 - f1_score_mod: 0.0707 - recall_mod: 0.0376 - precision_mod: 0.6325 - dur_error: 0.5697 - maestro_dur_loss: 0.0285 - val_loss: 0.1226 - val_f1_score_mod: 0.0752 - val_recall_mod: 0.0398 - val_precision_mod: 0.7170 - val_dur_error: 0.3801 - val_maestro_dur_loss: 0.0190\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12258\n",
      "25256/25256 - 168s - loss: 0.1377 - f1_score_mod: 0.0816 - recall_mod: 0.0437 - precision_mod: 0.6436 - dur_error: 0.5528 - maestro_dur_loss: 0.0276 - val_loss: 0.1229 - val_f1_score_mod: 0.0744 - val_recall_mod: 0.0392 - val_precision_mod: 0.7499 - val_dur_error: 0.4007 - val_maestro_dur_loss: 0.0200\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12258 to 0.12158, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 169s - loss: 0.1364 - f1_score_mod: 0.0939 - recall_mod: 0.0508 - precision_mod: 0.6430 - dur_error: 0.5493 - maestro_dur_loss: 0.0275 - val_loss: 0.1216 - val_f1_score_mod: 0.0879 - val_recall_mod: 0.0468 - val_precision_mod: 0.7538 - val_dur_error: 0.3918 - val_maestro_dur_loss: 0.0196\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.12158\n",
      "25256/25256 - 167s - loss: 0.1344 - f1_score_mod: 0.1015 - recall_mod: 0.0552 - precision_mod: 0.6494 - dur_error: 0.5318 - maestro_dur_loss: 0.0266 - val_loss: 0.1220 - val_f1_score_mod: 0.1039 - val_recall_mod: 0.0560 - val_precision_mod: 0.7407 - val_dur_error: 0.4292 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.12158 to 0.11711, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 168s - loss: 0.1331 - f1_score_mod: 0.1130 - recall_mod: 0.0621 - precision_mod: 0.6554 - dur_error: 0.5226 - maestro_dur_loss: 0.0261 - val_loss: 0.1171 - val_f1_score_mod: 0.1046 - val_recall_mod: 0.0563 - val_precision_mod: 0.7581 - val_dur_error: 0.3355 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11711 to 0.11594, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 168s - loss: 0.1316 - f1_score_mod: 0.1177 - recall_mod: 0.0649 - precision_mod: 0.6567 - dur_error: 0.5067 - maestro_dur_loss: 0.0253 - val_loss: 0.1159 - val_f1_score_mod: 0.1161 - val_recall_mod: 0.0631 - val_precision_mod: 0.7396 - val_dur_error: 0.3321 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11594 to 0.11491, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 167s - loss: 0.1306 - f1_score_mod: 0.1236 - recall_mod: 0.0684 - precision_mod: 0.6592 - dur_error: 0.5024 - maestro_dur_loss: 0.0251 - val_loss: 0.1149 - val_f1_score_mod: 0.1394 - val_recall_mod: 0.0772 - val_precision_mod: 0.7278 - val_dur_error: 0.3175 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11491\n",
      "25256/25256 - 168s - loss: 0.1294 - f1_score_mod: 0.1311 - recall_mod: 0.0729 - precision_mod: 0.6616 - dur_error: 0.4908 - maestro_dur_loss: 0.0245 - val_loss: 0.1185 - val_f1_score_mod: 0.1389 - val_recall_mod: 0.0770 - val_precision_mod: 0.7246 - val_dur_error: 0.4009 - val_maestro_dur_loss: 0.0200\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11491\n",
      "25256/25256 - 170s - loss: 0.1284 - f1_score_mod: 0.1388 - recall_mod: 0.0777 - precision_mod: 0.6648 - dur_error: 0.4843 - maestro_dur_loss: 0.0242 - val_loss: 0.1192 - val_f1_score_mod: 0.1303 - val_recall_mod: 0.0717 - val_precision_mod: 0.7270 - val_dur_error: 0.4199 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11491 to 0.11346, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 169s - loss: 0.1274 - f1_score_mod: 0.1419 - recall_mod: 0.0796 - precision_mod: 0.6666 - dur_error: 0.4776 - maestro_dur_loss: 0.0239 - val_loss: 0.1135 - val_f1_score_mod: 0.1367 - val_recall_mod: 0.0754 - val_precision_mod: 0.7571 - val_dur_error: 0.3230 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11346\n",
      "25256/25256 - 170s - loss: 0.1270 - f1_score_mod: 0.1487 - recall_mod: 0.0837 - precision_mod: 0.6771 - dur_error: 0.4784 - maestro_dur_loss: 0.0239 - val_loss: 0.1191 - val_f1_score_mod: 0.1754 - val_recall_mod: 0.1002 - val_precision_mod: 0.7096 - val_dur_error: 0.4295 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11346\n",
      "25256/25256 - 169s - loss: 0.1260 - f1_score_mod: 0.1533 - recall_mod: 0.0866 - precision_mod: 0.6823 - dur_error: 0.4713 - maestro_dur_loss: 0.0236 - val_loss: 0.1146 - val_f1_score_mod: 0.1830 - val_recall_mod: 0.1058 - val_precision_mod: 0.6847 - val_dur_error: 0.3598 - val_maestro_dur_loss: 0.0180\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11346\n",
      "25256/25256 - 174s - loss: 0.1254 - f1_score_mod: 0.1605 - recall_mod: 0.0914 - precision_mod: 0.6685 - dur_error: 0.4711 - maestro_dur_loss: 0.0236 - val_loss: 0.1164 - val_f1_score_mod: 0.1744 - val_recall_mod: 0.0998 - val_precision_mod: 0.7021 - val_dur_error: 0.3997 - val_maestro_dur_loss: 0.0200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.11346 to 0.10987, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 172s - loss: 0.1246 - f1_score_mod: 0.1622 - recall_mod: 0.0924 - precision_mod: 0.6741 - dur_error: 0.4601 - maestro_dur_loss: 0.0230 - val_loss: 0.1099 - val_f1_score_mod: 0.1726 - val_recall_mod: 0.0982 - val_precision_mod: 0.7278 - val_dur_error: 0.2901 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10987\n",
      "25256/25256 - 169s - loss: 0.1240 - f1_score_mod: 0.1682 - recall_mod: 0.0961 - precision_mod: 0.6842 - dur_error: 0.4601 - maestro_dur_loss: 0.0230 - val_loss: 0.1169 - val_f1_score_mod: 0.1831 - val_recall_mod: 0.1049 - val_precision_mod: 0.7293 - val_dur_error: 0.4274 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10987\n",
      "25256/25256 - 170s - loss: 0.1234 - f1_score_mod: 0.1758 - recall_mod: 0.1012 - precision_mod: 0.6846 - dur_error: 0.4562 - maestro_dur_loss: 0.0228 - val_loss: 0.1104 - val_f1_score_mod: 0.1976 - val_recall_mod: 0.1149 - val_precision_mod: 0.7196 - val_dur_error: 0.3075 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10987 to 0.10942, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 167s - loss: 0.1225 - f1_score_mod: 0.1791 - recall_mod: 0.1033 - precision_mod: 0.6825 - dur_error: 0.4493 - maestro_dur_loss: 0.0225 - val_loss: 0.1094 - val_f1_score_mod: 0.1775 - val_recall_mod: 0.1010 - val_precision_mod: 0.7436 - val_dur_error: 0.2984 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10942\n",
      "25256/25256 - 167s - loss: 0.1218 - f1_score_mod: 0.1846 - recall_mod: 0.1067 - precision_mod: 0.6976 - dur_error: 0.4470 - maestro_dur_loss: 0.0224 - val_loss: 0.1159 - val_f1_score_mod: 0.1961 - val_recall_mod: 0.1135 - val_precision_mod: 0.7296 - val_dur_error: 0.4277 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10942 to 0.10927, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 168s - loss: 0.1214 - f1_score_mod: 0.1915 - recall_mod: 0.1112 - precision_mod: 0.6973 - dur_error: 0.4451 - maestro_dur_loss: 0.0223 - val_loss: 0.1093 - val_f1_score_mod: 0.1740 - val_recall_mod: 0.0982 - val_precision_mod: 0.7787 - val_dur_error: 0.2968 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10927 to 0.10833, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 168s - loss: 0.1210 - f1_score_mod: 0.1922 - recall_mod: 0.1119 - precision_mod: 0.6937 - dur_error: 0.4429 - maestro_dur_loss: 0.0221 - val_loss: 0.1083 - val_f1_score_mod: 0.2025 - val_recall_mod: 0.1183 - val_precision_mod: 0.7132 - val_dur_error: 0.2930 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10833 to 0.10754, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 172s - loss: 0.1207 - f1_score_mod: 0.1983 - recall_mod: 0.1158 - precision_mod: 0.6955 - dur_error: 0.4455 - maestro_dur_loss: 0.0223 - val_loss: 0.1075 - val_f1_score_mod: 0.2096 - val_recall_mod: 0.1228 - val_precision_mod: 0.7246 - val_dur_error: 0.2864 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10754\n",
      "25256/25256 - 169s - loss: 0.1202 - f1_score_mod: 0.2027 - recall_mod: 0.1188 - precision_mod: 0.6975 - dur_error: 0.4435 - maestro_dur_loss: 0.0222 - val_loss: 0.1143 - val_f1_score_mod: 0.2342 - val_recall_mod: 0.1411 - val_precision_mod: 0.6977 - val_dur_error: 0.4132 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10754\n",
      "25256/25256 - 168s - loss: 0.1195 - f1_score_mod: 0.2094 - recall_mod: 0.1231 - precision_mod: 0.7077 - dur_error: 0.4388 - maestro_dur_loss: 0.0219 - val_loss: 0.1109 - val_f1_score_mod: 0.2180 - val_recall_mod: 0.1288 - val_precision_mod: 0.7340 - val_dur_error: 0.3559 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10754 to 0.10709, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 170s - loss: 0.1190 - f1_score_mod: 0.2129 - recall_mod: 0.1256 - precision_mod: 0.7070 - dur_error: 0.4363 - maestro_dur_loss: 0.0218 - val_loss: 0.1071 - val_f1_score_mod: 0.2249 - val_recall_mod: 0.1333 - val_precision_mod: 0.7247 - val_dur_error: 0.2856 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10709\n",
      "25256/25256 - 167s - loss: 0.1182 - f1_score_mod: 0.2165 - recall_mod: 0.1282 - precision_mod: 0.7030 - dur_error: 0.4273 - maestro_dur_loss: 0.0214 - val_loss: 0.1072 - val_f1_score_mod: 0.2366 - val_recall_mod: 0.1415 - val_precision_mod: 0.7277 - val_dur_error: 0.3025 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10709\n",
      "25256/25256 - 166s - loss: 0.1180 - f1_score_mod: 0.2206 - recall_mod: 0.1311 - precision_mod: 0.7050 - dur_error: 0.4321 - maestro_dur_loss: 0.0216 - val_loss: 0.1116 - val_f1_score_mod: 0.2417 - val_recall_mod: 0.1468 - val_precision_mod: 0.6937 - val_dur_error: 0.3873 - val_maestro_dur_loss: 0.0194\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10709 to 0.10608, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 167s - loss: 0.1173 - f1_score_mod: 0.2266 - recall_mod: 0.1352 - precision_mod: 0.7077 - dur_error: 0.4263 - maestro_dur_loss: 0.0213 - val_loss: 0.1061 - val_f1_score_mod: 0.2311 - val_recall_mod: 0.1373 - val_precision_mod: 0.7423 - val_dur_error: 0.2822 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.10608 to 0.10605, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 167s - loss: 0.1171 - f1_score_mod: 0.2295 - recall_mod: 0.1369 - precision_mod: 0.7153 - dur_error: 0.4302 - maestro_dur_loss: 0.0215 - val_loss: 0.1061 - val_f1_score_mod: 0.2241 - val_recall_mod: 0.1318 - val_precision_mod: 0.7571 - val_dur_error: 0.2898 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10605\n",
      "25256/25256 - 169s - loss: 0.1165 - f1_score_mod: 0.2312 - recall_mod: 0.1385 - precision_mod: 0.7089 - dur_error: 0.4237 - maestro_dur_loss: 0.0212 - val_loss: 0.1068 - val_f1_score_mod: 0.2481 - val_recall_mod: 0.1497 - val_precision_mod: 0.7285 - val_dur_error: 0.3163 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10605\n",
      "25256/25256 - 171s - loss: 0.1161 - f1_score_mod: 0.2388 - recall_mod: 0.1436 - precision_mod: 0.7150 - dur_error: 0.4270 - maestro_dur_loss: 0.0213 - val_loss: 0.1119 - val_f1_score_mod: 0.2578 - val_recall_mod: 0.1584 - val_precision_mod: 0.7052 - val_dur_error: 0.4128 - val_maestro_dur_loss: 0.0206\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.10605\n",
      "25256/25256 - 170s - loss: 0.1157 - f1_score_mod: 0.2415 - recall_mod: 0.1459 - precision_mod: 0.7088 - dur_error: 0.4203 - maestro_dur_loss: 0.0210 - val_loss: 0.1069 - val_f1_score_mod: 0.2491 - val_recall_mod: 0.1508 - val_precision_mod: 0.7321 - val_dur_error: 0.3269 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.10605 to 0.10424, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 172s - loss: 0.1152 - f1_score_mod: 0.2442 - recall_mod: 0.1477 - precision_mod: 0.7135 - dur_error: 0.4203 - maestro_dur_loss: 0.0210 - val_loss: 0.1042 - val_f1_score_mod: 0.2447 - val_recall_mod: 0.1468 - val_precision_mod: 0.7435 - val_dur_error: 0.2752 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10424\n",
      "25256/25256 - 171s - loss: 0.1147 - f1_score_mod: 0.2486 - recall_mod: 0.1505 - precision_mod: 0.7202 - dur_error: 0.4191 - maestro_dur_loss: 0.0210 - val_loss: 0.1083 - val_f1_score_mod: 0.2670 - val_recall_mod: 0.1646 - val_precision_mod: 0.7092 - val_dur_error: 0.3648 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10424\n",
      "25256/25256 - 192s - loss: 0.1141 - f1_score_mod: 0.2543 - recall_mod: 0.1546 - precision_mod: 0.7222 - dur_error: 0.4147 - maestro_dur_loss: 0.0207 - val_loss: 0.1156 - val_f1_score_mod: 0.2810 - val_recall_mod: 0.1756 - val_precision_mod: 0.7087 - val_dur_error: 0.5047 - val_maestro_dur_loss: 0.0252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.10424 to 0.10325, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 184s - loss: 0.1137 - f1_score_mod: 0.2577 - recall_mod: 0.1573 - precision_mod: 0.7200 - dur_error: 0.4170 - maestro_dur_loss: 0.0209 - val_loss: 0.1032 - val_f1_score_mod: 0.2648 - val_recall_mod: 0.1613 - val_precision_mod: 0.7426 - val_dur_error: 0.2715 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.10325\n",
      "25256/25256 - 179s - loss: 0.1133 - f1_score_mod: 0.2614 - recall_mod: 0.1599 - precision_mod: 0.7212 - dur_error: 0.4114 - maestro_dur_loss: 0.0206 - val_loss: 0.1067 - val_f1_score_mod: 0.2824 - val_recall_mod: 0.1754 - val_precision_mod: 0.7279 - val_dur_error: 0.3511 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.10325\n",
      "25256/25256 - 174s - loss: 0.1129 - f1_score_mod: 0.2668 - recall_mod: 0.1642 - precision_mod: 0.7207 - dur_error: 0.4117 - maestro_dur_loss: 0.0206 - val_loss: 0.1069 - val_f1_score_mod: 0.2884 - val_recall_mod: 0.1809 - val_precision_mod: 0.7173 - val_dur_error: 0.3550 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.10325\n",
      "25256/25256 - 154s - loss: 0.1126 - f1_score_mod: 0.2668 - recall_mod: 0.1641 - precision_mod: 0.7205 - dur_error: 0.4122 - maestro_dur_loss: 0.0206 - val_loss: 0.1036 - val_f1_score_mod: 0.2655 - val_recall_mod: 0.1626 - val_precision_mod: 0.7395 - val_dur_error: 0.3022 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.10325\n",
      "25256/25256 - 143s - loss: 0.1119 - f1_score_mod: 0.2732 - recall_mod: 0.1685 - precision_mod: 0.7268 - dur_error: 0.4100 - maestro_dur_loss: 0.0205 - val_loss: 0.1096 - val_f1_score_mod: 0.2958 - val_recall_mod: 0.1870 - val_precision_mod: 0.7130 - val_dur_error: 0.4176 - val_maestro_dur_loss: 0.0209\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.10325 to 0.10154, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 143s - loss: 0.1115 - f1_score_mod: 0.2744 - recall_mod: 0.1700 - precision_mod: 0.7186 - dur_error: 0.4092 - maestro_dur_loss: 0.0205 - val_loss: 0.1015 - val_f1_score_mod: 0.2978 - val_recall_mod: 0.1901 - val_precision_mod: 0.6941 - val_dur_error: 0.2633 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.10154\n",
      "25256/25256 - 147s - loss: 0.1110 - f1_score_mod: 0.2820 - recall_mod: 0.1753 - precision_mod: 0.7258 - dur_error: 0.4038 - maestro_dur_loss: 0.0202 - val_loss: 0.1041 - val_f1_score_mod: 0.2878 - val_recall_mod: 0.1802 - val_precision_mod: 0.7210 - val_dur_error: 0.3132 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.10154\n",
      "25256/25256 - 149s - loss: 0.1107 - f1_score_mod: 0.2840 - recall_mod: 0.1768 - precision_mod: 0.7267 - dur_error: 0.4045 - maestro_dur_loss: 0.0202 - val_loss: 0.1044 - val_f1_score_mod: 0.2867 - val_recall_mod: 0.1786 - val_precision_mod: 0.7353 - val_dur_error: 0.3355 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.10154\n",
      "25256/25256 - 159s - loss: 0.1104 - f1_score_mod: 0.2862 - recall_mod: 0.1789 - precision_mod: 0.7221 - dur_error: 0.4057 - maestro_dur_loss: 0.0203 - val_loss: 0.1048 - val_f1_score_mod: 0.2938 - val_recall_mod: 0.1845 - val_precision_mod: 0.7276 - val_dur_error: 0.3450 - val_maestro_dur_loss: 0.0172\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.10154 to 0.10079, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 142s - loss: 0.1100 - f1_score_mod: 0.2930 - recall_mod: 0.1842 - precision_mod: 0.7204 - dur_error: 0.4062 - maestro_dur_loss: 0.0203 - val_loss: 0.1008 - val_f1_score_mod: 0.2915 - val_recall_mod: 0.1829 - val_precision_mod: 0.7277 - val_dur_error: 0.2681 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.10079 to 0.10069, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 153s - loss: 0.1092 - f1_score_mod: 0.2968 - recall_mod: 0.1865 - precision_mod: 0.7325 - dur_error: 0.4010 - maestro_dur_loss: 0.0200 - val_loss: 0.1007 - val_f1_score_mod: 0.2884 - val_recall_mod: 0.1793 - val_precision_mod: 0.7416 - val_dur_error: 0.2707 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.10069 to 0.10053, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 148s - loss: 0.1090 - f1_score_mod: 0.2976 - recall_mod: 0.1876 - precision_mod: 0.7248 - dur_error: 0.4011 - maestro_dur_loss: 0.0201 - val_loss: 0.1005 - val_f1_score_mod: 0.3038 - val_recall_mod: 0.1922 - val_precision_mod: 0.7293 - val_dur_error: 0.2701 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.10053\n",
      "25256/25256 - 139s - loss: 0.1085 - f1_score_mod: 0.3031 - recall_mod: 0.1917 - precision_mod: 0.7275 - dur_error: 0.3976 - maestro_dur_loss: 0.0199 - val_loss: 0.1054 - val_f1_score_mod: 0.3182 - val_recall_mod: 0.2044 - val_precision_mod: 0.7233 - val_dur_error: 0.3782 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.10053 to 0.10039, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 138s - loss: 0.1080 - f1_score_mod: 0.3074 - recall_mod: 0.1950 - precision_mod: 0.7288 - dur_error: 0.3979 - maestro_dur_loss: 0.0199 - val_loss: 0.1004 - val_f1_score_mod: 0.3164 - val_recall_mod: 0.2028 - val_precision_mod: 0.7208 - val_dur_error: 0.2789 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.10039 to 0.10008, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 142s - loss: 0.1077 - f1_score_mod: 0.3105 - recall_mod: 0.1976 - precision_mod: 0.7300 - dur_error: 0.3975 - maestro_dur_loss: 0.0199 - val_loss: 0.1001 - val_f1_score_mod: 0.3245 - val_recall_mod: 0.2109 - val_precision_mod: 0.7087 - val_dur_error: 0.2792 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.10008\n",
      "25256/25256 - 144s - loss: 0.1071 - f1_score_mod: 0.3118 - recall_mod: 0.1988 - precision_mod: 0.7274 - dur_error: 0.3922 - maestro_dur_loss: 0.0196 - val_loss: 0.1005 - val_f1_score_mod: 0.3240 - val_recall_mod: 0.2094 - val_precision_mod: 0.7276 - val_dur_error: 0.2901 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.10008 to 0.09980, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 145s - loss: 0.1067 - f1_score_mod: 0.3189 - recall_mod: 0.2044 - precision_mod: 0.7288 - dur_error: 0.3929 - maestro_dur_loss: 0.0196 - val_loss: 0.0998 - val_f1_score_mod: 0.3081 - val_recall_mod: 0.1943 - val_precision_mod: 0.7558 - val_dur_error: 0.2779 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09980\n",
      "25256/25256 - 135s - loss: 0.1062 - f1_score_mod: 0.3190 - recall_mod: 0.2041 - precision_mod: 0.7360 - dur_error: 0.3912 - maestro_dur_loss: 0.0196 - val_loss: 0.1014 - val_f1_score_mod: 0.3325 - val_recall_mod: 0.2165 - val_precision_mod: 0.7227 - val_dur_error: 0.3169 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09980 to 0.09870, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 136s - loss: 0.1056 - f1_score_mod: 0.3282 - recall_mod: 0.2115 - precision_mod: 0.7350 - dur_error: 0.3893 - maestro_dur_loss: 0.0195 - val_loss: 0.0987 - val_f1_score_mod: 0.3254 - val_recall_mod: 0.2095 - val_precision_mod: 0.7340 - val_dur_error: 0.2653 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.09870\n",
      "25256/25256 - 134s - loss: 0.1054 - f1_score_mod: 0.3302 - recall_mod: 0.2133 - precision_mod: 0.7353 - dur_error: 0.3917 - maestro_dur_loss: 0.0196 - val_loss: 0.0993 - val_f1_score_mod: 0.3283 - val_recall_mod: 0.2122 - val_precision_mod: 0.7300 - val_dur_error: 0.2806 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09870\n",
      "25256/25256 - 134s - loss: 0.1049 - f1_score_mod: 0.3333 - recall_mod: 0.2161 - precision_mod: 0.7337 - dur_error: 0.3876 - maestro_dur_loss: 0.0194 - val_loss: 0.0988 - val_f1_score_mod: 0.3395 - val_recall_mod: 0.2220 - val_precision_mod: 0.7249 - val_dur_error: 0.2654 - val_maestro_dur_loss: 0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.09870\n",
      "25256/25256 - 134s - loss: 0.1044 - f1_score_mod: 0.3384 - recall_mod: 0.2205 - precision_mod: 0.7308 - dur_error: 0.3861 - maestro_dur_loss: 0.0193 - val_loss: 0.1005 - val_f1_score_mod: 0.3423 - val_recall_mod: 0.2241 - val_precision_mod: 0.7297 - val_dur_error: 0.3077 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09870 to 0.09863, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1041 - f1_score_mod: 0.3410 - recall_mod: 0.2221 - precision_mod: 0.7385 - dur_error: 0.3867 - maestro_dur_loss: 0.0193 - val_loss: 0.0986 - val_f1_score_mod: 0.3419 - val_recall_mod: 0.2246 - val_precision_mod: 0.7219 - val_dur_error: 0.2755 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09863 to 0.09827, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1036 - f1_score_mod: 0.3450 - recall_mod: 0.2261 - precision_mod: 0.7327 - dur_error: 0.3829 - maestro_dur_loss: 0.0191 - val_loss: 0.0983 - val_f1_score_mod: 0.3423 - val_recall_mod: 0.2235 - val_precision_mod: 0.7341 - val_dur_error: 0.2771 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09827 to 0.09739, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1032 - f1_score_mod: 0.3494 - recall_mod: 0.2291 - precision_mod: 0.7402 - dur_error: 0.3832 - maestro_dur_loss: 0.0192 - val_loss: 0.0974 - val_f1_score_mod: 0.3380 - val_recall_mod: 0.2193 - val_precision_mod: 0.7408 - val_dur_error: 0.2700 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09739 to 0.09738, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cn_1pt0.h5\n",
      "25256/25256 - 134s - loss: 0.1026 - f1_score_mod: 0.3503 - recall_mod: 0.2302 - precision_mod: 0.7372 - dur_error: 0.3799 - maestro_dur_loss: 0.0190 - val_loss: 0.0974 - val_f1_score_mod: 0.3432 - val_recall_mod: 0.2235 - val_precision_mod: 0.7443 - val_dur_error: 0.2688 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09738\n",
      "25256/25256 - 139s - loss: 0.1022 - f1_score_mod: 0.3570 - recall_mod: 0.2360 - precision_mod: 0.7368 - dur_error: 0.3809 - maestro_dur_loss: 0.0190 - val_loss: 0.0979 - val_f1_score_mod: 0.3570 - val_recall_mod: 0.2381 - val_precision_mod: 0.7169 - val_dur_error: 0.2824 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 69/150\n",
      "Batch 12: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6656/25256 - 31s - loss: nan - f1_score_mod: 0.3587 - recall_mod: 0.2368 - precision_mod: 0.7428 - dur_error: 0.3762 - maestro_dur_loss: 0.0188\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipnorm = 1.0, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training here does a better job of generalizing to the validation set (as expected) but causes an earlier failure (in terms of training loss). For a visual comparison of performance depending on the dropout_rate, see Figures 5 and 6 from visualize_performance.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best performing model is ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cn_1pt0.h5. See the final section in visualize_performance.ipynb for an in-depth visual analysis of the performance of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
