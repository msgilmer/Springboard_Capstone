{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data we created in data_read_and_process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../train_and_val/X_train.npy')\n",
    "X_val = np.load('../train_and_val/X_val.npy')\n",
    "y_train = np.load('../train_and_val/y_train.npy')\n",
    "y_val = np.load('../train_and_val/y_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4):\n",
    "    \"\"\"Generate a keras Sequential model of the form as described in Figure 16 of\n",
    "    https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_lstm_nodes, return_sequences = True, input_shape = (16, 89,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(1, n_lstm_layers - 1):\n",
    "        model.add(LSTM(n_lstm_nodes, return_sequences = True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(n_lstm_nodes))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_lstm_nodes // 2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(n_dense_layers - 1):\n",
    "        model.add(Dense(n_lstm_nodes // 2))\n",
    "        model.add(Dropout(0.6))\n",
    "    model.add(Dense(89))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'RMSProp')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss and Metrics\n",
    "\n",
    "\\begin{equation*}\n",
    "bce\\_loss = \\frac{1}{N} (\\sum_{i=1}^{N} y_i log(p(y_i)) + (1 - y_i) log(1 - p(y_i)))\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "maestro\\_loss = 2 * Harshness \\lvert\\frac{d_{true} - d_{pred}}{d_{true} + d{_{pred}}}\\rvert\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\\begin{equation*}\n",
    "total\\_loss = MIN(2 * bce\\_loss, bce\\_loss + maestro\\_loss)\n",
    "\\end{equation*}\n",
    "\n",
    "where N = num_keys_piano, <b>Harshness</b> is a constant to be determined, and <b>d</b> gives the normalized duration. I'll call it the <b>Maestro Loss Function</b> since it pays special attention to the timing of the notes. It is usually composed of a Binary Cross Entropy Term with an additional term proportional to the relative error in duration between $d_{true}$ and $d_{pred}$. However, we limit the total_loss to be less than twice the bce_loss. We also define custom metrics, read the docstrings for their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def maestro_loss_wr(harshness): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last element (which repesents the duration). \n",
    "    The proportionality constant is the 'harshness' of the maestro in regards to\n",
    "    timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-1] * K.log(ypred[:, :-1]) + (1 - ytrue[:, :-1]) * \\\n",
    "                     K.log(1 - ypred[:, :-1]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, -1] elements will be zero\n",
    "            return bce_loss * 2     # This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "\n",
    "def precision_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified precision excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    pred_positives = K.sum(K.round(ypred[:, :-1]))\n",
    "    return true_positives / (pred_positives + K.epsilon())\n",
    "\n",
    "def recall_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified recall excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    poss_positives = K.sum(ytrue[:, :-1])\n",
    "    return true_positives / (poss_positives + K.epsilon())\n",
    "\n",
    "def f1_score_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified f1_score excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    precision = precision_mod(ytrue, ypred)\n",
    "    recall = recall_mod(ytrue, ypred)   \n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def dur_error(ytrue, ypred):\n",
    "    \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "    return 2 * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / (ytrue[:, -1] + ypred[:, -1] + \\\n",
    "                                                         K.epsilon())))\n",
    "\n",
    "def maestro_dur_loss_wr(harshness):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cols_dict(history):\n",
    "    \"\"\"return a mapping of desired column names to the corresponding columns in the\n",
    "    history dictionary (previously history.history where history is the return value\n",
    "    of model.train)\"\"\"\n",
    "    return {'maestro_loss': history['loss'], 'f1_score': history['f1_score_mod'], \\\n",
    " 'precision': history['precision_mod'], 'recall': history['recall_mod'], \\\n",
    " 'dur_error': history['dur_error'], 'dur_loss': history['maestro_dur_loss'], \\\n",
    " 'val_maestro_loss': history['val_loss'], 'val_f1_score': history['val_f1_score_mod'], \\\n",
    " 'val_precision': history['val_precision_mod'], 'val_recall': history['val_recall_mod'], \\\n",
    " 'val_dur_error': history['val_dur_error'], 'val_dur_loss': history['val_maestro_dur_loss']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a similar model to [this paper](https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972). To save time, let us use 2 LSTM layers and 1 Dense layer instead of (4 and 3 as in the paper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4, \\\n",
    "                     batch_size = 512, harshness = 0.05, lr = None, clipnorm = None, clipvalue = None, \\\n",
    "                     epochs = 150):\n",
    "    \"\"\"Train a model using the passed parameters, the data, and using the RMSprop optimizer. Write the\n",
    "    best model as a .h5 and a .csv containing columns for the training and validation custom loss and\n",
    "    metrics. Returns nothing.\"\"\"\n",
    "    model = lstm(n_lstm_layers = n_lstm_layers, n_dense_layers = n_dense_layers, \\\n",
    "                 n_lstm_nodes = n_lstm_nodes, dropout_rate = dropout_rate)\n",
    "\n",
    "    if (lr or clipnorm or clipvalue):\n",
    "        if (lr):          # It's required that the first argument to RMSprop is not None\n",
    "            opt = RMSprop(lr = lr, clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        elif (clipnorm):\n",
    "            opt = RMSprop(clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        else: # clipvalue\n",
    "            opt = RMSprop(clipvalue = clipvalue)\n",
    "    else:\n",
    "        opt = RMSprop()   # TypeError when all are None, so do this instead\n",
    "        \n",
    "    model.compile(loss = maestro_loss_wr(harshness), optimizer = opt, metrics = [f1_score_mod, recall_mod, \\\n",
    "                                                precision_mod, dur_error, maestro_dur_loss_wr(harshness)])\n",
    "    \n",
    "    filename = 'best_maestro_model_{0}_{1}_{2}_{3}'.format(n_lstm_layers, n_dense_layers, n_lstm_nodes, \\\n",
    "                                                          str(dropout_rate).replace('.', 'pt'))\n",
    "    if (lr):\n",
    "        filename += '_lr_{}'.format('%.0e' % Decimal(lr))\n",
    "    if (clipnorm):\n",
    "        filename += '_cn_{}'.format(str(clipnorm).replace('.', 'pt'))     \n",
    "    if (clipvalue):\n",
    "        filename += '_cv_{}'.format(str(clipvalue).replace('.', 'pt'))\n",
    "                                   \n",
    "    mc = ModelCheckpoint('../models/' + filename + '.h5', monitor = 'val_loss', mode = 'min', \\\n",
    "                                                         save_best_only = True, verbose = 1)\n",
    "                                   \n",
    "    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "    \n",
    "    # In most preliminary tests model training has failed at some point when the loss becomes NaN during\n",
    "    # validation\n",
    "    if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "        for key, value in history.history.items():\n",
    "            if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "                value.append(np.nan)\n",
    "                \n",
    "    df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "    df.index.name = 'Epochs'\n",
    "    df.to_csv('../model_data/' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14822, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 149s - loss: 0.2169 - f1_score_mod: 0.0142 - recall_mod: 0.0208 - precision_mod: 0.0933 - dur_error: 0.9866 - maestro_dur_loss: 0.0493 - val_loss: 0.1482 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5802 - val_maestro_dur_loss: 0.0290\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14822 to 0.13735, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 138s - loss: 0.1598 - f1_score_mod: 0.0054 - recall_mod: 0.0027 - precision_mod: 0.3476 - dur_error: 0.7020 - maestro_dur_loss: 0.0351 - val_loss: 0.1374 - val_f1_score_mod: 0.0137 - val_recall_mod: 0.0070 - val_precision_mod: 0.6813 - val_dur_error: 0.4574 - val_maestro_dur_loss: 0.0229\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13735 to 0.13232, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 133s - loss: 0.1490 - f1_score_mod: 0.0294 - recall_mod: 0.0153 - precision_mod: 0.5231 - dur_error: 0.6253 - maestro_dur_loss: 0.0313 - val_loss: 0.1323 - val_f1_score_mod: 0.0268 - val_recall_mod: 0.0137 - val_precision_mod: 0.7467 - val_dur_error: 0.4429 - val_maestro_dur_loss: 0.0221\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13232\n",
      "50/50 - 132s - loss: 0.1500 - f1_score_mod: 0.0491 - recall_mod: 0.0263 - precision_mod: 0.5224 - dur_error: 0.6787 - maestro_dur_loss: 0.0339 - val_loss: 0.1342 - val_f1_score_mod: 0.0326 - val_recall_mod: 0.0166 - val_precision_mod: 0.8471 - val_dur_error: 0.4639 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13232 to 0.12990, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 132s - loss: 0.1406 - f1_score_mod: 0.0607 - recall_mod: 0.0322 - precision_mod: 0.6104 - dur_error: 0.5871 - maestro_dur_loss: 0.0294 - val_loss: 0.1299 - val_f1_score_mod: 0.0506 - val_recall_mod: 0.0262 - val_precision_mod: 0.7702 - val_dur_error: 0.4681 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12990\n",
      "50/50 - 132s - loss: 0.1377 - f1_score_mod: 0.0770 - recall_mod: 0.0413 - precision_mod: 0.6272 - dur_error: 0.5783 - maestro_dur_loss: 0.0289 - val_loss: 0.1318 - val_f1_score_mod: 0.0984 - val_recall_mod: 0.0530 - val_precision_mod: 0.6939 - val_dur_error: 0.5502 - val_maestro_dur_loss: 0.0275\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12990 to 0.12203, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1352 - f1_score_mod: 0.0960 - recall_mod: 0.0523 - precision_mod: 0.6382 - dur_error: 0.5685 - maestro_dur_loss: 0.0284 - val_loss: 0.1220 - val_f1_score_mod: 0.1059 - val_recall_mod: 0.0570 - val_precision_mod: 0.7569 - val_dur_error: 0.4150 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12203\n",
      "50/50 - 131s - loss: 0.1324 - f1_score_mod: 0.1184 - recall_mod: 0.0656 - precision_mod: 0.6531 - dur_error: 0.5544 - maestro_dur_loss: 0.0277 - val_loss: 0.1244 - val_f1_score_mod: 0.1118 - val_recall_mod: 0.0603 - val_precision_mod: 0.7832 - val_dur_error: 0.4831 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12203\n",
      "50/50 - 134s - loss: 0.1304 - f1_score_mod: 0.1344 - recall_mod: 0.0754 - precision_mod: 0.6497 - dur_error: 0.5448 - maestro_dur_loss: 0.0272 - val_loss: 0.1249 - val_f1_score_mod: 0.1373 - val_recall_mod: 0.0756 - val_precision_mod: 0.7558 - val_dur_error: 0.5158 - val_maestro_dur_loss: 0.0258\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12203 to 0.11598, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1285 - f1_score_mod: 0.1523 - recall_mod: 0.0863 - precision_mod: 0.6701 - dur_error: 0.5391 - maestro_dur_loss: 0.0270 - val_loss: 0.1160 - val_f1_score_mod: 0.1617 - val_recall_mod: 0.0903 - val_precision_mod: 0.7818 - val_dur_error: 0.3748 - val_maestro_dur_loss: 0.0187\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11598\n",
      "50/50 - 131s - loss: 0.1264 - f1_score_mod: 0.1664 - recall_mod: 0.0953 - precision_mod: 0.6745 - dur_error: 0.5231 - maestro_dur_loss: 0.0262 - val_loss: 0.1223 - val_f1_score_mod: 0.1918 - val_recall_mod: 0.1110 - val_precision_mod: 0.7119 - val_dur_error: 0.5084 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11598 to 0.11333, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1251 - f1_score_mod: 0.1771 - recall_mod: 0.1022 - precision_mod: 0.6819 - dur_error: 0.5156 - maestro_dur_loss: 0.0258 - val_loss: 0.1133 - val_f1_score_mod: 0.1674 - val_recall_mod: 0.0937 - val_precision_mod: 0.7947 - val_dur_error: 0.3553 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11333\n",
      "50/50 - 131s - loss: 0.1237 - f1_score_mod: 0.1879 - recall_mod: 0.1093 - precision_mod: 0.6858 - dur_error: 0.5064 - maestro_dur_loss: 0.0253 - val_loss: 0.1133 - val_f1_score_mod: 0.1580 - val_recall_mod: 0.0876 - val_precision_mod: 0.8170 - val_dur_error: 0.3451 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11333\n",
      "50/50 - 131s - loss: 0.1229 - f1_score_mod: 0.1943 - recall_mod: 0.1134 - precision_mod: 0.7014 - dur_error: 0.5011 - maestro_dur_loss: 0.0251 - val_loss: 0.1137 - val_f1_score_mod: 0.2071 - val_recall_mod: 0.1200 - val_precision_mod: 0.7645 - val_dur_error: 0.3978 - val_maestro_dur_loss: 0.0199\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11333 to 0.10983, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1208 - f1_score_mod: 0.2066 - recall_mod: 0.1216 - precision_mod: 0.6997 - dur_error: 0.4831 - maestro_dur_loss: 0.0242 - val_loss: 0.1098 - val_f1_score_mod: 0.2396 - val_recall_mod: 0.1434 - val_precision_mod: 0.7333 - val_dur_error: 0.3333 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10983\n",
      "50/50 - 131s - loss: 0.1197 - f1_score_mod: 0.2177 - recall_mod: 0.1291 - precision_mod: 0.7055 - dur_error: 0.4820 - maestro_dur_loss: 0.0241 - val_loss: 0.1171 - val_f1_score_mod: 0.2115 - val_recall_mod: 0.1222 - val_precision_mod: 0.7927 - val_dur_error: 0.4790 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10983\n",
      "50/50 - 132s - loss: 0.1186 - f1_score_mod: 0.2264 - recall_mod: 0.1353 - precision_mod: 0.7063 - dur_error: 0.4717 - maestro_dur_loss: 0.0236 - val_loss: 0.1125 - val_f1_score_mod: 0.2279 - val_recall_mod: 0.1337 - val_precision_mod: 0.7749 - val_dur_error: 0.4120 - val_maestro_dur_loss: 0.0206\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10983 to 0.10811, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 132s - loss: 0.1181 - f1_score_mod: 0.2348 - recall_mod: 0.1409 - precision_mod: 0.7140 - dur_error: 0.4758 - maestro_dur_loss: 0.0238 - val_loss: 0.1081 - val_f1_score_mod: 0.2532 - val_recall_mod: 0.1519 - val_precision_mod: 0.7635 - val_dur_error: 0.3319 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10811\n",
      "50/50 - 132s - loss: 0.1165 - f1_score_mod: 0.2426 - recall_mod: 0.1466 - precision_mod: 0.7158 - dur_error: 0.4602 - maestro_dur_loss: 0.0230 - val_loss: 0.1095 - val_f1_score_mod: 0.2538 - val_recall_mod: 0.1520 - val_precision_mod: 0.7728 - val_dur_error: 0.3729 - val_maestro_dur_loss: 0.0186\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10811\n",
      "50/50 - 131s - loss: 0.1155 - f1_score_mod: 0.2534 - recall_mod: 0.1543 - precision_mod: 0.7192 - dur_error: 0.4565 - maestro_dur_loss: 0.0228 - val_loss: 0.1084 - val_f1_score_mod: 0.2758 - val_recall_mod: 0.1689 - val_precision_mod: 0.7546 - val_dur_error: 0.3645 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10811\n",
      "50/50 - 132s - loss: 0.1146 - f1_score_mod: 0.2572 - recall_mod: 0.1572 - precision_mod: 0.7162 - dur_error: 0.4498 - maestro_dur_loss: 0.0225 - val_loss: 0.1096 - val_f1_score_mod: 0.2911 - val_recall_mod: 0.1813 - val_precision_mod: 0.7408 - val_dur_error: 0.3985 - val_maestro_dur_loss: 0.0199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10811\n",
      "50/50 - 131s - loss: 0.1136 - f1_score_mod: 0.2653 - recall_mod: 0.1629 - precision_mod: 0.7232 - dur_error: 0.4448 - maestro_dur_loss: 0.0222 - val_loss: 0.1087 - val_f1_score_mod: 0.2944 - val_recall_mod: 0.1839 - val_precision_mod: 0.7412 - val_dur_error: 0.3894 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10811 to 0.10599, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1122 - f1_score_mod: 0.2738 - recall_mod: 0.1692 - precision_mod: 0.7261 - dur_error: 0.4359 - maestro_dur_loss: 0.0218 - val_loss: 0.1060 - val_f1_score_mod: 0.2943 - val_recall_mod: 0.1828 - val_precision_mod: 0.7582 - val_dur_error: 0.3455 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10599 to 0.10237, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1119 - f1_score_mod: 0.2792 - recall_mod: 0.1733 - precision_mod: 0.7257 - dur_error: 0.4376 - maestro_dur_loss: 0.0219 - val_loss: 0.1024 - val_f1_score_mod: 0.2959 - val_recall_mod: 0.1838 - val_precision_mod: 0.7626 - val_dur_error: 0.2873 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10237\n",
      "50/50 - 131s - loss: 0.1107 - f1_score_mod: 0.2877 - recall_mod: 0.1802 - precision_mod: 0.7226 - dur_error: 0.4320 - maestro_dur_loss: 0.0216 - val_loss: 0.1047 - val_f1_score_mod: 0.3049 - val_recall_mod: 0.1915 - val_precision_mod: 0.7504 - val_dur_error: 0.3393 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10237\n",
      "50/50 - 130s - loss: 0.1101 - f1_score_mod: 0.2913 - recall_mod: 0.1828 - precision_mod: 0.7228 - dur_error: 0.4280 - maestro_dur_loss: 0.0214 - val_loss: 0.1108 - val_f1_score_mod: 0.3134 - val_recall_mod: 0.1985 - val_precision_mod: 0.7453 - val_dur_error: 0.4619 - val_maestro_dur_loss: 0.0231\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10237\n",
      "50/50 - 130s - loss: 0.1092 - f1_score_mod: 0.2951 - recall_mod: 0.1857 - precision_mod: 0.7269 - dur_error: 0.4228 - maestro_dur_loss: 0.0211 - val_loss: 0.1067 - val_f1_score_mod: 0.3251 - val_recall_mod: 0.2089 - val_precision_mod: 0.7352 - val_dur_error: 0.3857 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10237\n",
      "50/50 - 131s - loss: 0.1085 - f1_score_mod: 0.3017 - recall_mod: 0.1908 - precision_mod: 0.7266 - dur_error: 0.4223 - maestro_dur_loss: 0.0211 - val_loss: 0.1060 - val_f1_score_mod: 0.3326 - val_recall_mod: 0.2159 - val_precision_mod: 0.7253 - val_dur_error: 0.3850 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10237 to 0.10049, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1073 - f1_score_mod: 0.3093 - recall_mod: 0.1966 - precision_mod: 0.7315 - dur_error: 0.4127 - maestro_dur_loss: 0.0206 - val_loss: 0.1005 - val_f1_score_mod: 0.3246 - val_recall_mod: 0.2062 - val_precision_mod: 0.7633 - val_dur_error: 0.2940 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10049\n",
      "50/50 - 131s - loss: 0.1066 - f1_score_mod: 0.3183 - recall_mod: 0.2041 - precision_mod: 0.7283 - dur_error: 0.4133 - maestro_dur_loss: 0.0207 - val_loss: 0.1060 - val_f1_score_mod: 0.3307 - val_recall_mod: 0.2115 - val_precision_mod: 0.7599 - val_dur_error: 0.4072 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10049 to 0.09963, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1058 - f1_score_mod: 0.3244 - recall_mod: 0.2090 - precision_mod: 0.7328 - dur_error: 0.4104 - maestro_dur_loss: 0.0205 - val_loss: 0.0996 - val_f1_score_mod: 0.3346 - val_recall_mod: 0.2149 - val_precision_mod: 0.7572 - val_dur_error: 0.2886 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09963 to 0.09902, saving model to ../models/best_maestro_model_2_1_512_0pt4.h5\n",
      "50/50 - 131s - loss: 0.1047 - f1_score_mod: 0.3343 - recall_mod: 0.2170 - precision_mod: 0.7346 - dur_error: 0.4058 - maestro_dur_loss: 0.0203 - val_loss: 0.0990 - val_f1_score_mod: 0.3370 - val_recall_mod: 0.2169 - val_precision_mod: 0.7561 - val_dur_error: 0.2797 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09902\n",
      "50/50 - 131s - loss: 0.1039 - f1_score_mod: 0.3377 - recall_mod: 0.2196 - precision_mod: 0.7335 - dur_error: 0.4017 - maestro_dur_loss: 0.0201 - val_loss: 0.0995 - val_f1_score_mod: 0.3437 - val_recall_mod: 0.2216 - val_precision_mod: 0.7673 - val_dur_error: 0.3070 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09902\n",
      "50/50 - 131s - loss: 0.1032 - f1_score_mod: 0.3434 - recall_mod: 0.2245 - precision_mod: 0.7371 - dur_error: 0.4008 - maestro_dur_loss: 0.0200 - val_loss: 0.1003 - val_f1_score_mod: 0.3498 - val_recall_mod: 0.2267 - val_precision_mod: 0.7678 - val_dur_error: 0.3238 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09902\n",
      "50/50 - 131s - loss: 0.1025 - f1_score_mod: 0.3529 - recall_mod: 0.2323 - precision_mod: 0.7371 - dur_error: 0.4031 - maestro_dur_loss: 0.0202 - val_loss: 0.1030 - val_f1_score_mod: 0.3794 - val_recall_mod: 0.2582 - val_precision_mod: 0.7168 - val_dur_error: 0.3841 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 36/150\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09902\n",
      "50/50 - 131s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll refer to the above model as the 'base_model'. Looks like we get a NaN loss while the model was still improving. This seems to be case of exploding gradients. To combat this, we can try lowering the learning rate as suggested [here](https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14800, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 140s - loss: 0.2446 - f1_score_mod: 0.0299 - recall_mod: 0.0387 - precision_mod: 0.0858 - dur_error: 1.0080 - maestro_dur_loss: 0.0504 - val_loss: 0.1480 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5389 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14800 to 0.14084, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1679 - f1_score_mod: 0.0054 - recall_mod: 0.0028 - precision_mod: 0.1578 - dur_error: 0.7188 - maestro_dur_loss: 0.0359 - val_loss: 0.1408 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4622 - val_maestro_dur_loss: 0.0231\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14084 to 0.13865, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1546 - f1_score_mod: 0.0092 - recall_mod: 0.0047 - precision_mod: 0.3701 - dur_error: 0.6363 - maestro_dur_loss: 0.0318 - val_loss: 0.1386 - val_f1_score_mod: 5.9147e-05 - val_recall_mod: 2.9593e-05 - val_precision_mod: 0.0455 - val_dur_error: 0.4531 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13865 to 0.13746, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 134s - loss: 0.1475 - f1_score_mod: 0.0252 - recall_mod: 0.0130 - precision_mod: 0.4932 - dur_error: 0.6008 - maestro_dur_loss: 0.0300 - val_loss: 0.1375 - val_f1_score_mod: 0.0288 - val_recall_mod: 0.0147 - val_precision_mod: 0.8119 - val_dur_error: 0.5573 - val_maestro_dur_loss: 0.0279\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13746 to 0.13672, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1432 - f1_score_mod: 0.0470 - recall_mod: 0.0247 - precision_mod: 0.5716 - dur_error: 0.5857 - maestro_dur_loss: 0.0293 - val_loss: 0.1367 - val_f1_score_mod: 0.0508 - val_recall_mod: 0.0264 - val_precision_mod: 0.7198 - val_dur_error: 0.5837 - val_maestro_dur_loss: 0.0292\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13672 to 0.12753, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1399 - f1_score_mod: 0.0678 - recall_mod: 0.0365 - precision_mod: 0.5818 - dur_error: 0.5727 - maestro_dur_loss: 0.0286 - val_loss: 0.1275 - val_f1_score_mod: 0.0824 - val_recall_mod: 0.0437 - val_precision_mod: 0.7437 - val_dur_error: 0.4609 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12753 to 0.12507, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1360 - f1_score_mod: 0.0812 - recall_mod: 0.0437 - precision_mod: 0.6127 - dur_error: 0.5474 - maestro_dur_loss: 0.0274 - val_loss: 0.1251 - val_f1_score_mod: 0.1156 - val_recall_mod: 0.0629 - val_precision_mod: 0.7199 - val_dur_error: 0.4400 - val_maestro_dur_loss: 0.0220\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12507 to 0.12487, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1337 - f1_score_mod: 0.1003 - recall_mod: 0.0547 - precision_mod: 0.6334 - dur_error: 0.5374 - maestro_dur_loss: 0.0269 - val_loss: 0.1249 - val_f1_score_mod: 0.1250 - val_recall_mod: 0.0688 - val_precision_mod: 0.6965 - val_dur_error: 0.4495 - val_maestro_dur_loss: 0.0225\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12487\n",
      "50/50 - 134s - loss: 0.1321 - f1_score_mod: 0.1154 - recall_mod: 0.0637 - precision_mod: 0.6499 - dur_error: 0.5371 - maestro_dur_loss: 0.0269 - val_loss: 0.1251 - val_f1_score_mod: 0.1505 - val_recall_mod: 0.0842 - val_precision_mod: 0.7169 - val_dur_error: 0.4836 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12487 to 0.11728, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1299 - f1_score_mod: 0.1320 - recall_mod: 0.0736 - precision_mod: 0.6620 - dur_error: 0.5218 - maestro_dur_loss: 0.0261 - val_loss: 0.1173 - val_f1_score_mod: 0.1536 - val_recall_mod: 0.0860 - val_precision_mod: 0.7240 - val_dur_error: 0.3641 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11728 to 0.11676, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1284 - f1_score_mod: 0.1445 - recall_mod: 0.0814 - precision_mod: 0.6660 - dur_error: 0.5145 - maestro_dur_loss: 0.0257 - val_loss: 0.1168 - val_f1_score_mod: 0.1506 - val_recall_mod: 0.0837 - val_precision_mod: 0.7611 - val_dur_error: 0.3732 - val_maestro_dur_loss: 0.0187\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11676\n",
      "50/50 - 132s - loss: 0.1263 - f1_score_mod: 0.1540 - recall_mod: 0.0874 - precision_mod: 0.6650 - dur_error: 0.4973 - maestro_dur_loss: 0.0249 - val_loss: 0.1206 - val_f1_score_mod: 0.1832 - val_recall_mod: 0.1048 - val_precision_mod: 0.7390 - val_dur_error: 0.4487 - val_maestro_dur_loss: 0.0224\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11676\n",
      "50/50 - 131s - loss: 0.1252 - f1_score_mod: 0.1612 - recall_mod: 0.0920 - precision_mod: 0.6690 - dur_error: 0.4890 - maestro_dur_loss: 0.0245 - val_loss: 0.1201 - val_f1_score_mod: 0.1974 - val_recall_mod: 0.1142 - val_precision_mod: 0.7322 - val_dur_error: 0.4634 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11676\n",
      "50/50 - 131s - loss: 0.1238 - f1_score_mod: 0.1718 - recall_mod: 0.0986 - precision_mod: 0.6807 - dur_error: 0.4804 - maestro_dur_loss: 0.0240 - val_loss: 0.1175 - val_f1_score_mod: 0.2261 - val_recall_mod: 0.1349 - val_precision_mod: 0.7034 - val_dur_error: 0.4137 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11676\n",
      "50/50 - 131s - loss: 0.1227 - f1_score_mod: 0.1823 - recall_mod: 0.1055 - precision_mod: 0.6865 - dur_error: 0.4694 - maestro_dur_loss: 0.0235 - val_loss: 0.1185 - val_f1_score_mod: 0.2050 - val_recall_mod: 0.1206 - val_precision_mod: 0.6862 - val_dur_error: 0.4195 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11676 to 0.11249, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1214 - f1_score_mod: 0.1870 - recall_mod: 0.1084 - precision_mod: 0.6927 - dur_error: 0.4587 - maestro_dur_loss: 0.0229 - val_loss: 0.1125 - val_f1_score_mod: 0.2074 - val_recall_mod: 0.1201 - val_precision_mod: 0.7652 - val_dur_error: 0.3580 - val_maestro_dur_loss: 0.0179\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11249\n",
      "50/50 - 131s - loss: 0.1204 - f1_score_mod: 0.1953 - recall_mod: 0.1140 - precision_mod: 0.6937 - dur_error: 0.4523 - maestro_dur_loss: 0.0226 - val_loss: 0.1158 - val_f1_score_mod: 0.2309 - val_recall_mod: 0.1367 - val_precision_mod: 0.7468 - val_dur_error: 0.4270 - val_maestro_dur_loss: 0.0213\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11249\n",
      "50/50 - 131s - loss: 0.1195 - f1_score_mod: 0.2049 - recall_mod: 0.1203 - precision_mod: 0.7009 - dur_error: 0.4466 - maestro_dur_loss: 0.0223 - val_loss: 0.1185 - val_f1_score_mod: 0.2175 - val_recall_mod: 0.1270 - val_precision_mod: 0.7624 - val_dur_error: 0.4985 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11249\n",
      "50/50 - 131s - loss: 0.1188 - f1_score_mod: 0.2081 - recall_mod: 0.1225 - precision_mod: 0.7004 - dur_error: 0.4451 - maestro_dur_loss: 0.0223 - val_loss: 0.1143 - val_f1_score_mod: 0.2515 - val_recall_mod: 0.1531 - val_precision_mod: 0.7067 - val_dur_error: 0.4204 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11249\n",
      "50/50 - 132s - loss: 0.1176 - f1_score_mod: 0.2161 - recall_mod: 0.1284 - precision_mod: 0.6937 - dur_error: 0.4338 - maestro_dur_loss: 0.0217 - val_loss: 0.1145 - val_f1_score_mod: 0.2448 - val_recall_mod: 0.1467 - val_precision_mod: 0.7423 - val_dur_error: 0.4300 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.11249 to 0.10817, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1171 - f1_score_mod: 0.2233 - recall_mod: 0.1329 - precision_mod: 0.7075 - dur_error: 0.4364 - maestro_dur_loss: 0.0218 - val_loss: 0.1082 - val_f1_score_mod: 0.2468 - val_recall_mod: 0.1479 - val_precision_mod: 0.7496 - val_dur_error: 0.3213 - val_maestro_dur_loss: 0.0161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10817\n",
      "50/50 - 132s - loss: 0.1162 - f1_score_mod: 0.2287 - recall_mod: 0.1368 - precision_mod: 0.7081 - dur_error: 0.4292 - maestro_dur_loss: 0.0215 - val_loss: 0.1086 - val_f1_score_mod: 0.2292 - val_recall_mod: 0.1345 - val_precision_mod: 0.7795 - val_dur_error: 0.3310 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10817 to 0.10614, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1160 - f1_score_mod: 0.2335 - recall_mod: 0.1401 - precision_mod: 0.7071 - dur_error: 0.4304 - maestro_dur_loss: 0.0215 - val_loss: 0.1061 - val_f1_score_mod: 0.2422 - val_recall_mod: 0.1437 - val_precision_mod: 0.7750 - val_dur_error: 0.2911 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10614\n",
      "50/50 - 131s - loss: 0.1153 - f1_score_mod: 0.2380 - recall_mod: 0.1433 - precision_mod: 0.7115 - dur_error: 0.4269 - maestro_dur_loss: 0.0213 - val_loss: 0.1065 - val_f1_score_mod: 0.2483 - val_recall_mod: 0.1483 - val_precision_mod: 0.7679 - val_dur_error: 0.2966 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.10614 to 0.10558, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1143 - f1_score_mod: 0.2451 - recall_mod: 0.1483 - precision_mod: 0.7163 - dur_error: 0.4197 - maestro_dur_loss: 0.0210 - val_loss: 0.1056 - val_f1_score_mod: 0.2742 - val_recall_mod: 0.1685 - val_precision_mod: 0.7382 - val_dur_error: 0.2996 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10558\n",
      "50/50 - 131s - loss: 0.1137 - f1_score_mod: 0.2511 - recall_mod: 0.1528 - precision_mod: 0.7132 - dur_error: 0.4187 - maestro_dur_loss: 0.0209 - val_loss: 0.1087 - val_f1_score_mod: 0.2603 - val_recall_mod: 0.1567 - val_precision_mod: 0.7735 - val_dur_error: 0.3684 - val_maestro_dur_loss: 0.0184\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10558 to 0.10502, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1129 - f1_score_mod: 0.2553 - recall_mod: 0.1554 - precision_mod: 0.7214 - dur_error: 0.4122 - maestro_dur_loss: 0.0206 - val_loss: 0.1050 - val_f1_score_mod: 0.2653 - val_recall_mod: 0.1609 - val_precision_mod: 0.7629 - val_dur_error: 0.2909 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10502 to 0.10444, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1124 - f1_score_mod: 0.2619 - recall_mod: 0.1603 - precision_mod: 0.7200 - dur_error: 0.4127 - maestro_dur_loss: 0.0206 - val_loss: 0.1044 - val_f1_score_mod: 0.2895 - val_recall_mod: 0.1810 - val_precision_mod: 0.7257 - val_dur_error: 0.2909 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10444 to 0.10390, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1119 - f1_score_mod: 0.2685 - recall_mod: 0.1651 - precision_mod: 0.7268 - dur_error: 0.4098 - maestro_dur_loss: 0.0205 - val_loss: 0.1039 - val_f1_score_mod: 0.2756 - val_recall_mod: 0.1679 - val_precision_mod: 0.7738 - val_dur_error: 0.2900 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10390\n",
      "50/50 - 131s - loss: 0.1115 - f1_score_mod: 0.2725 - recall_mod: 0.1685 - precision_mod: 0.7205 - dur_error: 0.4114 - maestro_dur_loss: 0.0206 - val_loss: 0.1079 - val_f1_score_mod: 0.3004 - val_recall_mod: 0.1877 - val_precision_mod: 0.7540 - val_dur_error: 0.3772 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10390 to 0.10265, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.1111 - f1_score_mod: 0.2759 - recall_mod: 0.1709 - precision_mod: 0.7236 - dur_error: 0.4100 - maestro_dur_loss: 0.0205 - val_loss: 0.1026 - val_f1_score_mod: 0.2856 - val_recall_mod: 0.1756 - val_precision_mod: 0.7687 - val_dur_error: 0.2747 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10265\n",
      "50/50 - 131s - loss: 0.1102 - f1_score_mod: 0.2823 - recall_mod: 0.1757 - precision_mod: 0.7258 - dur_error: 0.4016 - maestro_dur_loss: 0.0201 - val_loss: 0.1048 - val_f1_score_mod: 0.2961 - val_recall_mod: 0.1844 - val_precision_mod: 0.7538 - val_dur_error: 0.3319 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10265\n",
      "50/50 - 131s - loss: 0.1096 - f1_score_mod: 0.2849 - recall_mod: 0.1777 - precision_mod: 0.7251 - dur_error: 0.3996 - maestro_dur_loss: 0.0200 - val_loss: 0.1039 - val_f1_score_mod: 0.3039 - val_recall_mod: 0.1897 - val_precision_mod: 0.7659 - val_dur_error: 0.3107 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10265 to 0.10180, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1095 - f1_score_mod: 0.2894 - recall_mod: 0.1812 - precision_mod: 0.7261 - dur_error: 0.4056 - maestro_dur_loss: 0.0203 - val_loss: 0.1018 - val_f1_score_mod: 0.3080 - val_recall_mod: 0.1931 - val_precision_mod: 0.7630 - val_dur_error: 0.2797 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.10180 to 0.10112, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 133s - loss: 0.1089 - f1_score_mod: 0.2932 - recall_mod: 0.1836 - precision_mod: 0.7321 - dur_error: 0.3993 - maestro_dur_loss: 0.0200 - val_loss: 0.1011 - val_f1_score_mod: 0.3152 - val_recall_mod: 0.1994 - val_precision_mod: 0.7542 - val_dur_error: 0.2755 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10112\n",
      "50/50 - 131s - loss: 0.1082 - f1_score_mod: 0.2972 - recall_mod: 0.1868 - precision_mod: 0.7321 - dur_error: 0.3969 - maestro_dur_loss: 0.0198 - val_loss: 0.1039 - val_f1_score_mod: 0.3120 - val_recall_mod: 0.1964 - val_precision_mod: 0.7615 - val_dur_error: 0.3291 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10112\n",
      "50/50 - 131s - loss: 0.1077 - f1_score_mod: 0.3023 - recall_mod: 0.1909 - precision_mod: 0.7328 - dur_error: 0.3960 - maestro_dur_loss: 0.0198 - val_loss: 0.1014 - val_f1_score_mod: 0.3132 - val_recall_mod: 0.1974 - val_precision_mod: 0.7598 - val_dur_error: 0.2851 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.10112\n",
      "50/50 - 130s - loss: 0.1070 - f1_score_mod: 0.3035 - recall_mod: 0.1922 - precision_mod: 0.7282 - dur_error: 0.3906 - maestro_dur_loss: 0.0195 - val_loss: 0.1059 - val_f1_score_mod: 0.3123 - val_recall_mod: 0.1966 - val_precision_mod: 0.7619 - val_dur_error: 0.3839 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.10112 to 0.09983, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1067 - f1_score_mod: 0.3072 - recall_mod: 0.1948 - precision_mod: 0.7313 - dur_error: 0.3915 - maestro_dur_loss: 0.0196 - val_loss: 0.0998 - val_f1_score_mod: 0.3237 - val_recall_mod: 0.2060 - val_precision_mod: 0.7583 - val_dur_error: 0.2697 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09983\n",
      "50/50 - 130s - loss: 0.1062 - f1_score_mod: 0.3147 - recall_mod: 0.2006 - precision_mod: 0.7344 - dur_error: 0.3921 - maestro_dur_loss: 0.0196 - val_loss: 0.1006 - val_f1_score_mod: 0.3247 - val_recall_mod: 0.2071 - val_precision_mod: 0.7544 - val_dur_error: 0.2848 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09983 to 0.09959, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1055 - f1_score_mod: 0.3205 - recall_mod: 0.2056 - precision_mod: 0.7312 - dur_error: 0.3861 - maestro_dur_loss: 0.0193 - val_loss: 0.0996 - val_f1_score_mod: 0.3409 - val_recall_mod: 0.2211 - val_precision_mod: 0.7460 - val_dur_error: 0.2762 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09959 to 0.09911, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 133s - loss: 0.1051 - f1_score_mod: 0.3255 - recall_mod: 0.2092 - precision_mod: 0.7372 - dur_error: 0.3897 - maestro_dur_loss: 0.0195 - val_loss: 0.0991 - val_f1_score_mod: 0.3459 - val_recall_mod: 0.2258 - val_precision_mod: 0.7405 - val_dur_error: 0.2722 - val_maestro_dur_loss: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09911\n",
      "50/50 - 132s - loss: 0.1043 - f1_score_mod: 0.3306 - recall_mod: 0.2135 - precision_mod: 0.7372 - dur_error: 0.3818 - maestro_dur_loss: 0.0191 - val_loss: 0.0998 - val_f1_score_mod: 0.3395 - val_recall_mod: 0.2200 - val_precision_mod: 0.7448 - val_dur_error: 0.2861 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09911\n",
      "50/50 - 130s - loss: 0.1040 - f1_score_mod: 0.3369 - recall_mod: 0.2188 - precision_mod: 0.7367 - dur_error: 0.3840 - maestro_dur_loss: 0.0192 - val_loss: 0.0993 - val_f1_score_mod: 0.3318 - val_recall_mod: 0.2117 - val_precision_mod: 0.7682 - val_dur_error: 0.2768 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09911\n",
      "50/50 - 131s - loss: 0.1033 - f1_score_mod: 0.3399 - recall_mod: 0.2210 - precision_mod: 0.7419 - dur_error: 0.3783 - maestro_dur_loss: 0.0189 - val_loss: 0.1033 - val_f1_score_mod: 0.3463 - val_recall_mod: 0.2244 - val_precision_mod: 0.7593 - val_dur_error: 0.3677 - val_maestro_dur_loss: 0.0184\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09911\n",
      "50/50 - 131s - loss: 0.1029 - f1_score_mod: 0.3431 - recall_mod: 0.2240 - precision_mod: 0.7372 - dur_error: 0.3801 - maestro_dur_loss: 0.0190 - val_loss: 0.1021 - val_f1_score_mod: 0.3573 - val_recall_mod: 0.2346 - val_precision_mod: 0.7511 - val_dur_error: 0.3561 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09911 to 0.09831, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1023 - f1_score_mod: 0.3478 - recall_mod: 0.2277 - precision_mod: 0.7393 - dur_error: 0.3755 - maestro_dur_loss: 0.0188 - val_loss: 0.0983 - val_f1_score_mod: 0.3501 - val_recall_mod: 0.2275 - val_precision_mod: 0.7619 - val_dur_error: 0.2750 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09831 to 0.09790, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.1019 - f1_score_mod: 0.3522 - recall_mod: 0.2314 - precision_mod: 0.7421 - dur_error: 0.3778 - maestro_dur_loss: 0.0189 - val_loss: 0.0979 - val_f1_score_mod: 0.3612 - val_recall_mod: 0.2376 - val_precision_mod: 0.7546 - val_dur_error: 0.2777 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09790\n",
      "50/50 - 131s - loss: 0.1011 - f1_score_mod: 0.3557 - recall_mod: 0.2342 - precision_mod: 0.7432 - dur_error: 0.3726 - maestro_dur_loss: 0.0186 - val_loss: 0.0990 - val_f1_score_mod: 0.3687 - val_recall_mod: 0.2451 - val_precision_mod: 0.7456 - val_dur_error: 0.2989 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09790\n",
      "50/50 - 131s - loss: 0.1011 - f1_score_mod: 0.3614 - recall_mod: 0.2393 - precision_mod: 0.7404 - dur_error: 0.3779 - maestro_dur_loss: 0.0189 - val_loss: 0.0989 - val_f1_score_mod: 0.3670 - val_recall_mod: 0.2440 - val_precision_mod: 0.7421 - val_dur_error: 0.2992 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09790\n",
      "50/50 - 131s - loss: 0.1005 - f1_score_mod: 0.3653 - recall_mod: 0.2427 - precision_mod: 0.7417 - dur_error: 0.3763 - maestro_dur_loss: 0.0188 - val_loss: 0.0988 - val_f1_score_mod: 0.3765 - val_recall_mod: 0.2528 - val_precision_mod: 0.7392 - val_dur_error: 0.2979 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09790 to 0.09654, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.0998 - f1_score_mod: 0.3706 - recall_mod: 0.2475 - precision_mod: 0.7420 - dur_error: 0.3721 - maestro_dur_loss: 0.0186 - val_loss: 0.0965 - val_f1_score_mod: 0.3783 - val_recall_mod: 0.2541 - val_precision_mod: 0.7409 - val_dur_error: 0.2697 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09654\n",
      "50/50 - 130s - loss: 0.0994 - f1_score_mod: 0.3733 - recall_mod: 0.2497 - precision_mod: 0.7430 - dur_error: 0.3697 - maestro_dur_loss: 0.0185 - val_loss: 0.0971 - val_f1_score_mod: 0.3745 - val_recall_mod: 0.2493 - val_precision_mod: 0.7536 - val_dur_error: 0.2801 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09654\n",
      "50/50 - 131s - loss: 0.0988 - f1_score_mod: 0.3772 - recall_mod: 0.2528 - precision_mod: 0.7458 - dur_error: 0.3667 - maestro_dur_loss: 0.0183 - val_loss: 0.0969 - val_f1_score_mod: 0.3875 - val_recall_mod: 0.2631 - val_precision_mod: 0.7365 - val_dur_error: 0.2806 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09654 to 0.09647, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 134s - loss: 0.0987 - f1_score_mod: 0.3813 - recall_mod: 0.2566 - precision_mod: 0.7455 - dur_error: 0.3725 - maestro_dur_loss: 0.0186 - val_loss: 0.0965 - val_f1_score_mod: 0.3891 - val_recall_mod: 0.2646 - val_precision_mod: 0.7364 - val_dur_error: 0.2786 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09647\n",
      "50/50 - 132s - loss: 0.0978 - f1_score_mod: 0.3861 - recall_mod: 0.2608 - precision_mod: 0.7465 - dur_error: 0.3673 - maestro_dur_loss: 0.0184 - val_loss: 0.0994 - val_f1_score_mod: 0.3820 - val_recall_mod: 0.2575 - val_precision_mod: 0.7405 - val_dur_error: 0.3366 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09647 to 0.09561, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.0971 - f1_score_mod: 0.3896 - recall_mod: 0.2641 - precision_mod: 0.7450 - dur_error: 0.3657 - maestro_dur_loss: 0.0183 - val_loss: 0.0956 - val_f1_score_mod: 0.3926 - val_recall_mod: 0.2666 - val_precision_mod: 0.7453 - val_dur_error: 0.2609 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09561 to 0.09530, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.0968 - f1_score_mod: 0.3935 - recall_mod: 0.2677 - precision_mod: 0.7457 - dur_error: 0.3645 - maestro_dur_loss: 0.0182 - val_loss: 0.0953 - val_f1_score_mod: 0.3928 - val_recall_mod: 0.2672 - val_precision_mod: 0.7424 - val_dur_error: 0.2597 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09530\n",
      "50/50 - 131s - loss: 0.0964 - f1_score_mod: 0.3957 - recall_mod: 0.2698 - precision_mod: 0.7447 - dur_error: 0.3632 - maestro_dur_loss: 0.0182 - val_loss: 0.0955 - val_f1_score_mod: 0.3964 - val_recall_mod: 0.2709 - val_precision_mod: 0.7401 - val_dur_error: 0.2739 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09530\n",
      "50/50 - 132s - loss: 0.0958 - f1_score_mod: 0.4026 - recall_mod: 0.2755 - precision_mod: 0.7505 - dur_error: 0.3629 - maestro_dur_loss: 0.0181 - val_loss: 0.0989 - val_f1_score_mod: 0.4073 - val_recall_mod: 0.2851 - val_precision_mod: 0.7138 - val_dur_error: 0.3370 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.09530\n",
      "50/50 - 131s - loss: 0.0951 - f1_score_mod: 0.4056 - recall_mod: 0.2791 - precision_mod: 0.7443 - dur_error: 0.3600 - maestro_dur_loss: 0.0180 - val_loss: 0.0968 - val_f1_score_mod: 0.4041 - val_recall_mod: 0.2785 - val_precision_mod: 0.7374 - val_dur_error: 0.3099 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09530\n",
      "50/50 - 131s - loss: 0.0949 - f1_score_mod: 0.4105 - recall_mod: 0.2837 - precision_mod: 0.7453 - dur_error: 0.3621 - maestro_dur_loss: 0.0181 - val_loss: 0.0976 - val_f1_score_mod: 0.4100 - val_recall_mod: 0.2862 - val_precision_mod: 0.7236 - val_dur_error: 0.3181 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.09530\n",
      "50/50 - 132s - loss: 0.0940 - f1_score_mod: 0.4146 - recall_mod: 0.2873 - precision_mod: 0.7484 - dur_error: 0.3573 - maestro_dur_loss: 0.0179 - val_loss: 0.0966 - val_f1_score_mod: 0.4121 - val_recall_mod: 0.2865 - val_precision_mod: 0.7348 - val_dur_error: 0.3086 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09530 to 0.09460, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.0936 - f1_score_mod: 0.4175 - recall_mod: 0.2906 - precision_mod: 0.7433 - dur_error: 0.3562 - maestro_dur_loss: 0.0178 - val_loss: 0.0946 - val_f1_score_mod: 0.4108 - val_recall_mod: 0.2876 - val_precision_mod: 0.7199 - val_dur_error: 0.2615 - val_maestro_dur_loss: 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09460 to 0.09453, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 132s - loss: 0.0930 - f1_score_mod: 0.4217 - recall_mod: 0.2941 - precision_mod: 0.7474 - dur_error: 0.3547 - maestro_dur_loss: 0.0177 - val_loss: 0.0945 - val_f1_score_mod: 0.4149 - val_recall_mod: 0.2908 - val_precision_mod: 0.7250 - val_dur_error: 0.2657 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09453 to 0.09310, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.0928 - f1_score_mod: 0.4228 - recall_mod: 0.2956 - precision_mod: 0.7445 - dur_error: 0.3559 - maestro_dur_loss: 0.0178 - val_loss: 0.0931 - val_f1_score_mod: 0.4203 - val_recall_mod: 0.2972 - val_precision_mod: 0.7185 - val_dur_error: 0.2448 - val_maestro_dur_loss: 0.0122\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09310\n",
      "50/50 - 130s - loss: 0.0919 - f1_score_mod: 0.4280 - recall_mod: 0.3001 - precision_mod: 0.7476 - dur_error: 0.3523 - maestro_dur_loss: 0.0176 - val_loss: 0.0964 - val_f1_score_mod: 0.4285 - val_recall_mod: 0.3079 - val_precision_mod: 0.7047 - val_dur_error: 0.3051 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09310\n",
      "50/50 - 132s - loss: 0.0915 - f1_score_mod: 0.4363 - recall_mod: 0.3082 - precision_mod: 0.7498 - dur_error: 0.3511 - maestro_dur_loss: 0.0176 - val_loss: 0.0954 - val_f1_score_mod: 0.4280 - val_recall_mod: 0.3063 - val_precision_mod: 0.7110 - val_dur_error: 0.2891 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09310\n",
      "50/50 - 131s - loss: 0.0910 - f1_score_mod: 0.4411 - recall_mod: 0.3129 - precision_mod: 0.7487 - dur_error: 0.3538 - maestro_dur_loss: 0.0177 - val_loss: 0.0943 - val_f1_score_mod: 0.4275 - val_recall_mod: 0.3046 - val_precision_mod: 0.7174 - val_dur_error: 0.2702 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09310\n",
      "50/50 - 131s - loss: 0.0902 - f1_score_mod: 0.4433 - recall_mod: 0.3145 - precision_mod: 0.7524 - dur_error: 0.3492 - maestro_dur_loss: 0.0175 - val_loss: 0.0947 - val_f1_score_mod: 0.4287 - val_recall_mod: 0.3048 - val_precision_mod: 0.7233 - val_dur_error: 0.2856 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09310\n",
      "50/50 - 131s - loss: 0.0896 - f1_score_mod: 0.4481 - recall_mod: 0.3202 - precision_mod: 0.7483 - dur_error: 0.3462 - maestro_dur_loss: 0.0173 - val_loss: 0.0968 - val_f1_score_mod: 0.4306 - val_recall_mod: 0.3083 - val_precision_mod: 0.7151 - val_dur_error: 0.3186 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09310\n",
      "50/50 - 130s - loss: 0.0889 - f1_score_mod: 0.4541 - recall_mod: 0.3254 - precision_mod: 0.7535 - dur_error: 0.3458 - maestro_dur_loss: 0.0173 - val_loss: 0.0935 - val_f1_score_mod: 0.4353 - val_recall_mod: 0.3144 - val_precision_mod: 0.7084 - val_dur_error: 0.2583 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09310\n",
      "50/50 - 131s - loss: 0.0885 - f1_score_mod: 0.4572 - recall_mod: 0.3298 - precision_mod: 0.7469 - dur_error: 0.3455 - maestro_dur_loss: 0.0173 - val_loss: 0.0950 - val_f1_score_mod: 0.4359 - val_recall_mod: 0.3152 - val_precision_mod: 0.7074 - val_dur_error: 0.2955 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09310 to 0.09264, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.0880 - f1_score_mod: 0.4605 - recall_mod: 0.3331 - precision_mod: 0.7474 - dur_error: 0.3448 - maestro_dur_loss: 0.0172 - val_loss: 0.0926 - val_f1_score_mod: 0.4327 - val_recall_mod: 0.3083 - val_precision_mod: 0.7264 - val_dur_error: 0.2529 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.09264\n",
      "50/50 - 132s - loss: 0.0875 - f1_score_mod: 0.4678 - recall_mod: 0.3394 - precision_mod: 0.7539 - dur_error: 0.3462 - maestro_dur_loss: 0.0173 - val_loss: 0.0946 - val_f1_score_mod: 0.4405 - val_recall_mod: 0.3197 - val_precision_mod: 0.7085 - val_dur_error: 0.2868 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09264\n",
      "50/50 - 132s - loss: 0.0868 - f1_score_mod: 0.4727 - recall_mod: 0.3450 - precision_mod: 0.7520 - dur_error: 0.3433 - maestro_dur_loss: 0.0172 - val_loss: 0.0941 - val_f1_score_mod: 0.4515 - val_recall_mod: 0.3363 - val_precision_mod: 0.6873 - val_dur_error: 0.2727 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.09264\n",
      "50/50 - 131s - loss: 0.0864 - f1_score_mod: 0.4756 - recall_mod: 0.3482 - precision_mod: 0.7518 - dur_error: 0.3435 - maestro_dur_loss: 0.0172 - val_loss: 0.0934 - val_f1_score_mod: 0.4517 - val_recall_mod: 0.3340 - val_precision_mod: 0.6981 - val_dur_error: 0.2512 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09264\n",
      "50/50 - 131s - loss: 0.0855 - f1_score_mod: 0.4831 - recall_mod: 0.3557 - precision_mod: 0.7558 - dur_error: 0.3405 - maestro_dur_loss: 0.0170 - val_loss: 0.0938 - val_f1_score_mod: 0.4516 - val_recall_mod: 0.3360 - val_precision_mod: 0.6889 - val_dur_error: 0.2698 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.09264\n",
      "50/50 - 130s - loss: 0.0850 - f1_score_mod: 0.4898 - recall_mod: 0.3619 - precision_mod: 0.7595 - dur_error: 0.3400 - maestro_dur_loss: 0.0170 - val_loss: 0.0940 - val_f1_score_mod: 0.4520 - val_recall_mod: 0.3315 - val_precision_mod: 0.7109 - val_dur_error: 0.2871 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09264 to 0.09219, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04.h5\n",
      "50/50 - 131s - loss: 0.0844 - f1_score_mod: 0.4926 - recall_mod: 0.3660 - precision_mod: 0.7546 - dur_error: 0.3391 - maestro_dur_loss: 0.0170 - val_loss: 0.0922 - val_f1_score_mod: 0.4559 - val_recall_mod: 0.3367 - val_precision_mod: 0.7065 - val_dur_error: 0.2487 - val_maestro_dur_loss: 0.0124\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09219\n",
      "50/50 - 131s - loss: 0.0836 - f1_score_mod: 0.4961 - recall_mod: 0.3703 - precision_mod: 0.7525 - dur_error: 0.3344 - maestro_dur_loss: 0.0167 - val_loss: 0.0948 - val_f1_score_mod: 0.4582 - val_recall_mod: 0.3416 - val_precision_mod: 0.6962 - val_dur_error: 0.2984 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.09219\n",
      "50/50 - 131s - loss: 0.0831 - f1_score_mod: 0.5034 - recall_mod: 0.3774 - precision_mod: 0.7571 - dur_error: 0.3359 - maestro_dur_loss: 0.0168 - val_loss: 0.0931 - val_f1_score_mod: 0.4627 - val_recall_mod: 0.3443 - val_precision_mod: 0.7061 - val_dur_error: 0.2729 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.09219\n",
      "50/50 - 130s - loss: 0.0826 - f1_score_mod: 0.5090 - recall_mod: 0.3833 - precision_mod: 0.7586 - dur_error: 0.3365 - maestro_dur_loss: 0.0168 - val_loss: 0.0929 - val_f1_score_mod: 0.4600 - val_recall_mod: 0.3414 - val_precision_mod: 0.7053 - val_dur_error: 0.2557 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.09219\n",
      "50/50 - 131s - loss: 0.0820 - f1_score_mod: 0.5118 - recall_mod: 0.3871 - precision_mod: 0.7558 - dur_error: 0.3340 - maestro_dur_loss: 0.0167 - val_loss: 0.0928 - val_f1_score_mod: 0.4661 - val_recall_mod: 0.3483 - val_precision_mod: 0.7048 - val_dur_error: 0.2551 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.09219\n",
      "50/50 - 130s - loss: 0.0815 - f1_score_mod: 0.5172 - recall_mod: 0.3931 - precision_mod: 0.7571 - dur_error: 0.3330 - maestro_dur_loss: 0.0166 - val_loss: 0.0925 - val_f1_score_mod: 0.4653 - val_recall_mod: 0.3495 - val_precision_mod: 0.6962 - val_dur_error: 0.2474 - val_maestro_dur_loss: 0.0124\n",
      "Epoch 86/150\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.09219\n",
      "50/50 - 131s - loss: 0.0806 - f1_score_mod: 0.5246 - recall_mod: 0.4007 - precision_mod: 0.7604 - dur_error: 0.3321 - maestro_dur_loss: 0.0166 - val_loss: 0.0965 - val_f1_score_mod: 0.4716 - val_recall_mod: 0.3607 - val_precision_mod: 0.6815 - val_dur_error: 0.3287 - val_maestro_dur_loss: 0.0164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/150\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.09219\n",
      "50/50 - 131s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed significantly better but was also still improving just before the NaN loss. We can also try [gradient clipping](https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/). There are two methods to accomplish this and they are controlled by the optional parameters to RMSProp (clipnorm and clipvalue). clipnorm limits the L2 norm of the gradients after each batch by rescaling them to be lower than the value set by clipnorm. clipvalue enforces a maximum/minimum value (equal to +/- clipvalue) for each gradient. In the linked blog post, reasonable values are given to be 1.0 for clipnorm and 0.5 for clipvalue. \n",
    "\n",
    "We could simply try these but it is better to get a look at the gradients for our base model to inform our decision. For this, we will need to run the base model again (using train_by_batch so I can call the following functions in between batches). Note that training by batch will slightly alter the results when compared to train (insert link). To inform the clipnorm choice, we simply need to calculate the L2-norm of the gradients after each batch. To inform the clipvalue choice, we will need to look at the minimum/maximum gradients (over the batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norm_func(model):\n",
    "    \"\"\"Returns the gradient norm of the model averaged over all gradients of all layers\"\"\"\n",
    "\n",
    "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "    summed_squares = [K.sum(K.square(g)) for g in grads]\n",
    "    norm = K.sqrt(sum(summed_squares))\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    return K.function(inputs, [norm])\n",
    "\n",
    "def get_gradient_stats_func(model):\n",
    "    \"\"\"Returns the gradient statistics of the model\"\"\"\n",
    "\n",
    "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
    "    means = [K.mean(g) for g in grads]\n",
    "    minimums = [K.min(g) for g in grads]\n",
    "    maximums = [K.max(g) for g in grads]\n",
    "    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    return K.function(inputs, [means, minimums, maximums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 2:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 3:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 4:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 5:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 6:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 7:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 8:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 9:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 10:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 11:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 12:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 13:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 14:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 15:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 16:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 17:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 18:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 19:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 20:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 21:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 22:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 23:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 24:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 25:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 26:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 27:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 28:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 29:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 30:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 31:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 32:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 33:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 34:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 35:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 36:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 37:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 38:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 39:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 40:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 41:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 42:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 43:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 44:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 45:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 46:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 47:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 48:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 49:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n",
      "Epoch 50:\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "batch 25\n",
      "batch 26\n",
      "batch 27\n",
      "batch 28\n",
      "batch 29\n",
      "batch 30\n",
      "batch 31\n",
      "batch 32\n",
      "batch 33\n",
      "batch 34\n",
      "batch 35\n",
      "batch 36\n",
      "batch 37\n",
      "batch 38\n",
      "batch 39\n",
      "batch 40\n",
      "batch 41\n",
      "batch 42\n",
      "batch 43\n",
      "batch 44\n",
      "batch 45\n",
      "batch 46\n",
      "batch 47\n",
      "batch 48\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()   # For some reason, I need to do this to have\n",
    "                                         # access to total_loss. It's strange because \n",
    "                                         # model.run_eagerly = False both before and \n",
    "                                         # after running the command, as it should.\n",
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4)\n",
    "harshness = 0.05\n",
    "model.compile(loss = maestro_loss_wr(0.05), optimizer = RMSprop(), metrics = [f1_score_mod, dur_error, maestro_dur_loss_wr(0.05)])\n",
    "\n",
    "get_gradient_norms = get_gradient_norm_func(model)\n",
    "get_gradient_stats = get_gradient_stats_func(model)\n",
    "norms_by_batch = []\n",
    "stats_by_batch = []\n",
    "batch_size = 512\n",
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    print('Epoch {}:'.format(i + 1))\n",
    "    for j in range(len(y_train) // batch_size):\n",
    "        model.train_on_batch(X_train[j * batch_size: (j + 1) * batch_size], \\\n",
    "                             y_train[j * batch_size: (j + 1) * batch_size])\n",
    "        \n",
    "        # We just need to pass placeholder arrays of shape (?, window_size, 88),\n",
    "        # (?, n_keys_piano + 1) and (n_keys_piano + 1)\n",
    "        gradient_norms = get_gradient_norms([X_train[:2], y_train[:2], np.ones(2)])\n",
    "        gradient_stats = get_gradient_stats([X_train[:2], y_train[:2], np.ones(2)])\n",
    "        mean = np.mean(gradient_stats[0])\n",
    "        minimum = min(gradient_stats[1])\n",
    "        maximum = max(gradient_stats[2])\n",
    "        print('batch {}'.format(j))\n",
    "        norms_by_batch.append(gradient_norms[0])\n",
    "        stats_by_batch.append([mean, minimum, maximum])\n",
    "        \n",
    "    model.train_on_batch(X_train[(j + 1) * batch_size:], y_train[(j + 1) * batch_size:])\n",
    "    \n",
    "    gradient_norms = get_gradient_norms([X_train[:2], y_train[:2], np.ones(2)])\n",
    "    gradient_stats = get_gradient_stats([X_train[:2], y_train[:2], np.ones(2)])\n",
    "    mean = np.mean(gradient_stats[0])\n",
    "    minimum = min(gradient_stats[1])\n",
    "    maximum = max(gradient_stats[2])\n",
    "    norms_by_batch.append(gradient_norms[0])\n",
    "    stats_by_batch.append([mean, minimum, maximum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>norm</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th>Batch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.257828</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.017935</td>\n",
       "      <td>0.016901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.596167</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>-0.038911</td>\n",
       "      <td>0.068653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.776338</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.091977</td>\n",
       "      <td>0.037675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.637393</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>-0.092392</td>\n",
       "      <td>0.047412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.809789</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.044027</td>\n",
       "      <td>0.134970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>2404</th>\n",
       "      <td>0.313298</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.055644</td>\n",
       "      <td>0.031017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>0.482061</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.050685</td>\n",
       "      <td>0.064981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>0.519350</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.079119</td>\n",
       "      <td>0.042278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>0.511244</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>0.111103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>0.254221</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.042422</td>\n",
       "      <td>0.029684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2408 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 norm      mean       min       max\n",
       "Epoch Batch                                        \n",
       "1     1      0.257828  0.000344 -0.017935  0.016901\n",
       "      2      0.596167  0.000167 -0.038911  0.068653\n",
       "      3      0.776338  0.000112 -0.091977  0.037675\n",
       "      4      0.637393  0.000049 -0.092392  0.047412\n",
       "      5      0.809789  0.000065 -0.044027  0.134970\n",
       "...               ...       ...       ...       ...\n",
       "50    2404   0.313298 -0.000011 -0.055644  0.031017\n",
       "      2405   0.482061 -0.000052 -0.050685  0.064981\n",
       "      2406   0.519350  0.000033 -0.079119  0.042278\n",
       "      2407   0.511244 -0.000059 -0.047714  0.111103\n",
       "      2408   0.254221 -0.000014 -0.042422  0.029684\n",
       "\n",
       "[2408 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the dataframes\n",
    "df_norms = pd.DataFrame(norms_by_batch)\n",
    "df_stats = pd.DataFrame(stats_by_batch)\n",
    "df_grads = pd.concat([df_norms, df_stats], axis = 1).dropna()\n",
    "df_grads.columns = ['norm', 'mean', 'min', 'max']\n",
    "batches_per_epoch = 49\n",
    "df_grads.index = pd.MultiIndex.from_tuples([((batch // batches_per_epoch) + 1, batch) \\\n",
    "                    for batch in range(1, len(df_grads) + 1)], names = ('Epoch', 'Batch'))\n",
    "df_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data so we don't have to re-run the model\n",
    "df_grads.to_csv('../model_data/gradients_2_1_512_0pt4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x65728fe10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_grads = pd.read_csv('../model_data/gradients_2_1_512_0pt4.csv', index_col = [0, 1])\n",
    "df_grads.plot(y = 'norm', figsize = (12, 6), logy = True, xlabel = 'Epoch, Batch', \\\n",
    "             ylabel = 'L2-Norm', fontsize = 12)\n",
    "plt.axhline(1, color = 'y')\n",
    "plt.axhline(0.5, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot (note the logscale on the y-axis), we have added a line corresponding to the suggested clipnorm value of 1.0. We can see that on several occasions in early training the L2-norm exceeds this value (sometimes by quite a lot!). It is possible that the norm for the final batch would have exploded. To get a better idea, one could re-run with a smaller batch size. However, there is no way to know how much we would have to decrease the batch size to see such an effect. Since lowering the batch size would also increase the run-time, we will halt the exploration here. Our plan is now to train models with clipnorm = 1.0 (line in <font color='yellow'>yellow</font>) and 0.5 (line in <font color='red'>red</font>). The latter should have an effect on the late-stage training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x681bd82d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_grads.plot(y = ['mean', 'min', 'max'], xlabel = 'Epoch, Batch', \\\n",
    "             ylabel = 'Gradient', figsize = (12, 6), fontsize = 12, ylim = [-4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In several occassions in early training some gradients are very large. The times at which these events happen seem to correlate with the times that the gradient norm is very high. We can see more detail by reduing the range in the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x682f61f50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_grads.plot(y = ['mean', 'min', 'max'], xlabel = 'Epoch, Batch', \\\n",
    "             ylabel = 'Gradient', figsize = (12, 6), fontsize = 12, ylim = [-.5, .5])\n",
    "plt.axhline(0.2, color = 'y')\n",
    "plt.axhline(-0.2, color = 'y')\n",
    "plt.axhline(0.1, color = 'r')\n",
    "plt.axhline(-0.1, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can see that the mean says near zero while the min and max (aside from the spikes in early training) stay at relatively small values (< 0.2). A clipvalue of 0.5 was used in the [blog post](https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/)). Perhaps such a value would delay the NaN failure in the final batch. However, from the plot, I think a clipvalue of 0.2 (line in <font color='yellow'>yellow</font>) is a better start. And then, we can reduce it to 0.1 (line in <font color='red'>red</font>) which should certainly affect the late training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14381, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 132s - loss: 0.2169 - f1_score_mod: 0.0139 - recall_mod: 0.0210 - precision_mod: 0.1103 - dur_error: 0.9922 - maestro_dur_loss: 0.0496 - val_loss: 0.1438 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4486 - val_maestro_dur_loss: 0.0224\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14381 to 0.13693, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1595 - f1_score_mod: 0.0059 - recall_mod: 0.0030 - precision_mod: 0.3522 - dur_error: 0.6959 - maestro_dur_loss: 0.0348 - val_loss: 0.1369 - val_f1_score_mod: 0.0278 - val_recall_mod: 0.0142 - val_precision_mod: 0.6222 - val_dur_error: 0.4296 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13693\n",
      "25307/25307 - 129s - loss: 0.1489 - f1_score_mod: 0.0324 - recall_mod: 0.0169 - precision_mod: 0.5556 - dur_error: 0.6311 - maestro_dur_loss: 0.0316 - val_loss: 0.1392 - val_f1_score_mod: 0.0637 - val_recall_mod: 0.0336 - val_precision_mod: 0.6395 - val_dur_error: 0.5822 - val_maestro_dur_loss: 0.0291\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13693 to 0.13101, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.1427 - f1_score_mod: 0.0618 - recall_mod: 0.0330 - precision_mod: 0.6031 - dur_error: 0.6130 - maestro_dur_loss: 0.0306 - val_loss: 0.1310 - val_f1_score_mod: 0.1017 - val_recall_mod: 0.0554 - val_precision_mod: 0.6454 - val_dur_error: 0.4854 - val_maestro_dur_loss: 0.0243\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13101 to 0.12822, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1377 - f1_score_mod: 0.0911 - recall_mod: 0.0496 - precision_mod: 0.6194 - dur_error: 0.5869 - maestro_dur_loss: 0.0293 - val_loss: 0.1282 - val_f1_score_mod: 0.0844 - val_recall_mod: 0.0448 - val_precision_mod: 0.8110 - val_dur_error: 0.5072 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12822\n",
      "25307/25307 - 130s - loss: 0.1342 - f1_score_mod: 0.1158 - recall_mod: 0.0642 - precision_mod: 0.6452 - dur_error: 0.5716 - maestro_dur_loss: 0.0286 - val_loss: 0.1287 - val_f1_score_mod: 0.0957 - val_recall_mod: 0.0509 - val_precision_mod: 0.8296 - val_dur_error: 0.5095 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12822 to 0.12008, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1310 - f1_score_mod: 0.1418 - recall_mod: 0.0798 - precision_mod: 0.6639 - dur_error: 0.5555 - maestro_dur_loss: 0.0278 - val_loss: 0.1201 - val_f1_score_mod: 0.1339 - val_recall_mod: 0.0733 - val_precision_mod: 0.7941 - val_dur_error: 0.4277 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12008\n",
      "25307/25307 - 130s - loss: 0.1355 - f1_score_mod: 0.1442 - recall_mod: 0.0818 - precision_mod: 0.6408 - dur_error: 0.6279 - maestro_dur_loss: 0.0314 - val_loss: 0.1302 - val_f1_score_mod: 0.1643 - val_recall_mod: 0.0927 - val_precision_mod: 0.7336 - val_dur_error: 0.6306 - val_maestro_dur_loss: 0.0315\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12008\n",
      "25307/25307 - 130s - loss: 0.1283 - f1_score_mod: 0.1522 - recall_mod: 0.0860 - precision_mod: 0.6768 - dur_error: 0.5380 - maestro_dur_loss: 0.0269 - val_loss: 0.1252 - val_f1_score_mod: 0.1923 - val_recall_mod: 0.1106 - val_precision_mod: 0.7457 - val_dur_error: 0.5644 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12008 to 0.11965, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1263 - f1_score_mod: 0.1715 - recall_mod: 0.0986 - precision_mod: 0.6792 - dur_error: 0.5244 - maestro_dur_loss: 0.0262 - val_loss: 0.1197 - val_f1_score_mod: 0.1969 - val_recall_mod: 0.1136 - val_precision_mod: 0.7440 - val_dur_error: 0.4647 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11965\n",
      "25307/25307 - 130s - loss: 0.1244 - f1_score_mod: 0.1835 - recall_mod: 0.1063 - precision_mod: 0.6906 - dur_error: 0.5109 - maestro_dur_loss: 0.0255 - val_loss: 0.1199 - val_f1_score_mod: 0.2086 - val_recall_mod: 0.1213 - val_precision_mod: 0.7495 - val_dur_error: 0.4910 - val_maestro_dur_loss: 0.0245\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11965\n",
      "25307/25307 - 134s - loss: 0.1232 - f1_score_mod: 0.1948 - recall_mod: 0.1137 - precision_mod: 0.6932 - dur_error: 0.5082 - maestro_dur_loss: 0.0254 - val_loss: 0.1228 - val_f1_score_mod: 0.2592 - val_recall_mod: 0.1612 - val_precision_mod: 0.6683 - val_dur_error: 0.5358 - val_maestro_dur_loss: 0.0268\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11965 to 0.11366, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.1223 - f1_score_mod: 0.2058 - recall_mod: 0.1213 - precision_mod: 0.6905 - dur_error: 0.5015 - maestro_dur_loss: 0.0251 - val_loss: 0.1137 - val_f1_score_mod: 0.2265 - val_recall_mod: 0.1342 - val_precision_mod: 0.7354 - val_dur_error: 0.3633 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11366 to 0.10976, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.1206 - f1_score_mod: 0.2131 - recall_mod: 0.1262 - precision_mod: 0.6979 - dur_error: 0.4910 - maestro_dur_loss: 0.0245 - val_loss: 0.1098 - val_f1_score_mod: 0.2340 - val_recall_mod: 0.1382 - val_precision_mod: 0.7732 - val_dur_error: 0.3413 - val_maestro_dur_loss: 0.0171\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.10976\n",
      "25307/25307 - 130s - loss: 0.1196 - f1_score_mod: 0.2242 - recall_mod: 0.1336 - precision_mod: 0.7060 - dur_error: 0.4845 - maestro_dur_loss: 0.0242 - val_loss: 0.1106 - val_f1_score_mod: 0.2368 - val_recall_mod: 0.1402 - val_precision_mod: 0.7758 - val_dur_error: 0.3656 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10976\n",
      "25307/25307 - 130s - loss: 0.1184 - f1_score_mod: 0.2340 - recall_mod: 0.1406 - precision_mod: 0.7074 - dur_error: 0.4774 - maestro_dur_loss: 0.0239 - val_loss: 0.1119 - val_f1_score_mod: 0.2566 - val_recall_mod: 0.1551 - val_precision_mod: 0.7479 - val_dur_error: 0.4095 - val_maestro_dur_loss: 0.0205\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10976 to 0.10845, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.1170 - f1_score_mod: 0.2438 - recall_mod: 0.1479 - precision_mod: 0.7081 - dur_error: 0.4671 - maestro_dur_loss: 0.0234 - val_loss: 0.1085 - val_f1_score_mod: 0.2640 - val_recall_mod: 0.1607 - val_precision_mod: 0.7541 - val_dur_error: 0.3537 - val_maestro_dur_loss: 0.0177\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10845 to 0.10683, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1162 - f1_score_mod: 0.2498 - recall_mod: 0.1521 - precision_mod: 0.7082 - dur_error: 0.4626 - maestro_dur_loss: 0.0231 - val_loss: 0.1068 - val_f1_score_mod: 0.2612 - val_recall_mod: 0.1575 - val_precision_mod: 0.7775 - val_dur_error: 0.3289 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10683\n",
      "25307/25307 - 131s - loss: 0.1149 - f1_score_mod: 0.2580 - recall_mod: 0.1578 - precision_mod: 0.7141 - dur_error: 0.4547 - maestro_dur_loss: 0.0227 - val_loss: 0.1114 - val_f1_score_mod: 0.2859 - val_recall_mod: 0.1775 - val_precision_mod: 0.7386 - val_dur_error: 0.4373 - val_maestro_dur_loss: 0.0219\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10683\n",
      "25307/25307 - 131s - loss: 0.1142 - f1_score_mod: 0.2630 - recall_mod: 0.1616 - precision_mod: 0.7174 - dur_error: 0.4546 - maestro_dur_loss: 0.0227 - val_loss: 0.1114 - val_f1_score_mod: 0.3099 - val_recall_mod: 0.1983 - val_precision_mod: 0.7140 - val_dur_error: 0.4317 - val_maestro_dur_loss: 0.0216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10683 to 0.10681, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.1130 - f1_score_mod: 0.2742 - recall_mod: 0.1699 - precision_mod: 0.7189 - dur_error: 0.4447 - maestro_dur_loss: 0.0222 - val_loss: 0.1068 - val_f1_score_mod: 0.2982 - val_recall_mod: 0.1863 - val_precision_mod: 0.7508 - val_dur_error: 0.3632 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10681\n",
      "25307/25307 - 131s - loss: 0.1121 - f1_score_mod: 0.2796 - recall_mod: 0.1743 - precision_mod: 0.7180 - dur_error: 0.4418 - maestro_dur_loss: 0.0221 - val_loss: 0.1086 - val_f1_score_mod: 0.2886 - val_recall_mod: 0.1783 - val_precision_mod: 0.7678 - val_dur_error: 0.4076 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10681 to 0.10161, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.1112 - f1_score_mod: 0.2855 - recall_mod: 0.1782 - precision_mod: 0.7242 - dur_error: 0.4366 - maestro_dur_loss: 0.0218 - val_loss: 0.1016 - val_f1_score_mod: 0.3045 - val_recall_mod: 0.1911 - val_precision_mod: 0.7559 - val_dur_error: 0.2883 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10161\n",
      "25307/25307 - 131s - loss: 0.1099 - f1_score_mod: 0.2964 - recall_mod: 0.1868 - precision_mod: 0.7234 - dur_error: 0.4307 - maestro_dur_loss: 0.0215 - val_loss: 0.1048 - val_f1_score_mod: 0.3133 - val_recall_mod: 0.1976 - val_precision_mod: 0.7582 - val_dur_error: 0.3372 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.10161 to 0.10062, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1091 - f1_score_mod: 0.2995 - recall_mod: 0.1894 - precision_mod: 0.7204 - dur_error: 0.4270 - maestro_dur_loss: 0.0213 - val_loss: 0.1006 - val_f1_score_mod: 0.3147 - val_recall_mod: 0.1987 - val_precision_mod: 0.7633 - val_dur_error: 0.2827 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10062\n",
      "25307/25307 - 131s - loss: 0.1083 - f1_score_mod: 0.3076 - recall_mod: 0.1956 - precision_mod: 0.7263 - dur_error: 0.4229 - maestro_dur_loss: 0.0211 - val_loss: 0.1024 - val_f1_score_mod: 0.3402 - val_recall_mod: 0.2224 - val_precision_mod: 0.7259 - val_dur_error: 0.3297 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10062\n",
      "25307/25307 - 130s - loss: 0.1069 - f1_score_mod: 0.3164 - recall_mod: 0.2028 - precision_mod: 0.7237 - dur_error: 0.4144 - maestro_dur_loss: 0.0207 - val_loss: 0.1053 - val_f1_score_mod: 0.3416 - val_recall_mod: 0.2230 - val_precision_mod: 0.7341 - val_dur_error: 0.3853 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10062 to 0.09991, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1061 - f1_score_mod: 0.3254 - recall_mod: 0.2098 - precision_mod: 0.7300 - dur_error: 0.4133 - maestro_dur_loss: 0.0207 - val_loss: 0.0999 - val_f1_score_mod: 0.3382 - val_recall_mod: 0.2188 - val_precision_mod: 0.7475 - val_dur_error: 0.2907 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09991\n",
      "25307/25307 - 130s - loss: 0.1054 - f1_score_mod: 0.3315 - recall_mod: 0.2150 - precision_mod: 0.7280 - dur_error: 0.4147 - maestro_dur_loss: 0.0207 - val_loss: 0.1032 - val_f1_score_mod: 0.3501 - val_recall_mod: 0.2300 - val_precision_mod: 0.7388 - val_dur_error: 0.3639 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09991 to 0.09846, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.1044 - f1_score_mod: 0.3398 - recall_mod: 0.2217 - precision_mod: 0.7321 - dur_error: 0.4094 - maestro_dur_loss: 0.0205 - val_loss: 0.0985 - val_f1_score_mod: 0.3575 - val_recall_mod: 0.2357 - val_precision_mod: 0.7470 - val_dur_error: 0.2834 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09846 to 0.09761, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.1039 - f1_score_mod: 0.3445 - recall_mod: 0.2256 - precision_mod: 0.7334 - dur_error: 0.4096 - maestro_dur_loss: 0.0205 - val_loss: 0.0976 - val_f1_score_mod: 0.3446 - val_recall_mod: 0.2226 - val_precision_mod: 0.7657 - val_dur_error: 0.2717 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09761\n",
      "25307/25307 - 130s - loss: 0.1023 - f1_score_mod: 0.3539 - recall_mod: 0.2337 - precision_mod: 0.7333 - dur_error: 0.3994 - maestro_dur_loss: 0.0200 - val_loss: 0.1084 - val_f1_score_mod: 0.3710 - val_recall_mod: 0.2474 - val_precision_mod: 0.7431 - val_dur_error: 0.4976 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09761\n",
      "25307/25307 - 131s - loss: 0.1019 - f1_score_mod: 0.3605 - recall_mod: 0.2395 - precision_mod: 0.7326 - dur_error: 0.4036 - maestro_dur_loss: 0.0202 - val_loss: 0.0987 - val_f1_score_mod: 0.3616 - val_recall_mod: 0.2383 - val_precision_mod: 0.7525 - val_dur_error: 0.3170 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09761\n",
      "25307/25307 - 130s - loss: 0.1009 - f1_score_mod: 0.3684 - recall_mod: 0.2456 - precision_mod: 0.7403 - dur_error: 0.3980 - maestro_dur_loss: 0.0199 - val_loss: 0.1035 - val_f1_score_mod: 0.3870 - val_recall_mod: 0.2634 - val_precision_mod: 0.7308 - val_dur_error: 0.4105 - val_maestro_dur_loss: 0.0205\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09761\n",
      "25307/25307 - 131s - loss: 0.1000 - f1_score_mod: 0.3734 - recall_mod: 0.2508 - precision_mod: 0.7347 - dur_error: 0.3936 - maestro_dur_loss: 0.0197 - val_loss: 0.1030 - val_f1_score_mod: 0.3794 - val_recall_mod: 0.2537 - val_precision_mod: 0.7553 - val_dur_error: 0.4257 - val_maestro_dur_loss: 0.0213\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09761\n",
      "25307/25307 - 130s - loss: 0.0989 - f1_score_mod: 0.3820 - recall_mod: 0.2578 - precision_mod: 0.7408 - dur_error: 0.3899 - maestro_dur_loss: 0.0195 - val_loss: 0.1016 - val_f1_score_mod: 0.3965 - val_recall_mod: 0.2718 - val_precision_mod: 0.7349 - val_dur_error: 0.3968 - val_maestro_dur_loss: 0.0198\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09761 to 0.09493, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 131s - loss: 0.0983 - f1_score_mod: 0.3872 - recall_mod: 0.2636 - precision_mod: 0.7331 - dur_error: 0.3886 - maestro_dur_loss: 0.0194 - val_loss: 0.0949 - val_f1_score_mod: 0.3825 - val_recall_mod: 0.2572 - val_precision_mod: 0.7505 - val_dur_error: 0.2658 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09493\n",
      "25307/25307 - 130s - loss: 0.0973 - f1_score_mod: 0.3929 - recall_mod: 0.2676 - precision_mod: 0.7421 - dur_error: 0.3858 - maestro_dur_loss: 0.0193 - val_loss: 0.0951 - val_f1_score_mod: 0.3947 - val_recall_mod: 0.2695 - val_precision_mod: 0.7414 - val_dur_error: 0.2729 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09493\n",
      "25307/25307 - 130s - loss: 0.0962 - f1_score_mod: 0.4021 - recall_mod: 0.2765 - precision_mod: 0.7409 - dur_error: 0.3808 - maestro_dur_loss: 0.0190 - val_loss: 0.1016 - val_f1_score_mod: 0.4041 - val_recall_mod: 0.2802 - val_precision_mod: 0.7281 - val_dur_error: 0.4131 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09493 to 0.09369, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 130s - loss: 0.0954 - f1_score_mod: 0.4106 - recall_mod: 0.2843 - precision_mod: 0.7414 - dur_error: 0.3803 - maestro_dur_loss: 0.0190 - val_loss: 0.0937 - val_f1_score_mod: 0.4061 - val_recall_mod: 0.2807 - val_precision_mod: 0.7363 - val_dur_error: 0.2601 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09369\n",
      "25307/25307 - 130s - loss: 0.0947 - f1_score_mod: 0.4153 - recall_mod: 0.2887 - precision_mod: 0.7426 - dur_error: 0.3801 - maestro_dur_loss: 0.0190 - val_loss: 0.0954 - val_f1_score_mod: 0.4052 - val_recall_mod: 0.2764 - val_precision_mod: 0.7618 - val_dur_error: 0.3015 - val_maestro_dur_loss: 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09369 to 0.09301, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_1pt0.h5\n",
      "25307/25307 - 129s - loss: 0.0935 - f1_score_mod: 0.4236 - recall_mod: 0.2968 - precision_mod: 0.7439 - dur_error: 0.3753 - maestro_dur_loss: 0.0188 - val_loss: 0.0930 - val_f1_score_mod: 0.4187 - val_recall_mod: 0.2928 - val_precision_mod: 0.7357 - val_dur_error: 0.2530 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09301\n",
      "25307/25307 - 132s - loss: 0.0925 - f1_score_mod: 0.4312 - recall_mod: 0.3036 - precision_mod: 0.7456 - dur_error: 0.3696 - maestro_dur_loss: 0.0185 - val_loss: 0.0963 - val_f1_score_mod: 0.4152 - val_recall_mod: 0.2879 - val_precision_mod: 0.7470 - val_dur_error: 0.3272 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 44/150\n",
      "Batch 9: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5120/25307 - 21s - loss: nan - f1_score_mod: 0.4276 - recall_mod: 0.2998 - precision_mod: 0.7467 - dur_error: 0.3812 - maestro_dur_loss: 0.0191\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipnorm = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed almost as well on the validation set as the model with half the default learning rate, in fewer epochs. However, it failed at a significantly higher training loss. Still, the clipnorm value seems to help in comparion to the base model. Let's now try with clipnorm = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14471, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 133s - loss: 0.2233 - f1_score_mod: 0.0145 - recall_mod: 0.0241 - precision_mod: 0.0862 - dur_error: 0.9982 - maestro_dur_loss: 0.0499 - val_loss: 0.1447 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4688 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14471 to 0.13938, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1617 - f1_score_mod: 0.0027 - recall_mod: 0.0014 - precision_mod: 0.2303 - dur_error: 0.7063 - maestro_dur_loss: 0.0353 - val_loss: 0.1394 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4489 - val_maestro_dur_loss: 0.0224\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13938\n",
      "25307/25307 - 130s - loss: 0.1505 - f1_score_mod: 0.0211 - recall_mod: 0.0109 - precision_mod: 0.5336 - dur_error: 0.6417 - maestro_dur_loss: 0.0321 - val_loss: 0.1395 - val_f1_score_mod: 0.0445 - val_recall_mod: 0.0230 - val_precision_mod: 0.7572 - val_dur_error: 0.5867 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13938 to 0.12942, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1435 - f1_score_mod: 0.0539 - recall_mod: 0.0287 - precision_mod: 0.5985 - dur_error: 0.6073 - maestro_dur_loss: 0.0304 - val_loss: 0.1294 - val_f1_score_mod: 0.0384 - val_recall_mod: 0.0197 - val_precision_mod: 0.8757 - val_dur_error: 0.4365 - val_maestro_dur_loss: 0.0218\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12942\n",
      "25307/25307 - 130s - loss: 0.1386 - f1_score_mod: 0.0775 - recall_mod: 0.0418 - precision_mod: 0.6275 - dur_error: 0.5822 - maestro_dur_loss: 0.0291 - val_loss: 0.1338 - val_f1_score_mod: 0.0498 - val_recall_mod: 0.0258 - val_precision_mod: 0.8788 - val_dur_error: 0.5526 - val_maestro_dur_loss: 0.0276\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12942 to 0.12402, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1354 - f1_score_mod: 0.1034 - recall_mod: 0.0567 - precision_mod: 0.6385 - dur_error: 0.5759 - maestro_dur_loss: 0.0288 - val_loss: 0.1240 - val_f1_score_mod: 0.0933 - val_recall_mod: 0.0496 - val_precision_mod: 0.8214 - val_dur_error: 0.4295 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12402 to 0.12263, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1416 - f1_score_mod: 0.1069 - recall_mod: 0.0594 - precision_mod: 0.5856 - dur_error: 0.6509 - maestro_dur_loss: 0.0325 - val_loss: 0.1226 - val_f1_score_mod: 0.1305 - val_recall_mod: 0.0717 - val_precision_mod: 0.7451 - val_dur_error: 0.4440 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12263\n",
      "25307/25307 - 137s - loss: 0.1320 - f1_score_mod: 0.1243 - recall_mod: 0.0691 - precision_mod: 0.6521 - dur_error: 0.5579 - maestro_dur_loss: 0.0279 - val_loss: 0.1242 - val_f1_score_mod: 0.1673 - val_recall_mod: 0.0953 - val_precision_mod: 0.6938 - val_dur_error: 0.4972 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12263 to 0.12081, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1302 - f1_score_mod: 0.1393 - recall_mod: 0.0786 - precision_mod: 0.6424 - dur_error: 0.5419 - maestro_dur_loss: 0.0271 - val_loss: 0.1208 - val_f1_score_mod: 0.1205 - val_recall_mod: 0.0654 - val_precision_mod: 0.7916 - val_dur_error: 0.3799 - val_maestro_dur_loss: 0.0190\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12081\n",
      "25307/25307 - 131s - loss: 0.1289 - f1_score_mod: 0.1484 - recall_mod: 0.0839 - precision_mod: 0.6678 - dur_error: 0.5346 - maestro_dur_loss: 0.0267 - val_loss: 0.1230 - val_f1_score_mod: 0.1484 - val_recall_mod: 0.0823 - val_precision_mod: 0.7760 - val_dur_error: 0.4958 - val_maestro_dur_loss: 0.0248\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12081 to 0.11777, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1276 - f1_score_mod: 0.1628 - recall_mod: 0.0931 - precision_mod: 0.6717 - dur_error: 0.5288 - maestro_dur_loss: 0.0264 - val_loss: 0.1178 - val_f1_score_mod: 0.1625 - val_recall_mod: 0.0911 - val_precision_mod: 0.7630 - val_dur_error: 0.4218 - val_maestro_dur_loss: 0.0211\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11777 to 0.11396, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 129s - loss: 0.1256 - f1_score_mod: 0.1768 - recall_mod: 0.1021 - precision_mod: 0.6763 - dur_error: 0.5146 - maestro_dur_loss: 0.0257 - val_loss: 0.1140 - val_f1_score_mod: 0.1707 - val_recall_mod: 0.0960 - val_precision_mod: 0.7866 - val_dur_error: 0.3548 - val_maestro_dur_loss: 0.0177\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11396 to 0.11393, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1240 - f1_score_mod: 0.1829 - recall_mod: 0.1061 - precision_mod: 0.6823 - dur_error: 0.5054 - maestro_dur_loss: 0.0253 - val_loss: 0.1139 - val_f1_score_mod: 0.1875 - val_recall_mod: 0.1073 - val_precision_mod: 0.7656 - val_dur_error: 0.3779 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11393 to 0.11062, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1228 - f1_score_mod: 0.1947 - recall_mod: 0.1138 - precision_mod: 0.6852 - dur_error: 0.4967 - maestro_dur_loss: 0.0248 - val_loss: 0.1106 - val_f1_score_mod: 0.2132 - val_recall_mod: 0.1242 - val_precision_mod: 0.7595 - val_dur_error: 0.3307 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11062\n",
      "25307/25307 - 130s - loss: 0.1211 - f1_score_mod: 0.2041 - recall_mod: 0.1201 - precision_mod: 0.6914 - dur_error: 0.4835 - maestro_dur_loss: 0.0242 - val_loss: 0.1136 - val_f1_score_mod: 0.1898 - val_recall_mod: 0.1078 - val_precision_mod: 0.8096 - val_dur_error: 0.3642 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11062\n",
      "25307/25307 - 130s - loss: 0.1221 - f1_score_mod: 0.2074 - recall_mod: 0.1230 - precision_mod: 0.6815 - dur_error: 0.4967 - maestro_dur_loss: 0.0248 - val_loss: 0.1137 - val_f1_score_mod: 0.1934 - val_recall_mod: 0.1101 - val_precision_mod: 0.8022 - val_dur_error: 0.3793 - val_maestro_dur_loss: 0.0190\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11062\n",
      "25307/25307 - 131s - loss: 0.1197 - f1_score_mod: 0.2090 - recall_mod: 0.1236 - precision_mod: 0.6914 - dur_error: 0.4713 - maestro_dur_loss: 0.0236 - val_loss: 0.1187 - val_f1_score_mod: 0.2375 - val_recall_mod: 0.1436 - val_precision_mod: 0.6906 - val_dur_error: 0.4288 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11062 to 0.10829, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1197 - f1_score_mod: 0.2161 - recall_mod: 0.1284 - precision_mod: 0.6962 - dur_error: 0.4714 - maestro_dur_loss: 0.0236 - val_loss: 0.1083 - val_f1_score_mod: 0.2229 - val_recall_mod: 0.1305 - val_precision_mod: 0.7713 - val_dur_error: 0.3148 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10829\n",
      "25307/25307 - 130s - loss: 0.1182 - f1_score_mod: 0.2245 - recall_mod: 0.1340 - precision_mod: 0.7001 - dur_error: 0.4650 - maestro_dur_loss: 0.0232 - val_loss: 0.1089 - val_f1_score_mod: 0.2428 - val_recall_mod: 0.1453 - val_precision_mod: 0.7500 - val_dur_error: 0.3368 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10829\n",
      "25307/25307 - 130s - loss: 0.1176 - f1_score_mod: 0.2320 - recall_mod: 0.1394 - precision_mod: 0.7025 - dur_error: 0.4640 - maestro_dur_loss: 0.0232 - val_loss: 0.1129 - val_f1_score_mod: 0.2603 - val_recall_mod: 0.1576 - val_precision_mod: 0.7553 - val_dur_error: 0.4311 - val_maestro_dur_loss: 0.0216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10829 to 0.10600, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1165 - f1_score_mod: 0.2411 - recall_mod: 0.1456 - precision_mod: 0.7134 - dur_error: 0.4571 - maestro_dur_loss: 0.0229 - val_loss: 0.1060 - val_f1_score_mod: 0.2649 - val_recall_mod: 0.1615 - val_precision_mod: 0.7450 - val_dur_error: 0.3079 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10600\n",
      "25307/25307 - 130s - loss: 0.1154 - f1_score_mod: 0.2490 - recall_mod: 0.1512 - precision_mod: 0.7151 - dur_error: 0.4509 - maestro_dur_loss: 0.0225 - val_loss: 0.1076 - val_f1_score_mod: 0.2688 - val_recall_mod: 0.1634 - val_precision_mod: 0.7706 - val_dur_error: 0.3461 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10600 to 0.10521, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1145 - f1_score_mod: 0.2591 - recall_mod: 0.1583 - precision_mod: 0.7213 - dur_error: 0.4467 - maestro_dur_loss: 0.0223 - val_loss: 0.1052 - val_f1_score_mod: 0.2833 - val_recall_mod: 0.1751 - val_precision_mod: 0.7462 - val_dur_error: 0.3125 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10521\n",
      "25307/25307 - 130s - loss: 0.1132 - f1_score_mod: 0.2638 - recall_mod: 0.1620 - precision_mod: 0.7182 - dur_error: 0.4344 - maestro_dur_loss: 0.0217 - val_loss: 0.1089 - val_f1_score_mod: 0.2806 - val_recall_mod: 0.1721 - val_precision_mod: 0.7674 - val_dur_error: 0.3831 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10521\n",
      "25307/25307 - 130s - loss: 0.1124 - f1_score_mod: 0.2707 - recall_mod: 0.1670 - precision_mod: 0.7227 - dur_error: 0.4336 - maestro_dur_loss: 0.0217 - val_loss: 0.1114 - val_f1_score_mod: 0.2909 - val_recall_mod: 0.1809 - val_precision_mod: 0.7483 - val_dur_error: 0.4375 - val_maestro_dur_loss: 0.0219\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10521\n",
      "25307/25307 - 130s - loss: 0.1123 - f1_score_mod: 0.2769 - recall_mod: 0.1714 - precision_mod: 0.7251 - dur_error: 0.4395 - maestro_dur_loss: 0.0220 - val_loss: 0.1075 - val_f1_score_mod: 0.2901 - val_recall_mod: 0.1791 - val_precision_mod: 0.7685 - val_dur_error: 0.3819 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10521 to 0.10281, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1111 - f1_score_mod: 0.2835 - recall_mod: 0.1767 - precision_mod: 0.7256 - dur_error: 0.4324 - maestro_dur_loss: 0.0216 - val_loss: 0.1028 - val_f1_score_mod: 0.3049 - val_recall_mod: 0.1908 - val_precision_mod: 0.7650 - val_dur_error: 0.2991 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10281 to 0.10137, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1100 - f1_score_mod: 0.2892 - recall_mod: 0.1814 - precision_mod: 0.7199 - dur_error: 0.4249 - maestro_dur_loss: 0.0212 - val_loss: 0.1014 - val_f1_score_mod: 0.3076 - val_recall_mod: 0.1931 - val_precision_mod: 0.7580 - val_dur_error: 0.2827 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10137\n",
      "25307/25307 - 130s - loss: 0.1091 - f1_score_mod: 0.2975 - recall_mod: 0.1875 - precision_mod: 0.7279 - dur_error: 0.4218 - maestro_dur_loss: 0.0211 - val_loss: 0.1037 - val_f1_score_mod: 0.3060 - val_recall_mod: 0.1915 - val_precision_mod: 0.7695 - val_dur_error: 0.3408 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10137\n",
      "25307/25307 - 130s - loss: 0.1081 - f1_score_mod: 0.3058 - recall_mod: 0.1939 - precision_mod: 0.7282 - dur_error: 0.4183 - maestro_dur_loss: 0.0209 - val_loss: 0.1077 - val_f1_score_mod: 0.3170 - val_recall_mod: 0.2003 - val_precision_mod: 0.7659 - val_dur_error: 0.4262 - val_maestro_dur_loss: 0.0213\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10137 to 0.10008, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 130s - loss: 0.1073 - f1_score_mod: 0.3153 - recall_mod: 0.2016 - precision_mod: 0.7294 - dur_error: 0.4140 - maestro_dur_loss: 0.0207 - val_loss: 0.1001 - val_f1_score_mod: 0.3217 - val_recall_mod: 0.2039 - val_precision_mod: 0.7687 - val_dur_error: 0.2809 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10008\n",
      "25307/25307 - 130s - loss: 0.1064 - f1_score_mod: 0.3211 - recall_mod: 0.2063 - precision_mod: 0.7301 - dur_error: 0.4125 - maestro_dur_loss: 0.0206 - val_loss: 0.1027 - val_f1_score_mod: 0.3367 - val_recall_mod: 0.2161 - val_precision_mod: 0.7659 - val_dur_error: 0.3526 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.10008 to 0.09895, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 133s - loss: 0.1055 - f1_score_mod: 0.3295 - recall_mod: 0.2131 - precision_mod: 0.7326 - dur_error: 0.4088 - maestro_dur_loss: 0.0204 - val_loss: 0.0989 - val_f1_score_mod: 0.3405 - val_recall_mod: 0.2194 - val_precision_mod: 0.7627 - val_dur_error: 0.2868 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09895\n",
      "25307/25307 - 131s - loss: 0.1045 - f1_score_mod: 0.3341 - recall_mod: 0.2164 - precision_mod: 0.7371 - dur_error: 0.4035 - maestro_dur_loss: 0.0202 - val_loss: 0.1036 - val_f1_score_mod: 0.3584 - val_recall_mod: 0.2378 - val_precision_mod: 0.7317 - val_dur_error: 0.3814 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09895 to 0.09798, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1035 - f1_score_mod: 0.3407 - recall_mod: 0.2220 - precision_mod: 0.7357 - dur_error: 0.3986 - maestro_dur_loss: 0.0199 - val_loss: 0.0980 - val_f1_score_mod: 0.3465 - val_recall_mod: 0.2236 - val_precision_mod: 0.7742 - val_dur_error: 0.2835 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09798\n",
      "25307/25307 - 131s - loss: 0.1027 - f1_score_mod: 0.3503 - recall_mod: 0.2304 - precision_mod: 0.7371 - dur_error: 0.3972 - maestro_dur_loss: 0.0199 - val_loss: 0.0983 - val_f1_score_mod: 0.3496 - val_recall_mod: 0.2258 - val_precision_mod: 0.7791 - val_dur_error: 0.2898 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09798 to 0.09753, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1021 - f1_score_mod: 0.3565 - recall_mod: 0.2353 - precision_mod: 0.7388 - dur_error: 0.3977 - maestro_dur_loss: 0.0199 - val_loss: 0.0975 - val_f1_score_mod: 0.3597 - val_recall_mod: 0.2373 - val_precision_mod: 0.7488 - val_dur_error: 0.2837 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09753 to 0.09706, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 132s - loss: 0.1011 - f1_score_mod: 0.3627 - recall_mod: 0.2407 - precision_mod: 0.7405 - dur_error: 0.3934 - maestro_dur_loss: 0.0197 - val_loss: 0.0971 - val_f1_score_mod: 0.3637 - val_recall_mod: 0.2380 - val_precision_mod: 0.7755 - val_dur_error: 0.2847 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09706 to 0.09587, saving model to ../models/best_maestro_model_2_1_512_0pt4_cn_0pt5.h5\n",
      "25307/25307 - 131s - loss: 0.1002 - f1_score_mod: 0.3678 - recall_mod: 0.2448 - precision_mod: 0.7425 - dur_error: 0.3914 - maestro_dur_loss: 0.0196 - val_loss: 0.0959 - val_f1_score_mod: 0.3640 - val_recall_mod: 0.2375 - val_precision_mod: 0.7854 - val_dur_error: 0.2675 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09587\n",
      "25307/25307 - 131s - loss: 0.0991 - f1_score_mod: 0.3730 - recall_mod: 0.2495 - precision_mod: 0.7421 - dur_error: 0.3843 - maestro_dur_loss: 0.0192 - val_loss: 0.1041 - val_f1_score_mod: 0.3907 - val_recall_mod: 0.2662 - val_precision_mod: 0.7365 - val_dur_error: 0.4355 - val_maestro_dur_loss: 0.0218\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09587\n",
      "25307/25307 - 131s - loss: 0.0984 - f1_score_mod: 0.3819 - recall_mod: 0.2580 - precision_mod: 0.7399 - dur_error: 0.3854 - maestro_dur_loss: 0.0193 - val_loss: 0.0970 - val_f1_score_mod: 0.3711 - val_recall_mod: 0.2432 - val_precision_mod: 0.7870 - val_dur_error: 0.2836 - val_maestro_dur_loss: 0.0142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "Batch 12: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6656/25307 - 28s - loss: nan - f1_score_mod: 0.3954 - recall_mod: 0.2682 - precision_mod: 0.7548 - dur_error: 0.3901 - maestro_dur_loss: 0.0195\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipnorm = 0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Surprisingly, this model failed earlier than the last one. This is possibly due to the randomized weight initialization performed by keras. Let's now try specifying clipvalue = 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14871, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.2171 - f1_score_mod: 0.0151 - recall_mod: 0.0231 - precision_mod: 0.0964 - dur_error: 0.9845 - maestro_dur_loss: 0.0492 - val_loss: 0.1487 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6111 - val_maestro_dur_loss: 0.0306\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14871 to 0.13644, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1577 - f1_score_mod: 0.0099 - recall_mod: 0.0050 - precision_mod: 0.4126 - dur_error: 0.6688 - maestro_dur_loss: 0.0334 - val_loss: 0.1364 - val_f1_score_mod: 0.0176 - val_recall_mod: 0.0089 - val_precision_mod: 0.7681 - val_dur_error: 0.4531 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13644 to 0.13488, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1479 - f1_score_mod: 0.0393 - recall_mod: 0.0206 - precision_mod: 0.5528 - dur_error: 0.6213 - maestro_dur_loss: 0.0311 - val_loss: 0.1349 - val_f1_score_mod: 0.0777 - val_recall_mod: 0.0416 - val_precision_mod: 0.6277 - val_dur_error: 0.5344 - val_maestro_dur_loss: 0.0267\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13488 to 0.12789, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1418 - f1_score_mod: 0.0726 - recall_mod: 0.0391 - precision_mod: 0.5849 - dur_error: 0.6015 - maestro_dur_loss: 0.0301 - val_loss: 0.1279 - val_f1_score_mod: 0.0647 - val_recall_mod: 0.0338 - val_precision_mod: 0.8278 - val_dur_error: 0.4491 - val_maestro_dur_loss: 0.0225\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12789\n",
      "25307/25307 - 130s - loss: 0.1404 - f1_score_mod: 0.0889 - recall_mod: 0.0484 - precision_mod: 0.5860 - dur_error: 0.6102 - maestro_dur_loss: 0.0305 - val_loss: 0.1304 - val_f1_score_mod: 0.0749 - val_recall_mod: 0.0394 - val_precision_mod: 0.7758 - val_dur_error: 0.5104 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12789 to 0.12666, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1359 - f1_score_mod: 0.0991 - recall_mod: 0.0541 - precision_mod: 0.6341 - dur_error: 0.5708 - maestro_dur_loss: 0.0285 - val_loss: 0.1267 - val_f1_score_mod: 0.0983 - val_recall_mod: 0.0525 - val_precision_mod: 0.7919 - val_dur_error: 0.4543 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12666\n",
      "25307/25307 - 130s - loss: 0.1334 - f1_score_mod: 0.1170 - recall_mod: 0.0648 - precision_mod: 0.6432 - dur_error: 0.5594 - maestro_dur_loss: 0.0280 - val_loss: 0.1302 - val_f1_score_mod: 0.1632 - val_recall_mod: 0.0929 - val_precision_mod: 0.6793 - val_dur_error: 0.5831 - val_maestro_dur_loss: 0.0292\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12666\n",
      "25307/25307 - 130s - loss: 0.1318 - f1_score_mod: 0.1336 - recall_mod: 0.0750 - precision_mod: 0.6471 - dur_error: 0.5536 - maestro_dur_loss: 0.0277 - val_loss: 0.1288 - val_f1_score_mod: 0.1457 - val_recall_mod: 0.0808 - val_precision_mod: 0.7529 - val_dur_error: 0.5758 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12666 to 0.11939, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1301 - f1_score_mod: 0.1458 - recall_mod: 0.0825 - precision_mod: 0.6640 - dur_error: 0.5475 - maestro_dur_loss: 0.0274 - val_loss: 0.1194 - val_f1_score_mod: 0.1564 - val_recall_mod: 0.0881 - val_precision_mod: 0.7158 - val_dur_error: 0.4208 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11939\n",
      "25307/25307 - 129s - loss: 0.1277 - f1_score_mod: 0.1628 - recall_mod: 0.0932 - precision_mod: 0.6695 - dur_error: 0.5305 - maestro_dur_loss: 0.0265 - val_loss: 0.1255 - val_f1_score_mod: 0.1728 - val_recall_mod: 0.0982 - val_precision_mod: 0.7256 - val_dur_error: 0.5658 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11939\n",
      "25307/25307 - 129s - loss: 0.1266 - f1_score_mod: 0.1720 - recall_mod: 0.0992 - precision_mod: 0.6689 - dur_error: 0.5280 - maestro_dur_loss: 0.0264 - val_loss: 0.1211 - val_f1_score_mod: 0.2050 - val_recall_mod: 0.1199 - val_precision_mod: 0.7155 - val_dur_error: 0.4955 - val_maestro_dur_loss: 0.0248\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11939 to 0.11569, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1246 - f1_score_mod: 0.1852 - recall_mod: 0.1077 - precision_mod: 0.6799 - dur_error: 0.5128 - maestro_dur_loss: 0.0256 - val_loss: 0.1157 - val_f1_score_mod: 0.2210 - val_recall_mod: 0.1309 - val_precision_mod: 0.7132 - val_dur_error: 0.4144 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11569\n",
      "25307/25307 - 130s - loss: 0.1234 - f1_score_mod: 0.1956 - recall_mod: 0.1144 - precision_mod: 0.6862 - dur_error: 0.5073 - maestro_dur_loss: 0.0254 - val_loss: 0.1213 - val_f1_score_mod: 0.2059 - val_recall_mod: 0.1197 - val_precision_mod: 0.7435 - val_dur_error: 0.5299 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11569\n",
      "25307/25307 - 129s - loss: 0.1218 - f1_score_mod: 0.2063 - recall_mod: 0.1216 - precision_mod: 0.6975 - dur_error: 0.4955 - maestro_dur_loss: 0.0248 - val_loss: 0.1197 - val_f1_score_mod: 0.2227 - val_recall_mod: 0.1309 - val_precision_mod: 0.7523 - val_dur_error: 0.5169 - val_maestro_dur_loss: 0.0258\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11569\n",
      "25307/25307 - 129s - loss: 0.1213 - f1_score_mod: 0.2111 - recall_mod: 0.1249 - precision_mod: 0.6963 - dur_error: 0.4966 - maestro_dur_loss: 0.0248 - val_loss: 0.1242 - val_f1_score_mod: 0.2229 - val_recall_mod: 0.1314 - val_precision_mod: 0.7425 - val_dur_error: 0.6057 - val_maestro_dur_loss: 0.0303\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11569 to 0.10854, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1201 - f1_score_mod: 0.2206 - recall_mod: 0.1314 - precision_mod: 0.6978 - dur_error: 0.4876 - maestro_dur_loss: 0.0244 - val_loss: 0.1085 - val_f1_score_mod: 0.2273 - val_recall_mod: 0.1332 - val_precision_mod: 0.7837 - val_dur_error: 0.3206 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10854\n",
      "25307/25307 - 129s - loss: 0.1186 - f1_score_mod: 0.2303 - recall_mod: 0.1381 - precision_mod: 0.7018 - dur_error: 0.4759 - maestro_dur_loss: 0.0238 - val_loss: 0.1113 - val_f1_score_mod: 0.2480 - val_recall_mod: 0.1496 - val_precision_mod: 0.7315 - val_dur_error: 0.3621 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10854\n",
      "25307/25307 - 130s - loss: 0.1176 - f1_score_mod: 0.2375 - recall_mod: 0.1432 - precision_mod: 0.7052 - dur_error: 0.4671 - maestro_dur_loss: 0.0234 - val_loss: 0.1086 - val_f1_score_mod: 0.2587 - val_recall_mod: 0.1568 - val_precision_mod: 0.7451 - val_dur_error: 0.3386 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.10854 to 0.10796, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1172 - f1_score_mod: 0.2392 - recall_mod: 0.1444 - precision_mod: 0.7069 - dur_error: 0.4671 - maestro_dur_loss: 0.0234 - val_loss: 0.1080 - val_f1_score_mod: 0.2359 - val_recall_mod: 0.1388 - val_precision_mod: 0.7923 - val_dur_error: 0.3204 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10796\n",
      "25307/25307 - 130s - loss: 0.1160 - f1_score_mod: 0.2517 - recall_mod: 0.1532 - precision_mod: 0.7170 - dur_error: 0.4611 - maestro_dur_loss: 0.0231 - val_loss: 0.1089 - val_f1_score_mod: 0.2650 - val_recall_mod: 0.1593 - val_precision_mod: 0.7961 - val_dur_error: 0.3709 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10796\n",
      "25307/25307 - 130s - loss: 0.1147 - f1_score_mod: 0.2582 - recall_mod: 0.1580 - precision_mod: 0.7131 - dur_error: 0.4512 - maestro_dur_loss: 0.0226 - val_loss: 0.1126 - val_f1_score_mod: 0.2792 - val_recall_mod: 0.1721 - val_precision_mod: 0.7428 - val_dur_error: 0.4514 - val_maestro_dur_loss: 0.0226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10796 to 0.10488, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1142 - f1_score_mod: 0.2631 - recall_mod: 0.1616 - precision_mod: 0.7172 - dur_error: 0.4516 - maestro_dur_loss: 0.0226 - val_loss: 0.1049 - val_f1_score_mod: 0.2857 - val_recall_mod: 0.1760 - val_precision_mod: 0.7692 - val_dur_error: 0.3043 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10488 to 0.10386, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1133 - f1_score_mod: 0.2706 - recall_mod: 0.1674 - precision_mod: 0.7150 - dur_error: 0.4448 - maestro_dur_loss: 0.0222 - val_loss: 0.1039 - val_f1_score_mod: 0.2930 - val_recall_mod: 0.1822 - val_precision_mod: 0.7515 - val_dur_error: 0.2998 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10386\n",
      "25307/25307 - 130s - loss: 0.1123 - f1_score_mod: 0.2795 - recall_mod: 0.1739 - precision_mod: 0.7213 - dur_error: 0.4442 - maestro_dur_loss: 0.0222 - val_loss: 0.1042 - val_f1_score_mod: 0.2986 - val_recall_mod: 0.1862 - val_precision_mod: 0.7580 - val_dur_error: 0.3137 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10386\n",
      "25307/25307 - 133s - loss: 0.1114 - f1_score_mod: 0.2845 - recall_mod: 0.1776 - precision_mod: 0.7202 - dur_error: 0.4364 - maestro_dur_loss: 0.0218 - val_loss: 0.1074 - val_f1_score_mod: 0.3161 - val_recall_mod: 0.2000 - val_precision_mod: 0.7607 - val_dur_error: 0.3897 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10386\n",
      "25307/25307 - 130s - loss: 0.1105 - f1_score_mod: 0.2918 - recall_mod: 0.1830 - precision_mod: 0.7271 - dur_error: 0.4317 - maestro_dur_loss: 0.0216 - val_loss: 0.1089 - val_f1_score_mod: 0.3219 - val_recall_mod: 0.2053 - val_precision_mod: 0.7495 - val_dur_error: 0.4232 - val_maestro_dur_loss: 0.0212\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10386 to 0.10101, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1095 - f1_score_mod: 0.3000 - recall_mod: 0.1893 - precision_mod: 0.7292 - dur_error: 0.4256 - maestro_dur_loss: 0.0213 - val_loss: 0.1010 - val_f1_score_mod: 0.3108 - val_recall_mod: 0.1950 - val_precision_mod: 0.7744 - val_dur_error: 0.2862 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10101\n",
      "25307/25307 - 130s - loss: 0.1086 - f1_score_mod: 0.3052 - recall_mod: 0.1938 - precision_mod: 0.7260 - dur_error: 0.4195 - maestro_dur_loss: 0.0210 - val_loss: 0.1054 - val_f1_score_mod: 0.3182 - val_recall_mod: 0.2009 - val_precision_mod: 0.7694 - val_dur_error: 0.3859 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10101\n",
      "25307/25307 - 130s - loss: 0.1079 - f1_score_mod: 0.3112 - recall_mod: 0.1981 - precision_mod: 0.7297 - dur_error: 0.4220 - maestro_dur_loss: 0.0211 - val_loss: 0.1011 - val_f1_score_mod: 0.3270 - val_recall_mod: 0.2074 - val_precision_mod: 0.7769 - val_dur_error: 0.3038 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10101\n",
      "25307/25307 - 129s - loss: 0.1066 - f1_score_mod: 0.3188 - recall_mod: 0.2045 - precision_mod: 0.7307 - dur_error: 0.4120 - maestro_dur_loss: 0.0206 - val_loss: 0.1103 - val_f1_score_mod: 0.3539 - val_recall_mod: 0.2328 - val_precision_mod: 0.7441 - val_dur_error: 0.4941 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10101\n",
      "25307/25307 - 129s - loss: 0.1058 - f1_score_mod: 0.3281 - recall_mod: 0.2122 - precision_mod: 0.7319 - dur_error: 0.4115 - maestro_dur_loss: 0.0206 - val_loss: 0.1022 - val_f1_score_mod: 0.3425 - val_recall_mod: 0.2210 - val_precision_mod: 0.7674 - val_dur_error: 0.3362 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10101\n",
      "25307/25307 - 130s - loss: 0.1052 - f1_score_mod: 0.3358 - recall_mod: 0.2180 - precision_mod: 0.7355 - dur_error: 0.4121 - maestro_dur_loss: 0.0206 - val_loss: 0.1054 - val_f1_score_mod: 0.3534 - val_recall_mod: 0.2310 - val_precision_mod: 0.7550 - val_dur_error: 0.4104 - val_maestro_dur_loss: 0.0205\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.10101 to 0.09949, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1042 - f1_score_mod: 0.3408 - recall_mod: 0.2223 - precision_mod: 0.7357 - dur_error: 0.4052 - maestro_dur_loss: 0.0203 - val_loss: 0.0995 - val_f1_score_mod: 0.3530 - val_recall_mod: 0.2298 - val_precision_mod: 0.7643 - val_dur_error: 0.3014 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09949\n",
      "25307/25307 - 130s - loss: 0.1035 - f1_score_mod: 0.3468 - recall_mod: 0.2271 - precision_mod: 0.7380 - dur_error: 0.4054 - maestro_dur_loss: 0.0203 - val_loss: 0.1000 - val_f1_score_mod: 0.3616 - val_recall_mod: 0.2377 - val_precision_mod: 0.7599 - val_dur_error: 0.3162 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09949 to 0.09823, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.1024 - f1_score_mod: 0.3547 - recall_mod: 0.2337 - precision_mod: 0.7389 - dur_error: 0.3990 - maestro_dur_loss: 0.0199 - val_loss: 0.0982 - val_f1_score_mod: 0.3533 - val_recall_mod: 0.2295 - val_precision_mod: 0.7726 - val_dur_error: 0.2928 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09823 to 0.09689, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 129s - loss: 0.1018 - f1_score_mod: 0.3627 - recall_mod: 0.2407 - precision_mod: 0.7402 - dur_error: 0.3961 - maestro_dur_loss: 0.0198 - val_loss: 0.0969 - val_f1_score_mod: 0.3663 - val_recall_mod: 0.2405 - val_precision_mod: 0.7731 - val_dur_error: 0.2754 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09689 to 0.09586, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 129s - loss: 0.1009 - f1_score_mod: 0.3676 - recall_mod: 0.2450 - precision_mod: 0.7400 - dur_error: 0.3953 - maestro_dur_loss: 0.0198 - val_loss: 0.0959 - val_f1_score_mod: 0.3800 - val_recall_mod: 0.2560 - val_precision_mod: 0.7393 - val_dur_error: 0.2664 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09586\n",
      "25307/25307 - 129s - loss: 0.0999 - f1_score_mod: 0.3719 - recall_mod: 0.2490 - precision_mod: 0.7376 - dur_error: 0.3892 - maestro_dur_loss: 0.0195 - val_loss: 0.1037 - val_f1_score_mod: 0.3785 - val_recall_mod: 0.2543 - val_precision_mod: 0.7434 - val_dur_error: 0.4323 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09586\n",
      "25307/25307 - 130s - loss: 0.0992 - f1_score_mod: 0.3800 - recall_mod: 0.2557 - precision_mod: 0.7437 - dur_error: 0.3903 - maestro_dur_loss: 0.0195 - val_loss: 0.1013 - val_f1_score_mod: 0.3848 - val_recall_mod: 0.2587 - val_precision_mod: 0.7533 - val_dur_error: 0.3861 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09586\n",
      "25307/25307 - 130s - loss: 0.0984 - f1_score_mod: 0.3851 - recall_mod: 0.2613 - precision_mod: 0.7360 - dur_error: 0.3861 - maestro_dur_loss: 0.0193 - val_loss: 0.0976 - val_f1_score_mod: 0.3802 - val_recall_mod: 0.2529 - val_precision_mod: 0.7683 - val_dur_error: 0.3243 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09586\n",
      "25307/25307 - 129s - loss: 0.0977 - f1_score_mod: 0.3934 - recall_mod: 0.2678 - precision_mod: 0.7448 - dur_error: 0.3871 - maestro_dur_loss: 0.0194 - val_loss: 0.0980 - val_f1_score_mod: 0.3885 - val_recall_mod: 0.2616 - val_precision_mod: 0.7555 - val_dur_error: 0.3385 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09586\n",
      "25307/25307 - 129s - loss: 0.0967 - f1_score_mod: 0.3950 - recall_mod: 0.2696 - precision_mod: 0.7432 - dur_error: 0.3811 - maestro_dur_loss: 0.0191 - val_loss: 0.1030 - val_f1_score_mod: 0.4028 - val_recall_mod: 0.2772 - val_precision_mod: 0.7381 - val_dur_error: 0.4426 - val_maestro_dur_loss: 0.0221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09586 to 0.09386, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 129s - loss: 0.0958 - f1_score_mod: 0.4051 - recall_mod: 0.2790 - precision_mod: 0.7431 - dur_error: 0.3780 - maestro_dur_loss: 0.0189 - val_loss: 0.0939 - val_f1_score_mod: 0.3869 - val_recall_mod: 0.2595 - val_precision_mod: 0.7649 - val_dur_error: 0.2606 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09386\n",
      "25307/25307 - 130s - loss: 0.0948 - f1_score_mod: 0.4139 - recall_mod: 0.2870 - precision_mod: 0.7447 - dur_error: 0.3787 - maestro_dur_loss: 0.0189 - val_loss: 0.1004 - val_f1_score_mod: 0.4134 - val_recall_mod: 0.2917 - val_precision_mod: 0.7133 - val_dur_error: 0.4009 - val_maestro_dur_loss: 0.0200\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09386\n",
      "25307/25307 - 129s - loss: 0.0938 - f1_score_mod: 0.4185 - recall_mod: 0.2917 - precision_mod: 0.7430 - dur_error: 0.3714 - maestro_dur_loss: 0.0186 - val_loss: 0.0949 - val_f1_score_mod: 0.4066 - val_recall_mod: 0.2808 - val_precision_mod: 0.7384 - val_dur_error: 0.2978 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09386 to 0.09276, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt2.h5\n",
      "25307/25307 - 130s - loss: 0.0932 - f1_score_mod: 0.4269 - recall_mod: 0.2992 - precision_mod: 0.7475 - dur_error: 0.3748 - maestro_dur_loss: 0.0187 - val_loss: 0.0928 - val_f1_score_mod: 0.4099 - val_recall_mod: 0.2825 - val_precision_mod: 0.7488 - val_dur_error: 0.2590 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 47/150\n",
      "Batch 47: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "24576/25307 - 109s - loss: nan - f1_score_mod: 0.4339 - recall_mod: 0.3064 - precision_mod: 0.7463 - dur_error: 0.3720 - maestro_dur_loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipvalue = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yeilds the best performance yet aside from the run with the reduced learning rate, and it too fails at a significantly higher training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14224, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 133s - loss: 0.2188 - f1_score_mod: 0.0161 - recall_mod: 0.0273 - precision_mod: 0.1077 - dur_error: 0.9976 - maestro_dur_loss: 0.0499 - val_loss: 0.1422 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4639 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14224 to 0.14177, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 131s - loss: 0.1601 - f1_score_mod: 0.0033 - recall_mod: 0.0017 - precision_mod: 0.2592 - dur_error: 0.6883 - maestro_dur_loss: 0.0344 - val_loss: 0.1418 - val_f1_score_mod: 0.0016 - val_recall_mod: 8.1794e-04 - val_precision_mod: 0.7727 - val_dur_error: 0.5087 - val_maestro_dur_loss: 0.0254\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14177 to 0.13670, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 134s - loss: 0.1494 - f1_score_mod: 0.0227 - recall_mod: 0.0117 - precision_mod: 0.5092 - dur_error: 0.6308 - maestro_dur_loss: 0.0315 - val_loss: 0.1367 - val_f1_score_mod: 0.0392 - val_recall_mod: 0.0202 - val_precision_mod: 0.7100 - val_dur_error: 0.5305 - val_maestro_dur_loss: 0.0265\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13670 to 0.13022, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 130s - loss: 0.1527 - f1_score_mod: 0.0442 - recall_mod: 0.0236 - precision_mod: 0.4929 - dur_error: 0.6913 - maestro_dur_loss: 0.0346 - val_loss: 0.1302 - val_f1_score_mod: 0.0431 - val_recall_mod: 0.0224 - val_precision_mod: 0.7128 - val_dur_error: 0.4383 - val_maestro_dur_loss: 0.0219\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13022 to 0.12888, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 130s - loss: 0.1408 - f1_score_mod: 0.0640 - recall_mod: 0.0340 - precision_mod: 0.6024 - dur_error: 0.5927 - maestro_dur_loss: 0.0296 - val_loss: 0.1289 - val_f1_score_mod: 0.0604 - val_recall_mod: 0.0317 - val_precision_mod: 0.7364 - val_dur_error: 0.4438 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12888\n",
      "25307/25307 - 130s - loss: 0.1378 - f1_score_mod: 0.0818 - recall_mod: 0.0440 - precision_mod: 0.6308 - dur_error: 0.5800 - maestro_dur_loss: 0.0290 - val_loss: 0.1297 - val_f1_score_mod: 0.0813 - val_recall_mod: 0.0431 - val_precision_mod: 0.7659 - val_dur_error: 0.5117 - val_maestro_dur_loss: 0.0256\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12888\n",
      "25307/25307 - 130s - loss: 0.1350 - f1_score_mod: 0.0966 - recall_mod: 0.0527 - precision_mod: 0.6362 - dur_error: 0.5623 - maestro_dur_loss: 0.0281 - val_loss: 0.1304 - val_f1_score_mod: 0.1175 - val_recall_mod: 0.0640 - val_precision_mod: 0.7384 - val_dur_error: 0.5655 - val_maestro_dur_loss: 0.0283\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12888 to 0.12085, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1333 - f1_score_mod: 0.1111 - recall_mod: 0.0612 - precision_mod: 0.6353 - dur_error: 0.5614 - maestro_dur_loss: 0.0281 - val_loss: 0.1209 - val_f1_score_mod: 0.1335 - val_recall_mod: 0.0736 - val_precision_mod: 0.7333 - val_dur_error: 0.4045 - val_maestro_dur_loss: 0.0202\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12085\n",
      "25307/25307 - 132s - loss: 0.1308 - f1_score_mod: 0.1273 - recall_mod: 0.0710 - precision_mod: 0.6403 - dur_error: 0.5412 - maestro_dur_loss: 0.0271 - val_loss: 0.1221 - val_f1_score_mod: 0.1254 - val_recall_mod: 0.0683 - val_precision_mod: 0.7858 - val_dur_error: 0.4323 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12085 to 0.12007, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 134s - loss: 0.1300 - f1_score_mod: 0.1378 - recall_mod: 0.0775 - precision_mod: 0.6630 - dur_error: 0.5427 - maestro_dur_loss: 0.0271 - val_loss: 0.1201 - val_f1_score_mod: 0.1406 - val_recall_mod: 0.0774 - val_precision_mod: 0.7825 - val_dur_error: 0.4180 - val_maestro_dur_loss: 0.0209\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.12007\n",
      "25307/25307 - 132s - loss: 0.1295 - f1_score_mod: 0.1456 - recall_mod: 0.0822 - precision_mod: 0.6587 - dur_error: 0.5419 - maestro_dur_loss: 0.0271 - val_loss: 0.1220 - val_f1_score_mod: 0.1630 - val_recall_mod: 0.0922 - val_precision_mod: 0.7177 - val_dur_error: 0.4685 - val_maestro_dur_loss: 0.0234\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.12007 to 0.11671, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1280 - f1_score_mod: 0.1553 - recall_mod: 0.0885 - precision_mod: 0.6723 - dur_error: 0.5322 - maestro_dur_loss: 0.0266 - val_loss: 0.1167 - val_f1_score_mod: 0.1667 - val_recall_mod: 0.0939 - val_precision_mod: 0.7575 - val_dur_error: 0.3887 - val_maestro_dur_loss: 0.0194\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11671 to 0.11649, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 131s - loss: 0.1267 - f1_score_mod: 0.1662 - recall_mod: 0.0952 - precision_mod: 0.6778 - dur_error: 0.5265 - maestro_dur_loss: 0.0263 - val_loss: 0.1165 - val_f1_score_mod: 0.1502 - val_recall_mod: 0.0831 - val_precision_mod: 0.8027 - val_dur_error: 0.3892 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11649 to 0.11580, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 131s - loss: 0.1255 - f1_score_mod: 0.1752 - recall_mod: 0.1010 - precision_mod: 0.6848 - dur_error: 0.5180 - maestro_dur_loss: 0.0259 - val_loss: 0.1158 - val_f1_score_mod: 0.1870 - val_recall_mod: 0.1067 - val_precision_mod: 0.7697 - val_dur_error: 0.4014 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11580 to 0.11447, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 133s - loss: 0.1239 - f1_score_mod: 0.1887 - recall_mod: 0.1098 - precision_mod: 0.6838 - dur_error: 0.5054 - maestro_dur_loss: 0.0253 - val_loss: 0.1145 - val_f1_score_mod: 0.1898 - val_recall_mod: 0.1083 - val_precision_mod: 0.7709 - val_dur_error: 0.3847 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11447\n",
      "25307/25307 - 133s - loss: 0.1229 - f1_score_mod: 0.1931 - recall_mod: 0.1127 - precision_mod: 0.6932 - dur_error: 0.4991 - maestro_dur_loss: 0.0250 - val_loss: 0.1146 - val_f1_score_mod: 0.2116 - val_recall_mod: 0.1242 - val_precision_mod: 0.7248 - val_dur_error: 0.3946 - val_maestro_dur_loss: 0.0197\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11447 to 0.11108, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1220 - f1_score_mod: 0.2015 - recall_mod: 0.1183 - precision_mod: 0.6878 - dur_error: 0.4947 - maestro_dur_loss: 0.0247 - val_loss: 0.1111 - val_f1_score_mod: 0.2146 - val_recall_mod: 0.1247 - val_precision_mod: 0.7780 - val_dur_error: 0.3424 - val_maestro_dur_loss: 0.0171\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11108\n",
      "25307/25307 - 132s - loss: 0.1205 - f1_score_mod: 0.2077 - recall_mod: 0.1224 - precision_mod: 0.7013 - dur_error: 0.4814 - maestro_dur_loss: 0.0241 - val_loss: 0.1128 - val_f1_score_mod: 0.2035 - val_recall_mod: 0.1170 - val_precision_mod: 0.7884 - val_dur_error: 0.3828 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11108 to 0.11067, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 133s - loss: 0.1194 - f1_score_mod: 0.2168 - recall_mod: 0.1289 - precision_mod: 0.7014 - dur_error: 0.4730 - maestro_dur_loss: 0.0237 - val_loss: 0.1107 - val_f1_score_mod: 0.2018 - val_recall_mod: 0.1158 - val_precision_mod: 0.7962 - val_dur_error: 0.3556 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.11067 to 0.11010, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1189 - f1_score_mod: 0.2244 - recall_mod: 0.1336 - precision_mod: 0.7104 - dur_error: 0.4735 - maestro_dur_loss: 0.0237 - val_loss: 0.1101 - val_f1_score_mod: 0.2448 - val_recall_mod: 0.1467 - val_precision_mod: 0.7463 - val_dur_error: 0.3549 - val_maestro_dur_loss: 0.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11010\n",
      "25307/25307 - 142s - loss: 0.1173 - f1_score_mod: 0.2330 - recall_mod: 0.1397 - precision_mod: 0.7134 - dur_error: 0.4586 - maestro_dur_loss: 0.0229 - val_loss: 0.1118 - val_f1_score_mod: 0.2327 - val_recall_mod: 0.1377 - val_precision_mod: 0.7615 - val_dur_error: 0.4033 - val_maestro_dur_loss: 0.0202\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11010 to 0.10953, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 133s - loss: 0.1165 - f1_score_mod: 0.2395 - recall_mod: 0.1444 - precision_mod: 0.7149 - dur_error: 0.4559 - maestro_dur_loss: 0.0228 - val_loss: 0.1095 - val_f1_score_mod: 0.2342 - val_recall_mod: 0.1391 - val_precision_mod: 0.7529 - val_dur_error: 0.3539 - val_maestro_dur_loss: 0.0177\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10953 to 0.10745, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1158 - f1_score_mod: 0.2471 - recall_mod: 0.1499 - precision_mod: 0.7136 - dur_error: 0.4538 - maestro_dur_loss: 0.0227 - val_loss: 0.1074 - val_f1_score_mod: 0.2385 - val_recall_mod: 0.1408 - val_precision_mod: 0.7842 - val_dur_error: 0.3244 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10745\n",
      "25307/25307 - 132s - loss: 0.1150 - f1_score_mod: 0.2513 - recall_mod: 0.1528 - precision_mod: 0.7156 - dur_error: 0.4470 - maestro_dur_loss: 0.0224 - val_loss: 0.1094 - val_f1_score_mod: 0.2756 - val_recall_mod: 0.1699 - val_precision_mod: 0.7373 - val_dur_error: 0.3797 - val_maestro_dur_loss: 0.0190\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10745\n",
      "25307/25307 - 132s - loss: 0.1133 - f1_score_mod: 0.2602 - recall_mod: 0.1594 - precision_mod: 0.7121 - dur_error: 0.4349 - maestro_dur_loss: 0.0217 - val_loss: 0.1106 - val_f1_score_mod: 0.2683 - val_recall_mod: 0.1633 - val_precision_mod: 0.7587 - val_dur_error: 0.4094 - val_maestro_dur_loss: 0.0205\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10745\n",
      "25307/25307 - 133s - loss: 0.1133 - f1_score_mod: 0.2638 - recall_mod: 0.1623 - precision_mod: 0.7168 - dur_error: 0.4397 - maestro_dur_loss: 0.0220 - val_loss: 0.1125 - val_f1_score_mod: 0.2783 - val_recall_mod: 0.1705 - val_precision_mod: 0.7649 - val_dur_error: 0.4739 - val_maestro_dur_loss: 0.0237\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10745\n",
      "25307/25307 - 132s - loss: 0.1123 - f1_score_mod: 0.2719 - recall_mod: 0.1681 - precision_mod: 0.7206 - dur_error: 0.4380 - maestro_dur_loss: 0.0219 - val_loss: 0.1101 - val_f1_score_mod: 0.2963 - val_recall_mod: 0.1852 - val_precision_mod: 0.7462 - val_dur_error: 0.4242 - val_maestro_dur_loss: 0.0212\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10745 to 0.10448, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1115 - f1_score_mod: 0.2783 - recall_mod: 0.1731 - precision_mod: 0.7187 - dur_error: 0.4306 - maestro_dur_loss: 0.0215 - val_loss: 0.1045 - val_f1_score_mod: 0.2853 - val_recall_mod: 0.1755 - val_precision_mod: 0.7666 - val_dur_error: 0.3207 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10448 to 0.10418, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1110 - f1_score_mod: 0.2796 - recall_mod: 0.1738 - precision_mod: 0.7223 - dur_error: 0.4306 - maestro_dur_loss: 0.0215 - val_loss: 0.1042 - val_f1_score_mod: 0.2931 - val_recall_mod: 0.1815 - val_precision_mod: 0.7661 - val_dur_error: 0.3211 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10418\n",
      "25307/25307 - 132s - loss: 0.1099 - f1_score_mod: 0.2903 - recall_mod: 0.1817 - precision_mod: 0.7288 - dur_error: 0.4209 - maestro_dur_loss: 0.0210 - val_loss: 0.1062 - val_f1_score_mod: 0.3116 - val_recall_mod: 0.1983 - val_precision_mod: 0.7325 - val_dur_error: 0.3716 - val_maestro_dur_loss: 0.0186\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10418 to 0.10124, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1091 - f1_score_mod: 0.2968 - recall_mod: 0.1867 - precision_mod: 0.7306 - dur_error: 0.4211 - maestro_dur_loss: 0.0211 - val_loss: 0.1012 - val_f1_score_mod: 0.3141 - val_recall_mod: 0.1974 - val_precision_mod: 0.7727 - val_dur_error: 0.2891 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10124\n",
      "25307/25307 - 132s - loss: 0.1082 - f1_score_mod: 0.3022 - recall_mod: 0.1911 - precision_mod: 0.7274 - dur_error: 0.4132 - maestro_dur_loss: 0.0207 - val_loss: 0.1016 - val_f1_score_mod: 0.3158 - val_recall_mod: 0.1988 - val_precision_mod: 0.7745 - val_dur_error: 0.2979 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10124\n",
      "25307/25307 - 132s - loss: 0.1075 - f1_score_mod: 0.3110 - recall_mod: 0.1975 - precision_mod: 0.7364 - dur_error: 0.4138 - maestro_dur_loss: 0.0207 - val_loss: 0.1038 - val_f1_score_mod: 0.3241 - val_recall_mod: 0.2069 - val_precision_mod: 0.7549 - val_dur_error: 0.3566 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10124 to 0.10092, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1069 - f1_score_mod: 0.3129 - recall_mod: 0.1997 - precision_mod: 0.7275 - dur_error: 0.4116 - maestro_dur_loss: 0.0206 - val_loss: 0.1009 - val_f1_score_mod: 0.3227 - val_recall_mod: 0.2037 - val_precision_mod: 0.7821 - val_dur_error: 0.3083 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10092\n",
      "25307/25307 - 132s - loss: 0.1057 - f1_score_mod: 0.3209 - recall_mod: 0.2059 - precision_mod: 0.7311 - dur_error: 0.4025 - maestro_dur_loss: 0.0201 - val_loss: 0.1039 - val_f1_score_mod: 0.3396 - val_recall_mod: 0.2183 - val_precision_mod: 0.7700 - val_dur_error: 0.3629 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10092\n",
      "25307/25307 - 133s - loss: 0.1055 - f1_score_mod: 0.3314 - recall_mod: 0.2142 - precision_mod: 0.7373 - dur_error: 0.4088 - maestro_dur_loss: 0.0204 - val_loss: 0.1027 - val_f1_score_mod: 0.3505 - val_recall_mod: 0.2280 - val_precision_mod: 0.7618 - val_dur_error: 0.3568 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.10092 to 0.09840, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 132s - loss: 0.1042 - f1_score_mod: 0.3359 - recall_mod: 0.2178 - precision_mod: 0.7390 - dur_error: 0.3999 - maestro_dur_loss: 0.0200 - val_loss: 0.0984 - val_f1_score_mod: 0.3387 - val_recall_mod: 0.2184 - val_precision_mod: 0.7639 - val_dur_error: 0.2762 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09840\n",
      "25307/25307 - 131s - loss: 0.1032 - f1_score_mod: 0.3442 - recall_mod: 0.2245 - precision_mod: 0.7418 - dur_error: 0.3954 - maestro_dur_loss: 0.0198 - val_loss: 0.0995 - val_f1_score_mod: 0.3552 - val_recall_mod: 0.2318 - val_precision_mod: 0.7640 - val_dur_error: 0.3084 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09840 to 0.09784, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 131s - loss: 0.1031 - f1_score_mod: 0.3474 - recall_mod: 0.2274 - precision_mod: 0.7402 - dur_error: 0.3999 - maestro_dur_loss: 0.0200 - val_loss: 0.0978 - val_f1_score_mod: 0.3659 - val_recall_mod: 0.2436 - val_precision_mod: 0.7391 - val_dur_error: 0.2803 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09784\n",
      "25307/25307 - 133s - loss: 0.1018 - f1_score_mod: 0.3559 - recall_mod: 0.2345 - precision_mod: 0.7412 - dur_error: 0.3910 - maestro_dur_loss: 0.0196 - val_loss: 0.1035 - val_f1_score_mod: 0.3732 - val_recall_mod: 0.2494 - val_precision_mod: 0.7445 - val_dur_error: 0.4023 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09784\n",
      "25307/25307 - 132s - loss: 0.1012 - f1_score_mod: 0.3585 - recall_mod: 0.2375 - precision_mod: 0.7359 - dur_error: 0.3909 - maestro_dur_loss: 0.0195 - val_loss: 0.0994 - val_f1_score_mod: 0.3739 - val_recall_mod: 0.2487 - val_precision_mod: 0.7557 - val_dur_error: 0.3244 - val_maestro_dur_loss: 0.0162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09784\n",
      "25307/25307 - 132s - loss: 0.1006 - f1_score_mod: 0.3685 - recall_mod: 0.2456 - precision_mod: 0.7415 - dur_error: 0.3928 - maestro_dur_loss: 0.0196 - val_loss: 0.1027 - val_f1_score_mod: 0.3782 - val_recall_mod: 0.2537 - val_precision_mod: 0.7452 - val_dur_error: 0.4005 - val_maestro_dur_loss: 0.0200\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09784\n",
      "25307/25307 - 131s - loss: 0.0994 - f1_score_mod: 0.3728 - recall_mod: 0.2494 - precision_mod: 0.7427 - dur_error: 0.3835 - maestro_dur_loss: 0.0192 - val_loss: 0.0985 - val_f1_score_mod: 0.3694 - val_recall_mod: 0.2439 - val_precision_mod: 0.7670 - val_dur_error: 0.3238 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09784 to 0.09669, saving model to ../models/best_maestro_model_2_1_512_0pt4_cv_0pt1.h5\n",
      "25307/25307 - 131s - loss: 0.0987 - f1_score_mod: 0.3790 - recall_mod: 0.2544 - precision_mod: 0.7473 - dur_error: 0.3854 - maestro_dur_loss: 0.0193 - val_loss: 0.0967 - val_f1_score_mod: 0.3675 - val_recall_mod: 0.2421 - val_precision_mod: 0.7693 - val_dur_error: 0.2879 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09669\n",
      "25307/25307 - 137s - loss: 0.0979 - f1_score_mod: 0.3838 - recall_mod: 0.2596 - precision_mod: 0.7395 - dur_error: 0.3803 - maestro_dur_loss: 0.0190 - val_loss: 0.1022 - val_f1_score_mod: 0.4029 - val_recall_mod: 0.2783 - val_precision_mod: 0.7319 - val_dur_error: 0.4054 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 46/150\n",
      "Batch 5: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3072/25307 - 12s - loss: nan - f1_score_mod: 0.3930 - recall_mod: 0.2681 - precision_mod: 0.7393 - dur_error: 0.3683 - maestro_dur_loss: 0.0184\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(clipvalue = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, not as effective in delaying failure as clipvalue = 0.2. The best results were achieved with clipvalue = 0.2 and lr = 5e-4. Next, we will combine these settings. Check out visualize_performance.ipynb (Figures 1 and 2) for a comparison of the model training between the different approaches so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15133, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 151s - loss: 0.2537 - f1_score_mod: 0.0309 - recall_mod: 0.0437 - precision_mod: 0.0805 - dur_error: 1.0674 - maestro_dur_loss: 0.0534 - val_loss: 0.1513 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6080 - val_maestro_dur_loss: 0.0304\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15133 to 0.14374, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 166s - loss: 0.1688 - f1_score_mod: 0.0053 - recall_mod: 0.0027 - precision_mod: 0.1348 - dur_error: 0.7262 - maestro_dur_loss: 0.0363 - val_loss: 0.1437 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5279 - val_maestro_dur_loss: 0.0264\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14374 to 0.13569, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 139s - loss: 0.1553 - f1_score_mod: 0.0104 - recall_mod: 0.0053 - precision_mod: 0.3870 - dur_error: 0.6448 - maestro_dur_loss: 0.0322 - val_loss: 0.1357 - val_f1_score_mod: 0.0034 - val_recall_mod: 0.0017 - val_precision_mod: 0.6078 - val_dur_error: 0.4518 - val_maestro_dur_loss: 0.0226\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13569\n",
      "25307/25307 - 151s - loss: 0.1479 - f1_score_mod: 0.0275 - recall_mod: 0.0142 - precision_mod: 0.4961 - dur_error: 0.6035 - maestro_dur_loss: 0.0302 - val_loss: 0.1439 - val_f1_score_mod: 0.0286 - val_recall_mod: 0.0146 - val_precision_mod: 0.7755 - val_dur_error: 0.6782 - val_maestro_dur_loss: 0.0339\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13569 to 0.12865, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 157s - loss: 0.1431 - f1_score_mod: 0.0480 - recall_mod: 0.0252 - precision_mod: 0.5515 - dur_error: 0.5789 - maestro_dur_loss: 0.0289 - val_loss: 0.1286 - val_f1_score_mod: 0.0393 - val_recall_mod: 0.0202 - val_precision_mod: 0.8112 - val_dur_error: 0.4150 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12865 to 0.12642, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 157s - loss: 0.1396 - f1_score_mod: 0.0656 - recall_mod: 0.0349 - precision_mod: 0.6019 - dur_error: 0.5624 - maestro_dur_loss: 0.0281 - val_loss: 0.1264 - val_f1_score_mod: 0.0549 - val_recall_mod: 0.0285 - val_precision_mod: 0.8259 - val_dur_error: 0.4080 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12642\n",
      "25307/25307 - 155s - loss: 0.1363 - f1_score_mod: 0.0811 - recall_mod: 0.0437 - precision_mod: 0.6164 - dur_error: 0.5469 - maestro_dur_loss: 0.0273 - val_loss: 0.1350 - val_f1_score_mod: 0.1015 - val_recall_mod: 0.0546 - val_precision_mod: 0.7413 - val_dur_error: 0.6139 - val_maestro_dur_loss: 0.0307\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12642 to 0.12156, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 142s - loss: 0.1337 - f1_score_mod: 0.0994 - recall_mod: 0.0541 - precision_mod: 0.6401 - dur_error: 0.5375 - maestro_dur_loss: 0.0269 - val_loss: 0.1216 - val_f1_score_mod: 0.1068 - val_recall_mod: 0.0576 - val_precision_mod: 0.7823 - val_dur_error: 0.3922 - val_maestro_dur_loss: 0.0196\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12156\n",
      "25307/25307 - 156s - loss: 0.1316 - f1_score_mod: 0.1158 - recall_mod: 0.0639 - precision_mod: 0.6420 - dur_error: 0.5258 - maestro_dur_loss: 0.0263 - val_loss: 0.1222 - val_f1_score_mod: 0.1065 - val_recall_mod: 0.0573 - val_precision_mod: 0.7937 - val_dur_error: 0.4220 - val_maestro_dur_loss: 0.0211\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12156 to 0.12032, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 159s - loss: 0.1299 - f1_score_mod: 0.1273 - recall_mod: 0.0708 - precision_mod: 0.6613 - dur_error: 0.5175 - maestro_dur_loss: 0.0259 - val_loss: 0.1203 - val_f1_score_mod: 0.1664 - val_recall_mod: 0.0939 - val_precision_mod: 0.7444 - val_dur_error: 0.4276 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.12032\n",
      "25307/25307 - 157s - loss: 0.1289 - f1_score_mod: 0.1375 - recall_mod: 0.0773 - precision_mod: 0.6582 - dur_error: 0.5155 - maestro_dur_loss: 0.0258 - val_loss: 0.1241 - val_f1_score_mod: 0.1709 - val_recall_mod: 0.0966 - val_precision_mod: 0.7539 - val_dur_error: 0.5191 - val_maestro_dur_loss: 0.0260\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.12032 to 0.12001, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 163s - loss: 0.1269 - f1_score_mod: 0.1489 - recall_mod: 0.0840 - precision_mod: 0.6689 - dur_error: 0.5017 - maestro_dur_loss: 0.0251 - val_loss: 0.1200 - val_f1_score_mod: 0.1722 - val_recall_mod: 0.0973 - val_precision_mod: 0.7600 - val_dur_error: 0.4566 - val_maestro_dur_loss: 0.0228\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12001 to 0.11419, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 170s - loss: 0.1265 - f1_score_mod: 0.1545 - recall_mod: 0.0874 - precision_mod: 0.6766 - dur_error: 0.5009 - maestro_dur_loss: 0.0250 - val_loss: 0.1142 - val_f1_score_mod: 0.1637 - val_recall_mod: 0.0915 - val_precision_mod: 0.7902 - val_dur_error: 0.3452 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11419 to 0.11401, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 157s - loss: 0.1250 - f1_score_mod: 0.1663 - recall_mod: 0.0951 - precision_mod: 0.6780 - dur_error: 0.4892 - maestro_dur_loss: 0.0245 - val_loss: 0.1140 - val_f1_score_mod: 0.1765 - val_recall_mod: 0.0995 - val_precision_mod: 0.7912 - val_dur_error: 0.3530 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11401 to 0.11123, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 154s - loss: 0.1233 - f1_score_mod: 0.1749 - recall_mod: 0.1006 - precision_mod: 0.6851 - dur_error: 0.4736 - maestro_dur_loss: 0.0237 - val_loss: 0.1112 - val_f1_score_mod: 0.2020 - val_recall_mod: 0.1175 - val_precision_mod: 0.7326 - val_dur_error: 0.3068 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11123\n",
      "25307/25307 - 155s - loss: 0.1223 - f1_score_mod: 0.1811 - recall_mod: 0.1048 - precision_mod: 0.6802 - dur_error: 0.4639 - maestro_dur_loss: 0.0232 - val_loss: 0.1125 - val_f1_score_mod: 0.1873 - val_recall_mod: 0.1067 - val_precision_mod: 0.7743 - val_dur_error: 0.3396 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11123 to 0.11070, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 154s - loss: 0.1215 - f1_score_mod: 0.1877 - recall_mod: 0.1089 - precision_mod: 0.6942 - dur_error: 0.4618 - maestro_dur_loss: 0.0231 - val_loss: 0.1107 - val_f1_score_mod: 0.2156 - val_recall_mod: 0.1266 - val_precision_mod: 0.7398 - val_dur_error: 0.3124 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11070\n",
      "25307/25307 - 157s - loss: 0.1207 - f1_score_mod: 0.1950 - recall_mod: 0.1141 - precision_mod: 0.6872 - dur_error: 0.4594 - maestro_dur_loss: 0.0230 - val_loss: 0.1152 - val_f1_score_mod: 0.2142 - val_recall_mod: 0.1253 - val_precision_mod: 0.7665 - val_dur_error: 0.4223 - val_maestro_dur_loss: 0.0211\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11070\n",
      "25307/25307 - 139s - loss: 0.1202 - f1_score_mod: 0.2003 - recall_mod: 0.1175 - precision_mod: 0.6905 - dur_error: 0.4583 - maestro_dur_loss: 0.0229 - val_loss: 0.1155 - val_f1_score_mod: 0.2033 - val_recall_mod: 0.1168 - val_precision_mod: 0.7917 - val_dur_error: 0.4210 - val_maestro_dur_loss: 0.0210\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.11070 to 0.10778, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 134s - loss: 0.1189 - f1_score_mod: 0.2081 - recall_mod: 0.1227 - precision_mod: 0.6977 - dur_error: 0.4453 - maestro_dur_loss: 0.0223 - val_loss: 0.1078 - val_f1_score_mod: 0.2210 - val_recall_mod: 0.1290 - val_precision_mod: 0.7817 - val_dur_error: 0.2924 - val_maestro_dur_loss: 0.0146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10778 to 0.10766, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1182 - f1_score_mod: 0.2136 - recall_mod: 0.1263 - precision_mod: 0.7008 - dur_error: 0.4418 - maestro_dur_loss: 0.0221 - val_loss: 0.1077 - val_f1_score_mod: 0.2371 - val_recall_mod: 0.1407 - val_precision_mod: 0.7636 - val_dur_error: 0.2991 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10766 to 0.10730, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 154s - loss: 0.1177 - f1_score_mod: 0.2163 - recall_mod: 0.1282 - precision_mod: 0.6968 - dur_error: 0.4390 - maestro_dur_loss: 0.0219 - val_loss: 0.1073 - val_f1_score_mod: 0.2393 - val_recall_mod: 0.1421 - val_precision_mod: 0.7651 - val_dur_error: 0.2966 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10730\n",
      "25307/25307 - 155s - loss: 0.1170 - f1_score_mod: 0.2248 - recall_mod: 0.1339 - precision_mod: 0.7121 - dur_error: 0.4372 - maestro_dur_loss: 0.0219 - val_loss: 0.1133 - val_f1_score_mod: 0.2576 - val_recall_mod: 0.1563 - val_precision_mod: 0.7379 - val_dur_error: 0.4083 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10730 to 0.10618, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 155s - loss: 0.1161 - f1_score_mod: 0.2285 - recall_mod: 0.1366 - precision_mod: 0.7043 - dur_error: 0.4266 - maestro_dur_loss: 0.0213 - val_loss: 0.1062 - val_f1_score_mod: 0.2539 - val_recall_mod: 0.1525 - val_precision_mod: 0.7638 - val_dur_error: 0.2953 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10618\n",
      "25307/25307 - 156s - loss: 0.1157 - f1_score_mod: 0.2361 - recall_mod: 0.1421 - precision_mod: 0.7076 - dur_error: 0.4291 - maestro_dur_loss: 0.0215 - val_loss: 0.1079 - val_f1_score_mod: 0.2364 - val_recall_mod: 0.1394 - val_precision_mod: 0.7905 - val_dur_error: 0.3251 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10618 to 0.10595, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 157s - loss: 0.1148 - f1_score_mod: 0.2390 - recall_mod: 0.1437 - precision_mod: 0.7169 - dur_error: 0.4230 - maestro_dur_loss: 0.0211 - val_loss: 0.1059 - val_f1_score_mod: 0.2631 - val_recall_mod: 0.1605 - val_precision_mod: 0.7411 - val_dur_error: 0.2961 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10595\n",
      "25307/25307 - 157s - loss: 0.1143 - f1_score_mod: 0.2455 - recall_mod: 0.1487 - precision_mod: 0.7130 - dur_error: 0.4203 - maestro_dur_loss: 0.0210 - val_loss: 0.1109 - val_f1_score_mod: 0.2780 - val_recall_mod: 0.1717 - val_precision_mod: 0.7355 - val_dur_error: 0.4085 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10595\n",
      "25307/25307 - 154s - loss: 0.1137 - f1_score_mod: 0.2505 - recall_mod: 0.1525 - precision_mod: 0.7120 - dur_error: 0.4203 - maestro_dur_loss: 0.0210 - val_loss: 0.1092 - val_f1_score_mod: 0.2774 - val_recall_mod: 0.1702 - val_precision_mod: 0.7569 - val_dur_error: 0.3771 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10595 to 0.10469, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 151s - loss: 0.1130 - f1_score_mod: 0.2561 - recall_mod: 0.1563 - precision_mod: 0.7155 - dur_error: 0.4168 - maestro_dur_loss: 0.0208 - val_loss: 0.1047 - val_f1_score_mod: 0.2523 - val_recall_mod: 0.1500 - val_precision_mod: 0.8004 - val_dur_error: 0.2878 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10469\n",
      "25307/25307 - 158s - loss: 0.1126 - f1_score_mod: 0.2562 - recall_mod: 0.1566 - precision_mod: 0.7112 - dur_error: 0.4162 - maestro_dur_loss: 0.0208 - val_loss: 0.1081 - val_f1_score_mod: 0.2770 - val_recall_mod: 0.1696 - val_precision_mod: 0.7602 - val_dur_error: 0.3702 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10469 to 0.10295, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 154s - loss: 0.1120 - f1_score_mod: 0.2627 - recall_mod: 0.1612 - precision_mod: 0.7177 - dur_error: 0.4111 - maestro_dur_loss: 0.0206 - val_loss: 0.1029 - val_f1_score_mod: 0.2752 - val_recall_mod: 0.1688 - val_precision_mod: 0.7588 - val_dur_error: 0.2757 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10295\n",
      "25307/25307 - 155s - loss: 0.1114 - f1_score_mod: 0.2688 - recall_mod: 0.1658 - precision_mod: 0.7160 - dur_error: 0.4096 - maestro_dur_loss: 0.0205 - val_loss: 0.1035 - val_f1_score_mod: 0.2809 - val_recall_mod: 0.1717 - val_precision_mod: 0.7807 - val_dur_error: 0.2922 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10295\n",
      "25307/25307 - 154s - loss: 0.1111 - f1_score_mod: 0.2717 - recall_mod: 0.1676 - precision_mod: 0.7266 - dur_error: 0.4095 - maestro_dur_loss: 0.0205 - val_loss: 0.1067 - val_f1_score_mod: 0.3084 - val_recall_mod: 0.1953 - val_precision_mod: 0.7372 - val_dur_error: 0.3665 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10295\n",
      "25307/25307 - 155s - loss: 0.1104 - f1_score_mod: 0.2801 - recall_mod: 0.1739 - precision_mod: 0.7279 - dur_error: 0.4054 - maestro_dur_loss: 0.0203 - val_loss: 0.1030 - val_f1_score_mod: 0.3017 - val_recall_mod: 0.1892 - val_precision_mod: 0.7505 - val_dur_error: 0.2899 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.10295 to 0.10213, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 171s - loss: 0.1097 - f1_score_mod: 0.2836 - recall_mod: 0.1767 - precision_mod: 0.7233 - dur_error: 0.4014 - maestro_dur_loss: 0.0201 - val_loss: 0.1021 - val_f1_score_mod: 0.2852 - val_recall_mod: 0.1751 - val_precision_mod: 0.7776 - val_dur_error: 0.2814 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10213\n",
      "25307/25307 - 160s - loss: 0.1091 - f1_score_mod: 0.2908 - recall_mod: 0.1822 - precision_mod: 0.7266 - dur_error: 0.3992 - maestro_dur_loss: 0.0200 - val_loss: 0.1072 - val_f1_score_mod: 0.3070 - val_recall_mod: 0.1925 - val_precision_mod: 0.7617 - val_dur_error: 0.3862 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.10213 to 0.10087, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 161s - loss: 0.1087 - f1_score_mod: 0.2891 - recall_mod: 0.1810 - precision_mod: 0.7243 - dur_error: 0.3986 - maestro_dur_loss: 0.0199 - val_loss: 0.1009 - val_f1_score_mod: 0.3029 - val_recall_mod: 0.1896 - val_precision_mod: 0.7587 - val_dur_error: 0.2641 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.10087\n",
      "25307/25307 - 165s - loss: 0.1083 - f1_score_mod: 0.2954 - recall_mod: 0.1860 - precision_mod: 0.7219 - dur_error: 0.3988 - maestro_dur_loss: 0.0199 - val_loss: 0.1014 - val_f1_score_mod: 0.3101 - val_recall_mod: 0.1951 - val_precision_mod: 0.7577 - val_dur_error: 0.2803 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.10087\n",
      "25307/25307 - 162s - loss: 0.1077 - f1_score_mod: 0.3005 - recall_mod: 0.1897 - precision_mod: 0.7268 - dur_error: 0.3973 - maestro_dur_loss: 0.0199 - val_loss: 0.1044 - val_f1_score_mod: 0.3211 - val_recall_mod: 0.2037 - val_precision_mod: 0.7624 - val_dur_error: 0.3547 - val_maestro_dur_loss: 0.0177\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10087\n",
      "25307/25307 - 162s - loss: 0.1070 - f1_score_mod: 0.3053 - recall_mod: 0.1931 - precision_mod: 0.7348 - dur_error: 0.3896 - maestro_dur_loss: 0.0195 - val_loss: 0.1043 - val_f1_score_mod: 0.3186 - val_recall_mod: 0.2019 - val_precision_mod: 0.7584 - val_dur_error: 0.3555 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.10087 to 0.10048, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 165s - loss: 0.1068 - f1_score_mod: 0.3099 - recall_mod: 0.1973 - precision_mod: 0.7302 - dur_error: 0.3934 - maestro_dur_loss: 0.0197 - val_loss: 0.1005 - val_f1_score_mod: 0.3214 - val_recall_mod: 0.2030 - val_precision_mod: 0.7746 - val_dur_error: 0.2802 - val_maestro_dur_loss: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.10048 to 0.10032, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 159s - loss: 0.1061 - f1_score_mod: 0.3124 - recall_mod: 0.1994 - precision_mod: 0.7293 - dur_error: 0.3911 - maestro_dur_loss: 0.0196 - val_loss: 0.1003 - val_f1_score_mod: 0.3262 - val_recall_mod: 0.2080 - val_precision_mod: 0.7616 - val_dur_error: 0.2829 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.10032\n",
      "25307/25307 - 161s - loss: 0.1058 - f1_score_mod: 0.3157 - recall_mod: 0.2020 - precision_mod: 0.7275 - dur_error: 0.3885 - maestro_dur_loss: 0.0194 - val_loss: 0.1022 - val_f1_score_mod: 0.3351 - val_recall_mod: 0.2150 - val_precision_mod: 0.7627 - val_dur_error: 0.3306 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.10032\n",
      "25307/25307 - 168s - loss: 0.1052 - f1_score_mod: 0.3207 - recall_mod: 0.2058 - precision_mod: 0.7299 - dur_error: 0.3883 - maestro_dur_loss: 0.0194 - val_loss: 0.1005 - val_f1_score_mod: 0.3392 - val_recall_mod: 0.2195 - val_precision_mod: 0.7490 - val_dur_error: 0.2906 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.10032 to 0.09899, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 162s - loss: 0.1047 - f1_score_mod: 0.3258 - recall_mod: 0.2100 - precision_mod: 0.7319 - dur_error: 0.3857 - maestro_dur_loss: 0.0193 - val_loss: 0.0990 - val_f1_score_mod: 0.3479 - val_recall_mod: 0.2270 - val_precision_mod: 0.7462 - val_dur_error: 0.2727 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09899 to 0.09892, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 162s - loss: 0.1041 - f1_score_mod: 0.3318 - recall_mod: 0.2146 - precision_mod: 0.7353 - dur_error: 0.3816 - maestro_dur_loss: 0.0191 - val_loss: 0.0989 - val_f1_score_mod: 0.3480 - val_recall_mod: 0.2273 - val_precision_mod: 0.7468 - val_dur_error: 0.2786 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09892 to 0.09826, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 162s - loss: 0.1036 - f1_score_mod: 0.3358 - recall_mod: 0.2184 - precision_mod: 0.7309 - dur_error: 0.3819 - maestro_dur_loss: 0.0191 - val_loss: 0.0983 - val_f1_score_mod: 0.3572 - val_recall_mod: 0.2360 - val_precision_mod: 0.7386 - val_dur_error: 0.2690 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09826\n",
      "25307/25307 - 159s - loss: 0.1031 - f1_score_mod: 0.3427 - recall_mod: 0.2239 - precision_mod: 0.7336 - dur_error: 0.3823 - maestro_dur_loss: 0.0191 - val_loss: 0.0994 - val_f1_score_mod: 0.3439 - val_recall_mod: 0.2219 - val_precision_mod: 0.7680 - val_dur_error: 0.2923 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09826 to 0.09721, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 167s - loss: 0.1024 - f1_score_mod: 0.3453 - recall_mod: 0.2259 - precision_mod: 0.7368 - dur_error: 0.3800 - maestro_dur_loss: 0.0190 - val_loss: 0.0972 - val_f1_score_mod: 0.3565 - val_recall_mod: 0.2327 - val_precision_mod: 0.7655 - val_dur_error: 0.2597 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09721\n",
      "25307/25307 - 142s - loss: 0.1020 - f1_score_mod: 0.3494 - recall_mod: 0.2297 - precision_mod: 0.7341 - dur_error: 0.3784 - maestro_dur_loss: 0.0189 - val_loss: 0.0990 - val_f1_score_mod: 0.3569 - val_recall_mod: 0.2324 - val_precision_mod: 0.7704 - val_dur_error: 0.2962 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09721\n",
      "25307/25307 - 131s - loss: 0.1014 - f1_score_mod: 0.3536 - recall_mod: 0.2327 - precision_mod: 0.7387 - dur_error: 0.3743 - maestro_dur_loss: 0.0187 - val_loss: 0.0974 - val_f1_score_mod: 0.3637 - val_recall_mod: 0.2417 - val_precision_mod: 0.7402 - val_dur_error: 0.2738 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09721\n",
      "25307/25307 - 131s - loss: 0.1009 - f1_score_mod: 0.3599 - recall_mod: 0.2387 - precision_mod: 0.7354 - dur_error: 0.3775 - maestro_dur_loss: 0.0189 - val_loss: 0.0972 - val_f1_score_mod: 0.3652 - val_recall_mod: 0.2418 - val_precision_mod: 0.7498 - val_dur_error: 0.2699 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09721\n",
      "25307/25307 - 131s - loss: 0.1005 - f1_score_mod: 0.3624 - recall_mod: 0.2402 - precision_mod: 0.7411 - dur_error: 0.3726 - maestro_dur_loss: 0.0186 - val_loss: 0.0972 - val_f1_score_mod: 0.3729 - val_recall_mod: 0.2485 - val_precision_mod: 0.7500 - val_dur_error: 0.2766 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09721 to 0.09667, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.0999 - f1_score_mod: 0.3679 - recall_mod: 0.2455 - precision_mod: 0.7373 - dur_error: 0.3726 - maestro_dur_loss: 0.0186 - val_loss: 0.0967 - val_f1_score_mod: 0.3805 - val_recall_mod: 0.2550 - val_precision_mod: 0.7536 - val_dur_error: 0.2685 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09667\n",
      "25307/25307 - 134s - loss: 0.0993 - f1_score_mod: 0.3725 - recall_mod: 0.2490 - precision_mod: 0.7416 - dur_error: 0.3704 - maestro_dur_loss: 0.0185 - val_loss: 0.1025 - val_f1_score_mod: 0.3727 - val_recall_mod: 0.2486 - val_precision_mod: 0.7487 - val_dur_error: 0.3861 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09667\n",
      "25307/25307 - 135s - loss: 0.0990 - f1_score_mod: 0.3727 - recall_mod: 0.2495 - precision_mod: 0.7403 - dur_error: 0.3706 - maestro_dur_loss: 0.0185 - val_loss: 0.0993 - val_f1_score_mod: 0.3864 - val_recall_mod: 0.2627 - val_precision_mod: 0.7338 - val_dur_error: 0.3304 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09667 to 0.09607, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.0984 - f1_score_mod: 0.3777 - recall_mod: 0.2543 - precision_mod: 0.7373 - dur_error: 0.3681 - maestro_dur_loss: 0.0184 - val_loss: 0.0961 - val_f1_score_mod: 0.3864 - val_recall_mod: 0.2619 - val_precision_mod: 0.7391 - val_dur_error: 0.2692 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.09607\n",
      "25307/25307 - 133s - loss: 0.0978 - f1_score_mod: 0.3854 - recall_mod: 0.2604 - precision_mod: 0.7445 - dur_error: 0.3683 - maestro_dur_loss: 0.0184 - val_loss: 0.1009 - val_f1_score_mod: 0.4038 - val_recall_mod: 0.2835 - val_precision_mod: 0.7042 - val_dur_error: 0.3629 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09607\n",
      "25307/25307 - 131s - loss: 0.0974 - f1_score_mod: 0.3892 - recall_mod: 0.2642 - precision_mod: 0.7408 - dur_error: 0.3672 - maestro_dur_loss: 0.0184 - val_loss: 0.0998 - val_f1_score_mod: 0.3879 - val_recall_mod: 0.2630 - val_precision_mod: 0.7426 - val_dur_error: 0.3531 - val_maestro_dur_loss: 0.0177\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09607 to 0.09564, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.0970 - f1_score_mod: 0.3918 - recall_mod: 0.2666 - precision_mod: 0.7426 - dur_error: 0.3681 - maestro_dur_loss: 0.0184 - val_loss: 0.0956 - val_f1_score_mod: 0.3874 - val_recall_mod: 0.2628 - val_precision_mod: 0.7441 - val_dur_error: 0.2669 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09564 to 0.09511, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.0966 - f1_score_mod: 0.3936 - recall_mod: 0.2679 - precision_mod: 0.7445 - dur_error: 0.3658 - maestro_dur_loss: 0.0183 - val_loss: 0.0951 - val_f1_score_mod: 0.3983 - val_recall_mod: 0.2735 - val_precision_mod: 0.7340 - val_dur_error: 0.2614 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09511\n",
      "25307/25307 - 131s - loss: 0.0958 - f1_score_mod: 0.4004 - recall_mod: 0.2739 - precision_mod: 0.7464 - dur_error: 0.3650 - maestro_dur_loss: 0.0182 - val_loss: 0.0969 - val_f1_score_mod: 0.4044 - val_recall_mod: 0.2818 - val_precision_mod: 0.7218 - val_dur_error: 0.2907 - val_maestro_dur_loss: 0.0145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.09511\n",
      "25307/25307 - 131s - loss: 0.0957 - f1_score_mod: 0.4027 - recall_mod: 0.2769 - precision_mod: 0.7409 - dur_error: 0.3622 - maestro_dur_loss: 0.0181 - val_loss: 0.0960 - val_f1_score_mod: 0.3992 - val_recall_mod: 0.2752 - val_precision_mod: 0.7302 - val_dur_error: 0.2853 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09511 to 0.09444, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.0949 - f1_score_mod: 0.4080 - recall_mod: 0.2814 - precision_mod: 0.7448 - dur_error: 0.3623 - maestro_dur_loss: 0.0181 - val_loss: 0.0944 - val_f1_score_mod: 0.3999 - val_recall_mod: 0.2770 - val_precision_mod: 0.7213 - val_dur_error: 0.2591 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.09444\n",
      "25307/25307 - 133s - loss: 0.0944 - f1_score_mod: 0.4111 - recall_mod: 0.2849 - precision_mod: 0.7407 - dur_error: 0.3598 - maestro_dur_loss: 0.0180 - val_loss: 0.1004 - val_f1_score_mod: 0.4009 - val_recall_mod: 0.2758 - val_precision_mod: 0.7354 - val_dur_error: 0.3788 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09444\n",
      "25307/25307 - 132s - loss: 0.0936 - f1_score_mod: 0.4152 - recall_mod: 0.2884 - precision_mod: 0.7437 - dur_error: 0.3586 - maestro_dur_loss: 0.0179 - val_loss: 0.0945 - val_f1_score_mod: 0.4130 - val_recall_mod: 0.2893 - val_precision_mod: 0.7230 - val_dur_error: 0.2641 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09444\n",
      "25307/25307 - 131s - loss: 0.0933 - f1_score_mod: 0.4199 - recall_mod: 0.2935 - precision_mod: 0.7407 - dur_error: 0.3568 - maestro_dur_loss: 0.0178 - val_loss: 0.0946 - val_f1_score_mod: 0.4083 - val_recall_mod: 0.2825 - val_precision_mod: 0.7378 - val_dur_error: 0.2634 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09444\n",
      "25307/25307 - 131s - loss: 0.0928 - f1_score_mod: 0.4253 - recall_mod: 0.2975 - precision_mod: 0.7482 - dur_error: 0.3576 - maestro_dur_loss: 0.0179 - val_loss: 0.0968 - val_f1_score_mod: 0.4135 - val_recall_mod: 0.2894 - val_precision_mod: 0.7255 - val_dur_error: 0.3174 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09444\n",
      "25307/25307 - 131s - loss: 0.0920 - f1_score_mod: 0.4280 - recall_mod: 0.3004 - precision_mod: 0.7466 - dur_error: 0.3554 - maestro_dur_loss: 0.0178 - val_loss: 0.0979 - val_f1_score_mod: 0.4087 - val_recall_mod: 0.2876 - val_precision_mod: 0.7099 - val_dur_error: 0.3397 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09444\n",
      "25307/25307 - 132s - loss: 0.0915 - f1_score_mod: 0.4352 - recall_mod: 0.3074 - precision_mod: 0.7472 - dur_error: 0.3537 - maestro_dur_loss: 0.0177 - val_loss: 0.0995 - val_f1_score_mod: 0.4288 - val_recall_mod: 0.3115 - val_precision_mod: 0.6893 - val_dur_error: 0.3690 - val_maestro_dur_loss: 0.0184\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09444\n",
      "25307/25307 - 132s - loss: 0.0912 - f1_score_mod: 0.4360 - recall_mod: 0.3088 - precision_mod: 0.7438 - dur_error: 0.3552 - maestro_dur_loss: 0.0178 - val_loss: 0.0952 - val_f1_score_mod: 0.4312 - val_recall_mod: 0.3160 - val_precision_mod: 0.6801 - val_dur_error: 0.2842 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09444\n",
      "25307/25307 - 132s - loss: 0.0906 - f1_score_mod: 0.4431 - recall_mod: 0.3151 - precision_mod: 0.7479 - dur_error: 0.3510 - maestro_dur_loss: 0.0175 - val_loss: 0.0955 - val_f1_score_mod: 0.4272 - val_recall_mod: 0.3070 - val_precision_mod: 0.7046 - val_dur_error: 0.3064 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.09444 to 0.09318, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.0898 - f1_score_mod: 0.4488 - recall_mod: 0.3208 - precision_mod: 0.7486 - dur_error: 0.3516 - maestro_dur_loss: 0.0176 - val_loss: 0.0932 - val_f1_score_mod: 0.4249 - val_recall_mod: 0.3026 - val_precision_mod: 0.7148 - val_dur_error: 0.2544 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.09318\n",
      "25307/25307 - 136s - loss: 0.0895 - f1_score_mod: 0.4482 - recall_mod: 0.3211 - precision_mod: 0.7443 - dur_error: 0.3513 - maestro_dur_loss: 0.0176 - val_loss: 0.0938 - val_f1_score_mod: 0.4220 - val_recall_mod: 0.2983 - val_precision_mod: 0.7227 - val_dur_error: 0.2620 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.09318\n",
      "25307/25307 - 131s - loss: 0.0887 - f1_score_mod: 0.4579 - recall_mod: 0.3297 - precision_mod: 0.7516 - dur_error: 0.3459 - maestro_dur_loss: 0.0173 - val_loss: 0.0939 - val_f1_score_mod: 0.4360 - val_recall_mod: 0.3166 - val_precision_mod: 0.7010 - val_dur_error: 0.2703 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09318\n",
      "25307/25307 - 132s - loss: 0.0881 - f1_score_mod: 0.4639 - recall_mod: 0.3357 - precision_mod: 0.7518 - dur_error: 0.3456 - maestro_dur_loss: 0.0173 - val_loss: 0.0968 - val_f1_score_mod: 0.4376 - val_recall_mod: 0.3224 - val_precision_mod: 0.6833 - val_dur_error: 0.3188 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.09318 to 0.09263, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.0877 - f1_score_mod: 0.4653 - recall_mod: 0.3381 - precision_mod: 0.7477 - dur_error: 0.3469 - maestro_dur_loss: 0.0173 - val_loss: 0.0926 - val_f1_score_mod: 0.4425 - val_recall_mod: 0.3225 - val_precision_mod: 0.7055 - val_dur_error: 0.2499 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09263\n",
      "25307/25307 - 132s - loss: 0.0872 - f1_score_mod: 0.4677 - recall_mod: 0.3410 - precision_mod: 0.7456 - dur_error: 0.3461 - maestro_dur_loss: 0.0173 - val_loss: 0.0939 - val_f1_score_mod: 0.4444 - val_recall_mod: 0.3254 - val_precision_mod: 0.7016 - val_dur_error: 0.2788 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.09263\n",
      "25307/25307 - 134s - loss: 0.0863 - f1_score_mod: 0.4746 - recall_mod: 0.3482 - precision_mod: 0.7466 - dur_error: 0.3423 - maestro_dur_loss: 0.0171 - val_loss: 0.0935 - val_f1_score_mod: 0.4321 - val_recall_mod: 0.3083 - val_precision_mod: 0.7234 - val_dur_error: 0.2707 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09263 to 0.09210, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.0857 - f1_score_mod: 0.4803 - recall_mod: 0.3528 - precision_mod: 0.7536 - dur_error: 0.3434 - maestro_dur_loss: 0.0172 - val_loss: 0.0921 - val_f1_score_mod: 0.4518 - val_recall_mod: 0.3343 - val_precision_mod: 0.6978 - val_dur_error: 0.2450 - val_maestro_dur_loss: 0.0122\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09210\n",
      "25307/25307 - 131s - loss: 0.0852 - f1_score_mod: 0.4859 - recall_mod: 0.3588 - precision_mod: 0.7542 - dur_error: 0.3409 - maestro_dur_loss: 0.0170 - val_loss: 0.0930 - val_f1_score_mod: 0.4604 - val_recall_mod: 0.3446 - val_precision_mod: 0.6973 - val_dur_error: 0.2566 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 82/150\n",
      "Batch 41: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "21504/25307 - 97s - loss: nan - f1_score_mod: 0.4924 - recall_mod: 0.3655 - precision_mod: 0.7564 - dur_error: 0.3416 - maestro_dur_loss: 0.0171\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipvalue = 0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This produced the best model yet, but barely. As a final tuning step, I will vary the dropout_rate because I have a hunch that reducing dropout_rate may make the training more stable and perhaps delay model failure by NaN loss. For a visual comparison of performance with models using each setting on its own, see Figures 3 and 4 from visualize_performance.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14350, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 149s - loss: 0.2354 - f1_score_mod: 0.0213 - recall_mod: 0.0316 - precision_mod: 0.0808 - dur_error: 1.0024 - maestro_dur_loss: 0.0501 - val_loss: 0.1435 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4397 - val_maestro_dur_loss: 0.0220\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14350 to 0.14141, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 134s - loss: 0.1637 - f1_score_mod: 0.0024 - recall_mod: 0.0012 - precision_mod: 0.1509 - dur_error: 0.7021 - maestro_dur_loss: 0.0351 - val_loss: 0.1414 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4464 - val_maestro_dur_loss: 0.0223\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14141\n",
      "25307/25307 - 133s - loss: 0.1528 - f1_score_mod: 0.0046 - recall_mod: 0.0023 - precision_mod: 0.3591 - dur_error: 0.6196 - maestro_dur_loss: 0.0310 - val_loss: 0.1416 - val_f1_score_mod: 0.0068 - val_recall_mod: 0.0034 - val_precision_mod: 0.6997 - val_dur_error: 0.5596 - val_maestro_dur_loss: 0.0280\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14141 to 0.13380, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1459 - f1_score_mod: 0.0244 - recall_mod: 0.0127 - precision_mod: 0.4631 - dur_error: 0.5805 - maestro_dur_loss: 0.0290 - val_loss: 0.1338 - val_f1_score_mod: 0.0335 - val_recall_mod: 0.0172 - val_precision_mod: 0.6662 - val_dur_error: 0.4642 - val_maestro_dur_loss: 0.0232\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13380\n",
      "25307/25307 - 132s - loss: 0.1644 - f1_score_mod: 0.0265 - recall_mod: 0.0138 - precision_mod: 0.4165 - dur_error: 0.8715 - maestro_dur_loss: 0.0436 - val_loss: 0.1341 - val_f1_score_mod: 0.0186 - val_recall_mod: 0.0094 - val_precision_mod: 0.8004 - val_dur_error: 0.4592 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13380 to 0.13345, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1432 - f1_score_mod: 0.0308 - recall_mod: 0.0159 - precision_mod: 0.5566 - dur_error: 0.5828 - maestro_dur_loss: 0.0291 - val_loss: 0.1334 - val_f1_score_mod: 0.0432 - val_recall_mod: 0.0224 - val_precision_mod: 0.7561 - val_dur_error: 0.4931 - val_maestro_dur_loss: 0.0247\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13345\n",
      "25307/25307 - 132s - loss: 0.1393 - f1_score_mod: 0.0475 - recall_mod: 0.0248 - precision_mod: 0.6029 - dur_error: 0.5577 - maestro_dur_loss: 0.0279 - val_loss: 0.1383 - val_f1_score_mod: 0.0875 - val_recall_mod: 0.0471 - val_precision_mod: 0.6241 - val_dur_error: 0.5766 - val_maestro_dur_loss: 0.0288\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.13345 to 0.12775, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1364 - f1_score_mod: 0.0625 - recall_mod: 0.0331 - precision_mod: 0.6253 - dur_error: 0.5398 - maestro_dur_loss: 0.0270 - val_loss: 0.1278 - val_f1_score_mod: 0.0742 - val_recall_mod: 0.0392 - val_precision_mod: 0.7286 - val_dur_error: 0.4584 - val_maestro_dur_loss: 0.0229\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12775 to 0.12583, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1345 - f1_score_mod: 0.0775 - recall_mod: 0.0415 - precision_mod: 0.6326 - dur_error: 0.5322 - maestro_dur_loss: 0.0266 - val_loss: 0.1258 - val_f1_score_mod: 0.0835 - val_recall_mod: 0.0443 - val_precision_mod: 0.7350 - val_dur_error: 0.4388 - val_maestro_dur_loss: 0.0219\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12583\n",
      "25307/25307 - 132s - loss: 0.1325 - f1_score_mod: 0.0888 - recall_mod: 0.0480 - precision_mod: 0.6503 - dur_error: 0.5217 - maestro_dur_loss: 0.0261 - val_loss: 0.1269 - val_f1_score_mod: 0.0730 - val_recall_mod: 0.0384 - val_precision_mod: 0.8013 - val_dur_error: 0.4472 - val_maestro_dur_loss: 0.0224\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12583 to 0.12127, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1309 - f1_score_mod: 0.1014 - recall_mod: 0.0553 - precision_mod: 0.6462 - dur_error: 0.5137 - maestro_dur_loss: 0.0257 - val_loss: 0.1213 - val_f1_score_mod: 0.1093 - val_recall_mod: 0.0592 - val_precision_mod: 0.7620 - val_dur_error: 0.3906 - val_maestro_dur_loss: 0.0195\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12127\n",
      "25307/25307 - 132s - loss: 0.1294 - f1_score_mod: 0.1162 - recall_mod: 0.0640 - precision_mod: 0.6661 - dur_error: 0.5090 - maestro_dur_loss: 0.0255 - val_loss: 0.1269 - val_f1_score_mod: 0.1533 - val_recall_mod: 0.0867 - val_precision_mod: 0.6771 - val_dur_error: 0.5025 - val_maestro_dur_loss: 0.0251\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12127\n",
      "25307/25307 - 132s - loss: 0.1277 - f1_score_mod: 0.1308 - recall_mod: 0.0729 - precision_mod: 0.6759 - dur_error: 0.4992 - maestro_dur_loss: 0.0250 - val_loss: 0.1218 - val_f1_score_mod: 0.1710 - val_recall_mod: 0.0976 - val_precision_mod: 0.7058 - val_dur_error: 0.4423 - val_maestro_dur_loss: 0.0221\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.12127 to 0.11762, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1262 - f1_score_mod: 0.1439 - recall_mod: 0.0807 - precision_mod: 0.6798 - dur_error: 0.4917 - maestro_dur_loss: 0.0246 - val_loss: 0.1176 - val_f1_score_mod: 0.1861 - val_recall_mod: 0.1077 - val_precision_mod: 0.6937 - val_dur_error: 0.3811 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11762\n",
      "25307/25307 - 131s - loss: 0.1248 - f1_score_mod: 0.1562 - recall_mod: 0.0884 - precision_mod: 0.6827 - dur_error: 0.4846 - maestro_dur_loss: 0.0242 - val_loss: 0.1197 - val_f1_score_mod: 0.2023 - val_recall_mod: 0.1183 - val_precision_mod: 0.7052 - val_dur_error: 0.4321 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11762 to 0.11494, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1232 - f1_score_mod: 0.1637 - recall_mod: 0.0933 - precision_mod: 0.6819 - dur_error: 0.4723 - maestro_dur_loss: 0.0236 - val_loss: 0.1149 - val_f1_score_mod: 0.1802 - val_recall_mod: 0.1029 - val_precision_mod: 0.7349 - val_dur_error: 0.3631 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11494\n",
      "25307/25307 - 131s - loss: 0.1217 - f1_score_mod: 0.1735 - recall_mod: 0.0995 - precision_mod: 0.6874 - dur_error: 0.4625 - maestro_dur_loss: 0.0231 - val_loss: 0.1185 - val_f1_score_mod: 0.2151 - val_recall_mod: 0.1277 - val_precision_mod: 0.6899 - val_dur_error: 0.4309 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11494 to 0.11413, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1207 - f1_score_mod: 0.1824 - recall_mod: 0.1053 - precision_mod: 0.6978 - dur_error: 0.4568 - maestro_dur_loss: 0.0228 - val_loss: 0.1141 - val_f1_score_mod: 0.1995 - val_recall_mod: 0.1151 - val_precision_mod: 0.7574 - val_dur_error: 0.3832 - val_maestro_dur_loss: 0.0192\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11413 to 0.11094, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1193 - f1_score_mod: 0.1923 - recall_mod: 0.1119 - precision_mod: 0.6970 - dur_error: 0.4448 - maestro_dur_loss: 0.0222 - val_loss: 0.1109 - val_f1_score_mod: 0.1895 - val_recall_mod: 0.1079 - val_precision_mod: 0.7923 - val_dur_error: 0.3205 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.11094 to 0.10963, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1179 - f1_score_mod: 0.2008 - recall_mod: 0.1175 - precision_mod: 0.7007 - dur_error: 0.4326 - maestro_dur_loss: 0.0216 - val_loss: 0.1096 - val_f1_score_mod: 0.1971 - val_recall_mod: 0.1132 - val_precision_mod: 0.7751 - val_dur_error: 0.3108 - val_maestro_dur_loss: 0.0155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10963\n",
      "25307/25307 - 131s - loss: 0.1167 - f1_score_mod: 0.2100 - recall_mod: 0.1236 - precision_mod: 0.7104 - dur_error: 0.4234 - maestro_dur_loss: 0.0212 - val_loss: 0.1172 - val_f1_score_mod: 0.2548 - val_recall_mod: 0.1561 - val_precision_mod: 0.7001 - val_dur_error: 0.4616 - val_maestro_dur_loss: 0.0231\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10963\n",
      "25307/25307 - 131s - loss: 0.1160 - f1_score_mod: 0.2176 - recall_mod: 0.1290 - precision_mod: 0.7052 - dur_error: 0.4200 - maestro_dur_loss: 0.0210 - val_loss: 0.1145 - val_f1_score_mod: 0.2527 - val_recall_mod: 0.1539 - val_precision_mod: 0.7110 - val_dur_error: 0.4315 - val_maestro_dur_loss: 0.0216\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10963\n",
      "25307/25307 - 130s - loss: 0.1146 - f1_score_mod: 0.2269 - recall_mod: 0.1354 - precision_mod: 0.7091 - dur_error: 0.4086 - maestro_dur_loss: 0.0204 - val_loss: 0.1111 - val_f1_score_mod: 0.2336 - val_recall_mod: 0.1383 - val_precision_mod: 0.7593 - val_dur_error: 0.3759 - val_maestro_dur_loss: 0.0188\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10963\n",
      "25307/25307 - 131s - loss: 0.1141 - f1_score_mod: 0.2338 - recall_mod: 0.1401 - precision_mod: 0.7125 - dur_error: 0.4124 - maestro_dur_loss: 0.0206 - val_loss: 0.1138 - val_f1_score_mod: 0.2661 - val_recall_mod: 0.1642 - val_precision_mod: 0.7038 - val_dur_error: 0.4307 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10963\n",
      "25307/25307 - 131s - loss: 0.1131 - f1_score_mod: 0.2389 - recall_mod: 0.1438 - precision_mod: 0.7134 - dur_error: 0.4047 - maestro_dur_loss: 0.0202 - val_loss: 0.1113 - val_f1_score_mod: 0.2715 - val_recall_mod: 0.1683 - val_precision_mod: 0.7101 - val_dur_error: 0.3884 - val_maestro_dur_loss: 0.0194\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10963 to 0.10488, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1122 - f1_score_mod: 0.2462 - recall_mod: 0.1491 - precision_mod: 0.7161 - dur_error: 0.3976 - maestro_dur_loss: 0.0199 - val_loss: 0.1049 - val_f1_score_mod: 0.2580 - val_recall_mod: 0.1553 - val_precision_mod: 0.7677 - val_dur_error: 0.2860 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10488\n",
      "25307/25307 - 131s - loss: 0.1112 - f1_score_mod: 0.2520 - recall_mod: 0.1535 - precision_mod: 0.7141 - dur_error: 0.3938 - maestro_dur_loss: 0.0197 - val_loss: 0.1061 - val_f1_score_mod: 0.2582 - val_recall_mod: 0.1564 - val_precision_mod: 0.7473 - val_dur_error: 0.3111 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10488 to 0.10428, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1106 - f1_score_mod: 0.2605 - recall_mod: 0.1594 - precision_mod: 0.7189 - dur_error: 0.3917 - maestro_dur_loss: 0.0196 - val_loss: 0.1043 - val_f1_score_mod: 0.2676 - val_recall_mod: 0.1624 - val_precision_mod: 0.7706 - val_dur_error: 0.2854 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10428\n",
      "25307/25307 - 130s - loss: 0.1099 - f1_score_mod: 0.2665 - recall_mod: 0.1640 - precision_mod: 0.7182 - dur_error: 0.3904 - maestro_dur_loss: 0.0195 - val_loss: 0.1065 - val_f1_score_mod: 0.2932 - val_recall_mod: 0.1828 - val_precision_mod: 0.7450 - val_dur_error: 0.3333 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10428 to 0.10390, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1093 - f1_score_mod: 0.2737 - recall_mod: 0.1691 - precision_mod: 0.7237 - dur_error: 0.3894 - maestro_dur_loss: 0.0195 - val_loss: 0.1039 - val_f1_score_mod: 0.2990 - val_recall_mod: 0.1876 - val_precision_mod: 0.7430 - val_dur_error: 0.2984 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.10390 to 0.10294, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1083 - f1_score_mod: 0.2811 - recall_mod: 0.1748 - precision_mod: 0.7220 - dur_error: 0.3837 - maestro_dur_loss: 0.0192 - val_loss: 0.1029 - val_f1_score_mod: 0.2903 - val_recall_mod: 0.1795 - val_precision_mod: 0.7628 - val_dur_error: 0.2850 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10294\n",
      "25307/25307 - 131s - loss: 0.1077 - f1_score_mod: 0.2873 - recall_mod: 0.1793 - precision_mod: 0.7275 - dur_error: 0.3826 - maestro_dur_loss: 0.0191 - val_loss: 0.1033 - val_f1_score_mod: 0.2898 - val_recall_mod: 0.1800 - val_precision_mod: 0.7499 - val_dur_error: 0.2938 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10294\n",
      "25307/25307 - 131s - loss: 0.1069 - f1_score_mod: 0.2925 - recall_mod: 0.1833 - precision_mod: 0.7290 - dur_error: 0.3781 - maestro_dur_loss: 0.0189 - val_loss: 0.1036 - val_f1_score_mod: 0.3099 - val_recall_mod: 0.1973 - val_precision_mod: 0.7272 - val_dur_error: 0.3087 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10294 to 0.10293, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1063 - f1_score_mod: 0.3018 - recall_mod: 0.1903 - precision_mod: 0.7345 - dur_error: 0.3787 - maestro_dur_loss: 0.0189 - val_loss: 0.1029 - val_f1_score_mod: 0.3132 - val_recall_mod: 0.1979 - val_precision_mod: 0.7556 - val_dur_error: 0.3052 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10293\n",
      "25307/25307 - 132s - loss: 0.1052 - f1_score_mod: 0.3072 - recall_mod: 0.1947 - precision_mod: 0.7324 - dur_error: 0.3706 - maestro_dur_loss: 0.0185 - val_loss: 0.1061 - val_f1_score_mod: 0.3263 - val_recall_mod: 0.2097 - val_precision_mod: 0.7419 - val_dur_error: 0.3708 - val_maestro_dur_loss: 0.0185\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.10293 to 0.10140, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1048 - f1_score_mod: 0.3149 - recall_mod: 0.2009 - precision_mod: 0.7335 - dur_error: 0.3724 - maestro_dur_loss: 0.0186 - val_loss: 0.1014 - val_f1_score_mod: 0.3061 - val_recall_mod: 0.1906 - val_precision_mod: 0.7826 - val_dur_error: 0.2883 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10140\n",
      "25307/25307 - 132s - loss: 0.1040 - f1_score_mod: 0.3162 - recall_mod: 0.2018 - precision_mod: 0.7341 - dur_error: 0.3675 - maestro_dur_loss: 0.0184 - val_loss: 0.1103 - val_f1_score_mod: 0.3382 - val_recall_mod: 0.2219 - val_precision_mod: 0.7145 - val_dur_error: 0.4663 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10140 to 0.09950, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1035 - f1_score_mod: 0.3286 - recall_mod: 0.2122 - precision_mod: 0.7316 - dur_error: 0.3701 - maestro_dur_loss: 0.0185 - val_loss: 0.0995 - val_f1_score_mod: 0.3190 - val_recall_mod: 0.2010 - val_precision_mod: 0.7791 - val_dur_error: 0.2673 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09950\n",
      "25307/25307 - 132s - loss: 0.1027 - f1_score_mod: 0.3309 - recall_mod: 0.2138 - precision_mod: 0.7355 - dur_error: 0.3669 - maestro_dur_loss: 0.0183 - val_loss: 0.0996 - val_f1_score_mod: 0.3280 - val_recall_mod: 0.2096 - val_precision_mod: 0.7593 - val_dur_error: 0.2740 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09950\n",
      "25307/25307 - 132s - loss: 0.1018 - f1_score_mod: 0.3403 - recall_mod: 0.2217 - precision_mod: 0.7374 - dur_error: 0.3606 - maestro_dur_loss: 0.0180 - val_loss: 0.1025 - val_f1_score_mod: 0.3530 - val_recall_mod: 0.2348 - val_precision_mod: 0.7153 - val_dur_error: 0.3341 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09950 to 0.09896, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1013 - f1_score_mod: 0.3452 - recall_mod: 0.2250 - precision_mod: 0.7443 - dur_error: 0.3601 - maestro_dur_loss: 0.0180 - val_loss: 0.0990 - val_f1_score_mod: 0.3475 - val_recall_mod: 0.2248 - val_precision_mod: 0.7711 - val_dur_error: 0.2792 - val_maestro_dur_loss: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09896 to 0.09818, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1005 - f1_score_mod: 0.3508 - recall_mod: 0.2303 - precision_mod: 0.7395 - dur_error: 0.3549 - maestro_dur_loss: 0.0177 - val_loss: 0.0982 - val_f1_score_mod: 0.3538 - val_recall_mod: 0.2331 - val_precision_mod: 0.7377 - val_dur_error: 0.2710 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09818 to 0.09783, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1001 - f1_score_mod: 0.3560 - recall_mod: 0.2343 - precision_mod: 0.7437 - dur_error: 0.3620 - maestro_dur_loss: 0.0181 - val_loss: 0.0978 - val_f1_score_mod: 0.3598 - val_recall_mod: 0.2388 - val_precision_mod: 0.7331 - val_dur_error: 0.2702 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09783\n",
      "25307/25307 - 131s - loss: 0.0993 - f1_score_mod: 0.3630 - recall_mod: 0.2405 - precision_mod: 0.7434 - dur_error: 0.3562 - maestro_dur_loss: 0.0178 - val_loss: 0.0979 - val_f1_score_mod: 0.3529 - val_recall_mod: 0.2297 - val_precision_mod: 0.7661 - val_dur_error: 0.2705 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09783 to 0.09748, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.0985 - f1_score_mod: 0.3686 - recall_mod: 0.2452 - precision_mod: 0.7453 - dur_error: 0.3511 - maestro_dur_loss: 0.0176 - val_loss: 0.0975 - val_f1_score_mod: 0.3661 - val_recall_mod: 0.2438 - val_precision_mod: 0.7383 - val_dur_error: 0.2670 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09748\n",
      "25307/25307 - 131s - loss: 0.0977 - f1_score_mod: 0.3745 - recall_mod: 0.2501 - precision_mod: 0.7479 - dur_error: 0.3498 - maestro_dur_loss: 0.0175 - val_loss: 0.0984 - val_f1_score_mod: 0.3867 - val_recall_mod: 0.2653 - val_precision_mod: 0.7145 - val_dur_error: 0.2958 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09748\n",
      "25307/25307 - 131s - loss: 0.0974 - f1_score_mod: 0.3799 - recall_mod: 0.2555 - precision_mod: 0.7434 - dur_error: 0.3525 - maestro_dur_loss: 0.0176 - val_loss: 0.1041 - val_f1_score_mod: 0.3806 - val_recall_mod: 0.2587 - val_precision_mod: 0.7220 - val_dur_error: 0.4070 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09748\n",
      "25307/25307 - 132s - loss: 0.0965 - f1_score_mod: 0.3852 - recall_mod: 0.2604 - precision_mod: 0.7436 - dur_error: 0.3472 - maestro_dur_loss: 0.0174 - val_loss: 0.1004 - val_f1_score_mod: 0.3845 - val_recall_mod: 0.2611 - val_precision_mod: 0.7328 - val_dur_error: 0.3512 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09748 to 0.09740, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.0959 - f1_score_mod: 0.3896 - recall_mod: 0.2639 - precision_mod: 0.7461 - dur_error: 0.3475 - maestro_dur_loss: 0.0174 - val_loss: 0.0974 - val_f1_score_mod: 0.3958 - val_recall_mod: 0.2747 - val_precision_mod: 0.7103 - val_dur_error: 0.2815 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09740 to 0.09563, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.0954 - f1_score_mod: 0.3937 - recall_mod: 0.2679 - precision_mod: 0.7472 - dur_error: 0.3455 - maestro_dur_loss: 0.0173 - val_loss: 0.0956 - val_f1_score_mod: 0.3902 - val_recall_mod: 0.2660 - val_precision_mod: 0.7343 - val_dur_error: 0.2635 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09563\n",
      "25307/25307 - 131s - loss: 0.0946 - f1_score_mod: 0.3981 - recall_mod: 0.2721 - precision_mod: 0.7445 - dur_error: 0.3421 - maestro_dur_loss: 0.0171 - val_loss: 0.0998 - val_f1_score_mod: 0.4002 - val_recall_mod: 0.2790 - val_precision_mod: 0.7093 - val_dur_error: 0.3371 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09563 to 0.09527, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.0939 - f1_score_mod: 0.4069 - recall_mod: 0.2797 - precision_mod: 0.7497 - dur_error: 0.3407 - maestro_dur_loss: 0.0170 - val_loss: 0.0953 - val_f1_score_mod: 0.3904 - val_recall_mod: 0.2652 - val_precision_mod: 0.7433 - val_dur_error: 0.2633 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09527\n",
      "25307/25307 - 131s - loss: 0.0932 - f1_score_mod: 0.4128 - recall_mod: 0.2849 - precision_mod: 0.7513 - dur_error: 0.3432 - maestro_dur_loss: 0.0172 - val_loss: 0.0986 - val_f1_score_mod: 0.4098 - val_recall_mod: 0.2882 - val_precision_mod: 0.7114 - val_dur_error: 0.3338 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09527 to 0.09485, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.0924 - f1_score_mod: 0.4171 - recall_mod: 0.2891 - precision_mod: 0.7505 - dur_error: 0.3376 - maestro_dur_loss: 0.0169 - val_loss: 0.0949 - val_f1_score_mod: 0.4024 - val_recall_mod: 0.2790 - val_precision_mod: 0.7246 - val_dur_error: 0.2654 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09485\n",
      "25307/25307 - 132s - loss: 0.0917 - f1_score_mod: 0.4254 - recall_mod: 0.2969 - precision_mod: 0.7528 - dur_error: 0.3381 - maestro_dur_loss: 0.0169 - val_loss: 0.0964 - val_f1_score_mod: 0.4050 - val_recall_mod: 0.2810 - val_precision_mod: 0.7262 - val_dur_error: 0.2931 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09485 to 0.09440, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.0912 - f1_score_mod: 0.4286 - recall_mod: 0.2999 - precision_mod: 0.7530 - dur_error: 0.3373 - maestro_dur_loss: 0.0169 - val_loss: 0.0944 - val_f1_score_mod: 0.4065 - val_recall_mod: 0.2834 - val_precision_mod: 0.7216 - val_dur_error: 0.2630 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09440 to 0.09381, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.0906 - f1_score_mod: 0.4325 - recall_mod: 0.3049 - precision_mod: 0.7473 - dur_error: 0.3357 - maestro_dur_loss: 0.0168 - val_loss: 0.0938 - val_f1_score_mod: 0.4194 - val_recall_mod: 0.2978 - val_precision_mod: 0.7102 - val_dur_error: 0.2529 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0898 - f1_score_mod: 0.4386 - recall_mod: 0.3100 - precision_mod: 0.7515 - dur_error: 0.3324 - maestro_dur_loss: 0.0166 - val_loss: 0.0945 - val_f1_score_mod: 0.4219 - val_recall_mod: 0.3007 - val_precision_mod: 0.7082 - val_dur_error: 0.2723 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0892 - f1_score_mod: 0.4446 - recall_mod: 0.3158 - precision_mod: 0.7532 - dur_error: 0.3323 - maestro_dur_loss: 0.0166 - val_loss: 0.0950 - val_f1_score_mod: 0.4259 - val_recall_mod: 0.3051 - val_precision_mod: 0.7095 - val_dur_error: 0.2860 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0883 - f1_score_mod: 0.4522 - recall_mod: 0.3230 - precision_mod: 0.7559 - dur_error: 0.3305 - maestro_dur_loss: 0.0165 - val_loss: 0.1000 - val_f1_score_mod: 0.4314 - val_recall_mod: 0.3124 - val_precision_mod: 0.6976 - val_dur_error: 0.3884 - val_maestro_dur_loss: 0.0194\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0881 - f1_score_mod: 0.4577 - recall_mod: 0.3292 - precision_mod: 0.7528 - dur_error: 0.3318 - maestro_dur_loss: 0.0166 - val_loss: 0.0977 - val_f1_score_mod: 0.4272 - val_recall_mod: 0.3090 - val_precision_mod: 0.6938 - val_dur_error: 0.3380 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0872 - f1_score_mod: 0.4604 - recall_mod: 0.3328 - precision_mod: 0.7484 - dur_error: 0.3289 - maestro_dur_loss: 0.0164 - val_loss: 0.0939 - val_f1_score_mod: 0.4290 - val_recall_mod: 0.3061 - val_precision_mod: 0.7185 - val_dur_error: 0.2632 - val_maestro_dur_loss: 0.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0866 - f1_score_mod: 0.4693 - recall_mod: 0.3409 - precision_mod: 0.7552 - dur_error: 0.3300 - maestro_dur_loss: 0.0165 - val_loss: 0.0989 - val_f1_score_mod: 0.4352 - val_recall_mod: 0.3166 - val_precision_mod: 0.6984 - val_dur_error: 0.3722 - val_maestro_dur_loss: 0.0186\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.09381\n",
      "25307/25307 - 133s - loss: 0.0855 - f1_score_mod: 0.4742 - recall_mod: 0.3457 - precision_mod: 0.7563 - dur_error: 0.3241 - maestro_dur_loss: 0.0162 - val_loss: 0.1004 - val_f1_score_mod: 0.4513 - val_recall_mod: 0.3417 - val_precision_mod: 0.6653 - val_dur_error: 0.3879 - val_maestro_dur_loss: 0.0194\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0848 - f1_score_mod: 0.4820 - recall_mod: 0.3541 - precision_mod: 0.7568 - dur_error: 0.3242 - maestro_dur_loss: 0.0162 - val_loss: 0.0944 - val_f1_score_mod: 0.4420 - val_recall_mod: 0.3228 - val_precision_mod: 0.7024 - val_dur_error: 0.2888 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09381\n",
      "25307/25307 - 132s - loss: 0.0840 - f1_score_mod: 0.4873 - recall_mod: 0.3598 - precision_mod: 0.7570 - dur_error: 0.3226 - maestro_dur_loss: 0.0161 - val_loss: 0.0982 - val_f1_score_mod: 0.4442 - val_recall_mod: 0.3283 - val_precision_mod: 0.6877 - val_dur_error: 0.3654 - val_maestro_dur_loss: 0.0183\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09381 to 0.09358, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.0832 - f1_score_mod: 0.4942 - recall_mod: 0.3671 - precision_mod: 0.7581 - dur_error: 0.3197 - maestro_dur_loss: 0.0160 - val_loss: 0.0936 - val_f1_score_mod: 0.4388 - val_recall_mod: 0.3174 - val_precision_mod: 0.7132 - val_dur_error: 0.2717 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09358\n",
      "25307/25307 - 132s - loss: 0.0825 - f1_score_mod: 0.5000 - recall_mod: 0.3732 - precision_mod: 0.7585 - dur_error: 0.3177 - maestro_dur_loss: 0.0159 - val_loss: 0.0948 - val_f1_score_mod: 0.4494 - val_recall_mod: 0.3317 - val_precision_mod: 0.6986 - val_dur_error: 0.2988 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09358\n",
      "25307/25307 - 132s - loss: 0.0818 - f1_score_mod: 0.5081 - recall_mod: 0.3810 - precision_mod: 0.7635 - dur_error: 0.3199 - maestro_dur_loss: 0.0160 - val_loss: 0.0964 - val_f1_score_mod: 0.4594 - val_recall_mod: 0.3440 - val_precision_mod: 0.6925 - val_dur_error: 0.3323 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09358\n",
      "25307/25307 - 132s - loss: 0.0811 - f1_score_mod: 0.5135 - recall_mod: 0.3881 - precision_mod: 0.7599 - dur_error: 0.3176 - maestro_dur_loss: 0.0159 - val_loss: 0.0938 - val_f1_score_mod: 0.4602 - val_recall_mod: 0.3437 - val_precision_mod: 0.6978 - val_dur_error: 0.2805 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09358\n",
      "25307/25307 - 132s - loss: 0.0806 - f1_score_mod: 0.5183 - recall_mod: 0.3923 - precision_mod: 0.7650 - dur_error: 0.3182 - maestro_dur_loss: 0.0159 - val_loss: 0.0952 - val_f1_score_mod: 0.4722 - val_recall_mod: 0.3624 - val_precision_mod: 0.6784 - val_dur_error: 0.3096 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09358\n",
      "25307/25307 - 133s - loss: 0.0799 - f1_score_mod: 0.5239 - recall_mod: 0.3995 - precision_mod: 0.7616 - dur_error: 0.3181 - maestro_dur_loss: 0.0159 - val_loss: 0.0946 - val_f1_score_mod: 0.4624 - val_recall_mod: 0.3457 - val_precision_mod: 0.6986 - val_dur_error: 0.2992 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09358\n",
      "25307/25307 - 132s - loss: 0.0790 - f1_score_mod: 0.5303 - recall_mod: 0.4053 - precision_mod: 0.7678 - dur_error: 0.3147 - maestro_dur_loss: 0.0157 - val_loss: 0.0951 - val_f1_score_mod: 0.4695 - val_recall_mod: 0.3564 - val_precision_mod: 0.6886 - val_dur_error: 0.3164 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09358 to 0.09322, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.0779 - f1_score_mod: 0.5393 - recall_mod: 0.4161 - precision_mod: 0.7674 - dur_error: 0.3108 - maestro_dur_loss: 0.0155 - val_loss: 0.0932 - val_f1_score_mod: 0.4706 - val_recall_mod: 0.3623 - val_precision_mod: 0.6729 - val_dur_error: 0.2676 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0774 - f1_score_mod: 0.5427 - recall_mod: 0.4207 - precision_mod: 0.7656 - dur_error: 0.3106 - maestro_dur_loss: 0.0155 - val_loss: 0.0961 - val_f1_score_mod: 0.4677 - val_recall_mod: 0.3538 - val_precision_mod: 0.6912 - val_dur_error: 0.3389 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 76/150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0767 - f1_score_mod: 0.5489 - recall_mod: 0.4270 - precision_mod: 0.7691 - dur_error: 0.3108 - maestro_dur_loss: 0.0155 - val_loss: 0.0957 - val_f1_score_mod: 0.4815 - val_recall_mod: 0.3750 - val_precision_mod: 0.6732 - val_dur_error: 0.3199 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 77/150\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0760 - f1_score_mod: 0.5558 - recall_mod: 0.4357 - precision_mod: 0.7684 - dur_error: 0.3108 - maestro_dur_loss: 0.0155 - val_loss: 0.0944 - val_f1_score_mod: 0.4803 - val_recall_mod: 0.3716 - val_precision_mod: 0.6797 - val_dur_error: 0.2826 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0750 - f1_score_mod: 0.5625 - recall_mod: 0.4438 - precision_mod: 0.7692 - dur_error: 0.3069 - maestro_dur_loss: 0.0153 - val_loss: 0.0943 - val_f1_score_mod: 0.4829 - val_recall_mod: 0.3770 - val_precision_mod: 0.6723 - val_dur_error: 0.2975 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 79/150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0745 - f1_score_mod: 0.5684 - recall_mod: 0.4494 - precision_mod: 0.7741 - dur_error: 0.3090 - maestro_dur_loss: 0.0154 - val_loss: 0.0937 - val_f1_score_mod: 0.4865 - val_recall_mod: 0.3781 - val_precision_mod: 0.6827 - val_dur_error: 0.2784 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 80/150\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0737 - f1_score_mod: 0.5751 - recall_mod: 0.4573 - precision_mod: 0.7755 - dur_error: 0.3068 - maestro_dur_loss: 0.0153 - val_loss: 0.0974 - val_f1_score_mod: 0.4926 - val_recall_mod: 0.3939 - val_precision_mod: 0.6580 - val_dur_error: 0.3340 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 81/150\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0728 - f1_score_mod: 0.5821 - recall_mod: 0.4671 - precision_mod: 0.7733 - dur_error: 0.3054 - maestro_dur_loss: 0.0153 - val_loss: 0.0934 - val_f1_score_mod: 0.4866 - val_recall_mod: 0.3798 - val_precision_mod: 0.6785 - val_dur_error: 0.2575 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 82/150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.09322\n",
      "25307/25307 - 132s - loss: 0.0721 - f1_score_mod: 0.5862 - recall_mod: 0.4714 - precision_mod: 0.7759 - dur_error: 0.3050 - maestro_dur_loss: 0.0152 - val_loss: 0.0962 - val_f1_score_mod: 0.4966 - val_recall_mod: 0.3958 - val_precision_mod: 0.6670 - val_dur_error: 0.3247 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 83/150\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09322 to 0.09312, saving model to ../models/best_maestro_model_2_1_512_0pt3_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.0713 - f1_score_mod: 0.5926 - recall_mod: 0.4789 - precision_mod: 0.7779 - dur_error: 0.3028 - maestro_dur_loss: 0.0151 - val_loss: 0.0931 - val_f1_score_mod: 0.5015 - val_recall_mod: 0.4000 - val_precision_mod: 0.6732 - val_dur_error: 0.2440 - val_maestro_dur_loss: 0.0122\n",
      "Epoch 84/150\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.09312\n",
      "25307/25307 - 132s - loss: 0.0707 - f1_score_mod: 0.6001 - recall_mod: 0.4889 - precision_mod: 0.7773 - dur_error: 0.3044 - maestro_dur_loss: 0.0152 - val_loss: 0.0953 - val_f1_score_mod: 0.4915 - val_recall_mod: 0.3870 - val_precision_mod: 0.6738 - val_dur_error: 0.2877 - val_maestro_dur_loss: 0.0144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/150\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.09312\n",
      "25307/25307 - 131s - loss: 0.0696 - f1_score_mod: 0.6064 - recall_mod: 0.4960 - precision_mod: 0.7812 - dur_error: 0.3005 - maestro_dur_loss: 0.0150 - val_loss: 0.0934 - val_f1_score_mod: 0.4989 - val_recall_mod: 0.4018 - val_precision_mod: 0.6584 - val_dur_error: 0.2505 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 86/150\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.09312\n",
      "25307/25307 - 132s - loss: 0.0689 - f1_score_mod: 0.6131 - recall_mod: 0.5048 - precision_mod: 0.7816 - dur_error: 0.2992 - maestro_dur_loss: 0.0150 - val_loss: 0.0963 - val_f1_score_mod: 0.5055 - val_recall_mod: 0.4144 - val_precision_mod: 0.6489 - val_dur_error: 0.3018 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 87/150\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.09312\n",
      "25307/25307 - 132s - loss: 0.0684 - f1_score_mod: 0.6189 - recall_mod: 0.5116 - precision_mod: 0.7839 - dur_error: 0.3024 - maestro_dur_loss: 0.0151 - val_loss: 0.0940 - val_f1_score_mod: 0.5052 - val_recall_mod: 0.4096 - val_precision_mod: 0.6595 - val_dur_error: 0.2469 - val_maestro_dur_loss: 0.0123\n",
      "Epoch 88/150\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.09312\n",
      "25307/25307 - 133s - loss: 0.0675 - f1_score_mod: 0.6264 - recall_mod: 0.5208 - precision_mod: 0.7864 - dur_error: 0.2989 - maestro_dur_loss: 0.0149 - val_loss: 0.0947 - val_f1_score_mod: 0.5113 - val_recall_mod: 0.4130 - val_precision_mod: 0.6716 - val_dur_error: 0.2534 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 89/150\n",
      "Batch 31: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "16384/25307 - 74s - loss: nan - f1_score_mod: 0.6339 - recall_mod: 0.5295 - precision_mod: 0.7900 - dur_error: 0.2991 - maestro_dur_loss: 0.0150\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipvalue = 0.2, dropout_rate = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It certainly does delay failure until the training loss is significantly lower. However, since dropout is designed to combat overfitting, lowering it increases overfitting causing the validation loss to remain relatively high. It now makes sense to increase dropout_rate since lowering it did not have a favorable effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15223, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 140s - loss: 0.2556 - f1_score_mod: 0.0329 - recall_mod: 0.0428 - precision_mod: 0.0720 - dur_error: 1.0468 - maestro_dur_loss: 0.0523 - val_loss: 0.1522 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5185 - val_maestro_dur_loss: 0.0259\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15223 to 0.15132, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1740 - f1_score_mod: 0.0079 - recall_mod: 0.0041 - precision_mod: 0.1312 - dur_error: 0.7665 - maestro_dur_loss: 0.0383 - val_loss: 0.1513 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6486 - val_maestro_dur_loss: 0.0324\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15132 to 0.14028, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1597 - f1_score_mod: 0.0059 - recall_mod: 0.0030 - precision_mod: 0.2863 - dur_error: 0.6784 - maestro_dur_loss: 0.0339 - val_loss: 0.1403 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4929 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14028 to 0.13279, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1520 - f1_score_mod: 0.0172 - recall_mod: 0.0088 - precision_mod: 0.4483 - dur_error: 0.6328 - maestro_dur_loss: 0.0316 - val_loss: 0.1328 - val_f1_score_mod: 0.0194 - val_recall_mod: 0.0098 - val_precision_mod: 0.7504 - val_dur_error: 0.4298 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13279 to 0.13214, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1468 - f1_score_mod: 0.0327 - recall_mod: 0.0170 - precision_mod: 0.5185 - dur_error: 0.6017 - maestro_dur_loss: 0.0301 - val_loss: 0.1321 - val_f1_score_mod: 0.0286 - val_recall_mod: 0.0147 - val_precision_mod: 0.8067 - val_dur_error: 0.4300 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13214\n",
      "25307/25307 - 132s - loss: 0.1430 - f1_score_mod: 0.0476 - recall_mod: 0.0249 - precision_mod: 0.5672 - dur_error: 0.5873 - maestro_dur_loss: 0.0294 - val_loss: 0.1375 - val_f1_score_mod: 0.0768 - val_recall_mod: 0.0409 - val_precision_mod: 0.6790 - val_dur_error: 0.5864 - val_maestro_dur_loss: 0.0293\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13214\n",
      "25307/25307 - 132s - loss: 0.1406 - f1_score_mod: 0.0604 - recall_mod: 0.0319 - precision_mod: 0.5940 - dur_error: 0.5767 - maestro_dur_loss: 0.0288 - val_loss: 0.1341 - val_f1_score_mod: 0.0912 - val_recall_mod: 0.0491 - val_precision_mod: 0.6810 - val_dur_error: 0.5645 - val_maestro_dur_loss: 0.0282\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.13214 to 0.12491, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1379 - f1_score_mod: 0.0727 - recall_mod: 0.0389 - precision_mod: 0.6082 - dur_error: 0.5657 - maestro_dur_loss: 0.0283 - val_loss: 0.1249 - val_f1_score_mod: 0.0906 - val_recall_mod: 0.0483 - val_precision_mod: 0.7487 - val_dur_error: 0.4253 - val_maestro_dur_loss: 0.0213\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12491\n",
      "25307/25307 - 132s - loss: 0.1359 - f1_score_mod: 0.0848 - recall_mod: 0.0456 - precision_mod: 0.6305 - dur_error: 0.5549 - maestro_dur_loss: 0.0277 - val_loss: 0.1277 - val_f1_score_mod: 0.0945 - val_recall_mod: 0.0504 - val_precision_mod: 0.7694 - val_dur_error: 0.5099 - val_maestro_dur_loss: 0.0255\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12491\n",
      "25307/25307 - 131s - loss: 0.1343 - f1_score_mod: 0.0958 - recall_mod: 0.0520 - precision_mod: 0.6331 - dur_error: 0.5479 - maestro_dur_loss: 0.0274 - val_loss: 0.1308 - val_f1_score_mod: 0.1185 - val_recall_mod: 0.0646 - val_precision_mod: 0.7322 - val_dur_error: 0.5912 - val_maestro_dur_loss: 0.0296\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12491 to 0.12367, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1322 - f1_score_mod: 0.1052 - recall_mod: 0.0574 - precision_mod: 0.6508 - dur_error: 0.5325 - maestro_dur_loss: 0.0266 - val_loss: 0.1237 - val_f1_score_mod: 0.1222 - val_recall_mod: 0.0666 - val_precision_mod: 0.7675 - val_dur_error: 0.4702 - val_maestro_dur_loss: 0.0235\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.12367 to 0.12003, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1309 - f1_score_mod: 0.1117 - recall_mod: 0.0613 - precision_mod: 0.6520 - dur_error: 0.5223 - maestro_dur_loss: 0.0261 - val_loss: 0.1200 - val_f1_score_mod: 0.1367 - val_recall_mod: 0.0755 - val_precision_mod: 0.7440 - val_dur_error: 0.4054 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12003 to 0.11532, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1295 - f1_score_mod: 0.1190 - recall_mod: 0.0656 - precision_mod: 0.6505 - dur_error: 0.5125 - maestro_dur_loss: 0.0256 - val_loss: 0.1153 - val_f1_score_mod: 0.1242 - val_recall_mod: 0.0675 - val_precision_mod: 0.7974 - val_dur_error: 0.3264 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11532 to 0.11441, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1283 - f1_score_mod: 0.1243 - recall_mod: 0.0689 - precision_mod: 0.6547 - dur_error: 0.5025 - maestro_dur_loss: 0.0251 - val_loss: 0.1144 - val_f1_score_mod: 0.1402 - val_recall_mod: 0.0772 - val_precision_mod: 0.7859 - val_dur_error: 0.3281 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11441\n",
      "25307/25307 - 132s - loss: 0.1271 - f1_score_mod: 0.1323 - recall_mod: 0.0737 - precision_mod: 0.6683 - dur_error: 0.4917 - maestro_dur_loss: 0.0246 - val_loss: 0.1220 - val_f1_score_mod: 0.1783 - val_recall_mod: 0.1018 - val_precision_mod: 0.7285 - val_dur_error: 0.4843 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11441 to 0.11210, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1259 - f1_score_mod: 0.1418 - recall_mod: 0.0795 - precision_mod: 0.6735 - dur_error: 0.4842 - maestro_dur_loss: 0.0242 - val_loss: 0.1121 - val_f1_score_mod: 0.1579 - val_recall_mod: 0.0878 - val_precision_mod: 0.7947 - val_dur_error: 0.3025 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11210\n",
      "25307/25307 - 133s - loss: 0.1252 - f1_score_mod: 0.1462 - recall_mod: 0.0822 - precision_mod: 0.6756 - dur_error: 0.4804 - maestro_dur_loss: 0.0240 - val_loss: 0.1194 - val_f1_score_mod: 0.1859 - val_recall_mod: 0.1062 - val_precision_mod: 0.7540 - val_dur_error: 0.4424 - val_maestro_dur_loss: 0.0221\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11210\n",
      "25307/25307 - 132s - loss: 0.1245 - f1_score_mod: 0.1548 - recall_mod: 0.0875 - precision_mod: 0.6867 - dur_error: 0.4766 - maestro_dur_loss: 0.0238 - val_loss: 0.1150 - val_f1_score_mod: 0.1672 - val_recall_mod: 0.0940 - val_precision_mod: 0.7727 - val_dur_error: 0.3859 - val_maestro_dur_loss: 0.0193\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11210 to 0.11108, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1236 - f1_score_mod: 0.1608 - recall_mod: 0.0913 - precision_mod: 0.6847 - dur_error: 0.4728 - maestro_dur_loss: 0.0236 - val_loss: 0.1111 - val_f1_score_mod: 0.1751 - val_recall_mod: 0.0988 - val_precision_mod: 0.7797 - val_dur_error: 0.3088 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11108\n",
      "25307/25307 - 132s - loss: 0.1227 - f1_score_mod: 0.1633 - recall_mod: 0.0930 - precision_mod: 0.6799 - dur_error: 0.4640 - maestro_dur_loss: 0.0232 - val_loss: 0.1116 - val_f1_score_mod: 0.1919 - val_recall_mod: 0.1100 - val_precision_mod: 0.7646 - val_dur_error: 0.3335 - val_maestro_dur_loss: 0.0167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11108\n",
      "25307/25307 - 132s - loss: 0.1222 - f1_score_mod: 0.1738 - recall_mod: 0.0997 - precision_mod: 0.6892 - dur_error: 0.4635 - maestro_dur_loss: 0.0232 - val_loss: 0.1155 - val_f1_score_mod: 0.2000 - val_recall_mod: 0.1158 - val_precision_mod: 0.7524 - val_dur_error: 0.4077 - val_maestro_dur_loss: 0.0204\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.11108\n",
      "25307/25307 - 132s - loss: 0.1214 - f1_score_mod: 0.1785 - recall_mod: 0.1028 - precision_mod: 0.6880 - dur_error: 0.4582 - maestro_dur_loss: 0.0229 - val_loss: 0.1178 - val_f1_score_mod: 0.2114 - val_recall_mod: 0.1227 - val_precision_mod: 0.7680 - val_dur_error: 0.4658 - val_maestro_dur_loss: 0.0233\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.11108 to 0.10981, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1206 - f1_score_mod: 0.1844 - recall_mod: 0.1065 - precision_mod: 0.6941 - dur_error: 0.4531 - maestro_dur_loss: 0.0227 - val_loss: 0.1098 - val_f1_score_mod: 0.2250 - val_recall_mod: 0.1325 - val_precision_mod: 0.7542 - val_dur_error: 0.3194 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10981\n",
      "25307/25307 - 132s - loss: 0.1203 - f1_score_mod: 0.1890 - recall_mod: 0.1096 - precision_mod: 0.6995 - dur_error: 0.4534 - maestro_dur_loss: 0.0227 - val_loss: 0.1116 - val_f1_score_mod: 0.2164 - val_recall_mod: 0.1269 - val_precision_mod: 0.7495 - val_dur_error: 0.3559 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.10981 to 0.10868, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1194 - f1_score_mod: 0.1960 - recall_mod: 0.1143 - precision_mod: 0.6997 - dur_error: 0.4456 - maestro_dur_loss: 0.0223 - val_loss: 0.1087 - val_f1_score_mod: 0.2084 - val_recall_mod: 0.1209 - val_precision_mod: 0.7628 - val_dur_error: 0.3066 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10868\n",
      "25307/25307 - 132s - loss: 0.1190 - f1_score_mod: 0.1988 - recall_mod: 0.1162 - precision_mod: 0.6971 - dur_error: 0.4447 - maestro_dur_loss: 0.0222 - val_loss: 0.1094 - val_f1_score_mod: 0.2360 - val_recall_mod: 0.1401 - val_precision_mod: 0.7539 - val_dur_error: 0.3367 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10868 to 0.10605, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1185 - f1_score_mod: 0.2026 - recall_mod: 0.1187 - precision_mod: 0.6983 - dur_error: 0.4432 - maestro_dur_loss: 0.0222 - val_loss: 0.1060 - val_f1_score_mod: 0.2363 - val_recall_mod: 0.1400 - val_precision_mod: 0.7634 - val_dur_error: 0.2743 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10605\n",
      "25307/25307 - 131s - loss: 0.1180 - f1_score_mod: 0.2062 - recall_mod: 0.1210 - precision_mod: 0.7049 - dur_error: 0.4400 - maestro_dur_loss: 0.0220 - val_loss: 0.1064 - val_f1_score_mod: 0.2313 - val_recall_mod: 0.1362 - val_precision_mod: 0.7797 - val_dur_error: 0.2818 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10605\n",
      "25307/25307 - 132s - loss: 0.1177 - f1_score_mod: 0.2094 - recall_mod: 0.1235 - precision_mod: 0.7016 - dur_error: 0.4395 - maestro_dur_loss: 0.0220 - val_loss: 0.1144 - val_f1_score_mod: 0.2480 - val_recall_mod: 0.1487 - val_precision_mod: 0.7546 - val_dur_error: 0.4459 - val_maestro_dur_loss: 0.0223\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10605\n",
      "25307/25307 - 132s - loss: 0.1169 - f1_score_mod: 0.2157 - recall_mod: 0.1277 - precision_mod: 0.7040 - dur_error: 0.4348 - maestro_dur_loss: 0.0217 - val_loss: 0.1067 - val_f1_score_mod: 0.2296 - val_recall_mod: 0.1349 - val_precision_mod: 0.7896 - val_dur_error: 0.2997 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10605\n",
      "25307/25307 - 132s - loss: 0.1166 - f1_score_mod: 0.2191 - recall_mod: 0.1300 - precision_mod: 0.7054 - dur_error: 0.4338 - maestro_dur_loss: 0.0217 - val_loss: 0.1081 - val_f1_score_mod: 0.2467 - val_recall_mod: 0.1477 - val_precision_mod: 0.7606 - val_dur_error: 0.3379 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10605\n",
      "25307/25307 - 132s - loss: 0.1162 - f1_score_mod: 0.2223 - recall_mod: 0.1322 - precision_mod: 0.7113 - dur_error: 0.4329 - maestro_dur_loss: 0.0216 - val_loss: 0.1094 - val_f1_score_mod: 0.2609 - val_recall_mod: 0.1582 - val_precision_mod: 0.7499 - val_dur_error: 0.3688 - val_maestro_dur_loss: 0.0184\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.10605 to 0.10427, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1156 - f1_score_mod: 0.2286 - recall_mod: 0.1365 - precision_mod: 0.7122 - dur_error: 0.4316 - maestro_dur_loss: 0.0216 - val_loss: 0.1043 - val_f1_score_mod: 0.2559 - val_recall_mod: 0.1538 - val_precision_mod: 0.7732 - val_dur_error: 0.2718 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10427\n",
      "25307/25307 - 132s - loss: 0.1150 - f1_score_mod: 0.2297 - recall_mod: 0.1373 - precision_mod: 0.7092 - dur_error: 0.4237 - maestro_dur_loss: 0.0212 - val_loss: 0.1059 - val_f1_score_mod: 0.2646 - val_recall_mod: 0.1612 - val_precision_mod: 0.7516 - val_dur_error: 0.3069 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10427\n",
      "25307/25307 - 132s - loss: 0.1148 - f1_score_mod: 0.2338 - recall_mod: 0.1401 - precision_mod: 0.7146 - dur_error: 0.4263 - maestro_dur_loss: 0.0213 - val_loss: 0.1132 - val_f1_score_mod: 0.2848 - val_recall_mod: 0.1782 - val_precision_mod: 0.7167 - val_dur_error: 0.4442 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10427\n",
      "25307/25307 - 132s - loss: 0.1145 - f1_score_mod: 0.2380 - recall_mod: 0.1431 - precision_mod: 0.7174 - dur_error: 0.4263 - maestro_dur_loss: 0.0213 - val_loss: 0.1046 - val_f1_score_mod: 0.2607 - val_recall_mod: 0.1568 - val_precision_mod: 0.7812 - val_dur_error: 0.2893 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.10427 to 0.10408, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1140 - f1_score_mod: 0.2423 - recall_mod: 0.1464 - precision_mod: 0.7106 - dur_error: 0.4237 - maestro_dur_loss: 0.0212 - val_loss: 0.1041 - val_f1_score_mod: 0.2579 - val_recall_mod: 0.1546 - val_precision_mod: 0.7848 - val_dur_error: 0.2835 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10408 to 0.10295, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1137 - f1_score_mod: 0.2439 - recall_mod: 0.1473 - precision_mod: 0.7151 - dur_error: 0.4219 - maestro_dur_loss: 0.0211 - val_loss: 0.1029 - val_f1_score_mod: 0.2716 - val_recall_mod: 0.1656 - val_precision_mod: 0.7626 - val_dur_error: 0.2755 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.10295\n",
      "25307/25307 - 132s - loss: 0.1131 - f1_score_mod: 0.2530 - recall_mod: 0.1540 - precision_mod: 0.7147 - dur_error: 0.4210 - maestro_dur_loss: 0.0210 - val_loss: 0.1073 - val_f1_score_mod: 0.2729 - val_recall_mod: 0.1655 - val_precision_mod: 0.7795 - val_dur_error: 0.3620 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10295\n",
      "25307/25307 - 132s - loss: 0.1126 - f1_score_mod: 0.2514 - recall_mod: 0.1529 - precision_mod: 0.7145 - dur_error: 0.4187 - maestro_dur_loss: 0.0209 - val_loss: 0.1033 - val_f1_score_mod: 0.2784 - val_recall_mod: 0.1701 - val_precision_mod: 0.7783 - val_dur_error: 0.2840 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10295\n",
      "25307/25307 - 132s - loss: 0.1123 - f1_score_mod: 0.2547 - recall_mod: 0.1552 - precision_mod: 0.7181 - dur_error: 0.4162 - maestro_dur_loss: 0.0208 - val_loss: 0.1052 - val_f1_score_mod: 0.2840 - val_recall_mod: 0.1754 - val_precision_mod: 0.7570 - val_dur_error: 0.3302 - val_maestro_dur_loss: 0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.10295\n",
      "25307/25307 - 132s - loss: 0.1119 - f1_score_mod: 0.2594 - recall_mod: 0.1583 - precision_mod: 0.7224 - dur_error: 0.4182 - maestro_dur_loss: 0.0209 - val_loss: 0.1053 - val_f1_score_mod: 0.2875 - val_recall_mod: 0.1779 - val_precision_mod: 0.7598 - val_dur_error: 0.3349 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.10295\n",
      "25307/25307 - 132s - loss: 0.1111 - f1_score_mod: 0.2630 - recall_mod: 0.1610 - precision_mod: 0.7227 - dur_error: 0.4085 - maestro_dur_loss: 0.0204 - val_loss: 0.1046 - val_f1_score_mod: 0.3074 - val_recall_mod: 0.1944 - val_precision_mod: 0.7404 - val_dur_error: 0.3239 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.10295\n",
      "25307/25307 - 133s - loss: 0.1111 - f1_score_mod: 0.2687 - recall_mod: 0.1654 - precision_mod: 0.7215 - dur_error: 0.4130 - maestro_dur_loss: 0.0206 - val_loss: 0.1055 - val_f1_score_mod: 0.3062 - val_recall_mod: 0.1927 - val_precision_mod: 0.7509 - val_dur_error: 0.3513 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.10295\n",
      "25307/25307 - 133s - loss: 0.1106 - f1_score_mod: 0.2682 - recall_mod: 0.1651 - precision_mod: 0.7196 - dur_error: 0.4114 - maestro_dur_loss: 0.0206 - val_loss: 0.1032 - val_f1_score_mod: 0.3007 - val_recall_mod: 0.1875 - val_precision_mod: 0.7632 - val_dur_error: 0.3102 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.10295 to 0.10212, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 134s - loss: 0.1101 - f1_score_mod: 0.2735 - recall_mod: 0.1689 - precision_mod: 0.7240 - dur_error: 0.4079 - maestro_dur_loss: 0.0204 - val_loss: 0.1021 - val_f1_score_mod: 0.3100 - val_recall_mod: 0.1969 - val_precision_mod: 0.7354 - val_dur_error: 0.2883 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.10212\n",
      "25307/25307 - 132s - loss: 0.1098 - f1_score_mod: 0.2805 - recall_mod: 0.1744 - precision_mod: 0.7215 - dur_error: 0.4089 - maestro_dur_loss: 0.0204 - val_loss: 0.1022 - val_f1_score_mod: 0.3088 - val_recall_mod: 0.1942 - val_precision_mod: 0.7629 - val_dur_error: 0.2989 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.10212\n",
      "25307/25307 - 132s - loss: 0.1093 - f1_score_mod: 0.2808 - recall_mod: 0.1748 - precision_mod: 0.7201 - dur_error: 0.4056 - maestro_dur_loss: 0.0203 - val_loss: 0.1034 - val_f1_score_mod: 0.3088 - val_recall_mod: 0.1936 - val_precision_mod: 0.7648 - val_dur_error: 0.3157 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.10212 to 0.10206, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1092 - f1_score_mod: 0.2835 - recall_mod: 0.1765 - precision_mod: 0.7258 - dur_error: 0.4051 - maestro_dur_loss: 0.0203 - val_loss: 0.1021 - val_f1_score_mod: 0.3149 - val_recall_mod: 0.1990 - val_precision_mod: 0.7578 - val_dur_error: 0.2983 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.10206 to 0.10112, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1085 - f1_score_mod: 0.2893 - recall_mod: 0.1810 - precision_mod: 0.7265 - dur_error: 0.4035 - maestro_dur_loss: 0.0202 - val_loss: 0.1011 - val_f1_score_mod: 0.3143 - val_recall_mod: 0.1981 - val_precision_mod: 0.7653 - val_dur_error: 0.2822 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.10112 to 0.09972, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1082 - f1_score_mod: 0.2954 - recall_mod: 0.1856 - precision_mod: 0.7300 - dur_error: 0.4025 - maestro_dur_loss: 0.0201 - val_loss: 0.0997 - val_f1_score_mod: 0.3270 - val_recall_mod: 0.2088 - val_precision_mod: 0.7587 - val_dur_error: 0.2687 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09972\n",
      "25307/25307 - 132s - loss: 0.1075 - f1_score_mod: 0.3015 - recall_mod: 0.1901 - precision_mod: 0.7332 - dur_error: 0.4004 - maestro_dur_loss: 0.0200 - val_loss: 0.1038 - val_f1_score_mod: 0.3263 - val_recall_mod: 0.2087 - val_precision_mod: 0.7534 - val_dur_error: 0.3498 - val_maestro_dur_loss: 0.0175\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09972\n",
      "25307/25307 - 132s - loss: 0.1075 - f1_score_mod: 0.3009 - recall_mod: 0.1900 - precision_mod: 0.7267 - dur_error: 0.4007 - maestro_dur_loss: 0.0200 - val_loss: 0.1026 - val_f1_score_mod: 0.3325 - val_recall_mod: 0.2138 - val_precision_mod: 0.7529 - val_dur_error: 0.3252 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09972 to 0.09942, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1067 - f1_score_mod: 0.3052 - recall_mod: 0.1935 - precision_mod: 0.7265 - dur_error: 0.3970 - maestro_dur_loss: 0.0199 - val_loss: 0.0994 - val_f1_score_mod: 0.3295 - val_recall_mod: 0.2108 - val_precision_mod: 0.7586 - val_dur_error: 0.2701 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09942\n",
      "25307/25307 - 135s - loss: 0.1062 - f1_score_mod: 0.3127 - recall_mod: 0.1993 - precision_mod: 0.7317 - dur_error: 0.3967 - maestro_dur_loss: 0.0198 - val_loss: 0.1009 - val_f1_score_mod: 0.3436 - val_recall_mod: 0.2235 - val_precision_mod: 0.7464 - val_dur_error: 0.3019 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09942\n",
      "25307/25307 - 133s - loss: 0.1058 - f1_score_mod: 0.3165 - recall_mod: 0.2024 - precision_mod: 0.7288 - dur_error: 0.3929 - maestro_dur_loss: 0.0196 - val_loss: 0.1002 - val_f1_score_mod: 0.3337 - val_recall_mod: 0.2137 - val_precision_mod: 0.7659 - val_dur_error: 0.2949 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.09942\n",
      "25307/25307 - 133s - loss: 0.1056 - f1_score_mod: 0.3160 - recall_mod: 0.2021 - precision_mod: 0.7301 - dur_error: 0.3937 - maestro_dur_loss: 0.0197 - val_loss: 0.1022 - val_f1_score_mod: 0.3436 - val_recall_mod: 0.2232 - val_precision_mod: 0.7494 - val_dur_error: 0.3350 - val_maestro_dur_loss: 0.0167\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09942 to 0.09867, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1051 - f1_score_mod: 0.3195 - recall_mod: 0.2049 - precision_mod: 0.7299 - dur_error: 0.3934 - maestro_dur_loss: 0.0197 - val_loss: 0.0987 - val_f1_score_mod: 0.3525 - val_recall_mod: 0.2306 - val_precision_mod: 0.7532 - val_dur_error: 0.2731 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09867 to 0.09858, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 134s - loss: 0.1047 - f1_score_mod: 0.3280 - recall_mod: 0.2114 - precision_mod: 0.7373 - dur_error: 0.3921 - maestro_dur_loss: 0.0196 - val_loss: 0.0986 - val_f1_score_mod: 0.3358 - val_recall_mod: 0.2156 - val_precision_mod: 0.7643 - val_dur_error: 0.2670 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09858\n",
      "25307/25307 - 133s - loss: 0.1041 - f1_score_mod: 0.3287 - recall_mod: 0.2120 - precision_mod: 0.7360 - dur_error: 0.3871 - maestro_dur_loss: 0.0194 - val_loss: 0.1010 - val_f1_score_mod: 0.3555 - val_recall_mod: 0.2335 - val_precision_mod: 0.7498 - val_dur_error: 0.3277 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09858 to 0.09788, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 134s - loss: 0.1039 - f1_score_mod: 0.3337 - recall_mod: 0.2161 - precision_mod: 0.7363 - dur_error: 0.3896 - maestro_dur_loss: 0.0195 - val_loss: 0.0979 - val_f1_score_mod: 0.3487 - val_recall_mod: 0.2266 - val_precision_mod: 0.7610 - val_dur_error: 0.2601 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09788 to 0.09740, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1034 - f1_score_mod: 0.3351 - recall_mod: 0.2177 - precision_mod: 0.7334 - dur_error: 0.3860 - maestro_dur_loss: 0.0193 - val_loss: 0.0974 - val_f1_score_mod: 0.3529 - val_recall_mod: 0.2299 - val_precision_mod: 0.7675 - val_dur_error: 0.2563 - val_maestro_dur_loss: 0.0128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09740 to 0.09686, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1032 - f1_score_mod: 0.3378 - recall_mod: 0.2198 - precision_mod: 0.7345 - dur_error: 0.3856 - maestro_dur_loss: 0.0193 - val_loss: 0.0969 - val_f1_score_mod: 0.3603 - val_recall_mod: 0.2367 - val_precision_mod: 0.7566 - val_dur_error: 0.2545 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.09686\n",
      "25307/25307 - 133s - loss: 0.1027 - f1_score_mod: 0.3408 - recall_mod: 0.2224 - precision_mod: 0.7330 - dur_error: 0.3864 - maestro_dur_loss: 0.0193 - val_loss: 0.0988 - val_f1_score_mod: 0.3641 - val_recall_mod: 0.2405 - val_precision_mod: 0.7519 - val_dur_error: 0.2925 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.09686\n",
      "25307/25307 - 132s - loss: 0.1023 - f1_score_mod: 0.3453 - recall_mod: 0.2261 - precision_mod: 0.7347 - dur_error: 0.3838 - maestro_dur_loss: 0.0192 - val_loss: 0.0997 - val_f1_score_mod: 0.3719 - val_recall_mod: 0.2473 - val_precision_mod: 0.7518 - val_dur_error: 0.3155 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09686\n",
      "25307/25307 - 133s - loss: 0.1015 - f1_score_mod: 0.3520 - recall_mod: 0.2318 - precision_mod: 0.7360 - dur_error: 0.3810 - maestro_dur_loss: 0.0190 - val_loss: 0.0972 - val_f1_score_mod: 0.3695 - val_recall_mod: 0.2443 - val_precision_mod: 0.7608 - val_dur_error: 0.2720 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09686\n",
      "25307/25307 - 133s - loss: 0.1014 - f1_score_mod: 0.3502 - recall_mod: 0.2304 - precision_mod: 0.7330 - dur_error: 0.3829 - maestro_dur_loss: 0.0191 - val_loss: 0.0982 - val_f1_score_mod: 0.3807 - val_recall_mod: 0.2563 - val_precision_mod: 0.7447 - val_dur_error: 0.2932 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09686\n",
      "25307/25307 - 134s - loss: 0.1009 - f1_score_mod: 0.3542 - recall_mod: 0.2344 - precision_mod: 0.7276 - dur_error: 0.3779 - maestro_dur_loss: 0.0189 - val_loss: 0.1021 - val_f1_score_mod: 0.3727 - val_recall_mod: 0.2480 - val_precision_mod: 0.7526 - val_dur_error: 0.3762 - val_maestro_dur_loss: 0.0188\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09686 to 0.09648, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1004 - f1_score_mod: 0.3588 - recall_mod: 0.2379 - precision_mod: 0.7344 - dur_error: 0.3808 - maestro_dur_loss: 0.0190 - val_loss: 0.0965 - val_f1_score_mod: 0.3751 - val_recall_mod: 0.2500 - val_precision_mod: 0.7538 - val_dur_error: 0.2644 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09648 to 0.09608, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1002 - f1_score_mod: 0.3626 - recall_mod: 0.2409 - precision_mod: 0.7364 - dur_error: 0.3761 - maestro_dur_loss: 0.0188 - val_loss: 0.0961 - val_f1_score_mod: 0.3746 - val_recall_mod: 0.2491 - val_precision_mod: 0.7577 - val_dur_error: 0.2630 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09608 to 0.09598, saving model to ../models/best_maestro_model_2_1_512_0pt5_lr_5e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.0993 - f1_score_mod: 0.3644 - recall_mod: 0.2424 - precision_mod: 0.7374 - dur_error: 0.3709 - maestro_dur_loss: 0.0185 - val_loss: 0.0960 - val_f1_score_mod: 0.3816 - val_recall_mod: 0.2589 - val_precision_mod: 0.7299 - val_dur_error: 0.2575 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09598\n",
      "25307/25307 - 133s - loss: 0.0991 - f1_score_mod: 0.3689 - recall_mod: 0.2463 - precision_mod: 0.7368 - dur_error: 0.3732 - maestro_dur_loss: 0.0187 - val_loss: 0.0964 - val_f1_score_mod: 0.3910 - val_recall_mod: 0.2660 - val_precision_mod: 0.7406 - val_dur_error: 0.2721 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 73/150\n",
      "Batch 46: Invalid loss, terminating training\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "24064/25307 - 109s - loss: nan - f1_score_mod: 0.3744 - recall_mod: 0.2513 - precision_mod: 0.7374 - dur_error: 0.3722 - maestro_dur_loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipvalue = 0.2, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training here does a better job of generalizing to the validation set (as expected) but causes an earlier failure (in terms of training loss). For a visual comparison of performance depending on the dropout_rate, see Figures 5 and 6 from visualize_performance.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best performing model is ../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5. See the final section in visualize_performance.ipynb for an in-depth visual analysis of the performance of this model. We can also load this model and generate some music, and this is the task of ../webapp, which also provides a user interface for customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25307 samples, validate on 10846 samples\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17353, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 162s - loss: 0.4134 - f1_score_mod: 0.0581 - recall_mod: 0.1424 - precision_mod: 0.0420 - dur_error: 1.2514 - maestro_dur_loss: 0.0626 - val_loss: 0.1735 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.7284 - val_maestro_dur_loss: 0.0364\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17353 to 0.15280, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 164s - loss: 0.2345 - f1_score_mod: 0.0437 - recall_mod: 0.0353 - precision_mod: 0.0620 - dur_error: 0.9727 - maestro_dur_loss: 0.0486 - val_loss: 0.1528 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5033 - val_maestro_dur_loss: 0.0252\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15280 to 0.14563, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 156s - loss: 0.1995 - f1_score_mod: 0.0243 - recall_mod: 0.0145 - precision_mod: 0.0818 - dur_error: 0.8553 - maestro_dur_loss: 0.0428 - val_loss: 0.1456 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4496 - val_maestro_dur_loss: 0.0225\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14563\n",
      "25307/25307 - 148s - loss: 0.1807 - f1_score_mod: 0.0141 - recall_mod: 0.0076 - precision_mod: 0.1119 - dur_error: 0.7726 - maestro_dur_loss: 0.0386 - val_loss: 0.1471 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5242 - val_maestro_dur_loss: 0.0262\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14563 to 0.14291, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 156s - loss: 0.1710 - f1_score_mod: 0.0090 - recall_mod: 0.0047 - precision_mod: 0.1490 - dur_error: 0.7127 - maestro_dur_loss: 0.0356 - val_loss: 0.1429 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4629 - val_maestro_dur_loss: 0.0231\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14291 to 0.14016, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 156s - loss: 0.1644 - f1_score_mod: 0.0047 - recall_mod: 0.0024 - precision_mod: 0.1593 - dur_error: 0.6684 - maestro_dur_loss: 0.0334 - val_loss: 0.1402 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4282 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.14016 to 0.13917, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 139s - loss: 0.1604 - f1_score_mod: 0.0030 - recall_mod: 0.0015 - precision_mod: 0.1697 - dur_error: 0.6434 - maestro_dur_loss: 0.0322 - val_loss: 0.1392 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4280 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13917\n",
      "25307/25307 - 132s - loss: 0.1564 - f1_score_mod: 0.0029 - recall_mod: 0.0015 - precision_mod: 0.2469 - dur_error: 0.6131 - maestro_dur_loss: 0.0307 - val_loss: 0.1392 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4480 - val_maestro_dur_loss: 0.0224\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.13917 to 0.13762, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1536 - f1_score_mod: 0.0039 - recall_mod: 0.0020 - precision_mod: 0.3177 - dur_error: 0.5988 - maestro_dur_loss: 0.0299 - val_loss: 0.1376 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4360 - val_maestro_dur_loss: 0.0218\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13762\n",
      "25307/25307 - 132s - loss: 0.1512 - f1_score_mod: 0.0069 - recall_mod: 0.0035 - precision_mod: 0.3976 - dur_error: 0.5871 - maestro_dur_loss: 0.0294 - val_loss: 0.1386 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4838 - val_maestro_dur_loss: 0.0242\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13762 to 0.13376, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1490 - f1_score_mod: 0.0117 - recall_mod: 0.0059 - precision_mod: 0.4551 - dur_error: 0.5756 - maestro_dur_loss: 0.0288 - val_loss: 0.1338 - val_f1_score_mod: 4.3522e-04 - val_recall_mod: 2.1780e-04 - val_precision_mod: 0.2727 - val_dur_error: 0.4136 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.13376 to 0.13227, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1469 - f1_score_mod: 0.0155 - recall_mod: 0.0079 - precision_mod: 0.4452 - dur_error: 0.5617 - maestro_dur_loss: 0.0281 - val_loss: 0.1323 - val_f1_score_mod: 0.0094 - val_recall_mod: 0.0047 - val_precision_mod: 0.7779 - val_dur_error: 0.4070 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.13227 to 0.13182, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1454 - f1_score_mod: 0.0207 - recall_mod: 0.0106 - precision_mod: 0.4695 - dur_error: 0.5557 - maestro_dur_loss: 0.0278 - val_loss: 0.1318 - val_f1_score_mod: 0.0125 - val_recall_mod: 0.0063 - val_precision_mod: 0.7677 - val_dur_error: 0.4018 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13182 to 0.13017, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 140s - loss: 0.1441 - f1_score_mod: 0.0268 - recall_mod: 0.0138 - precision_mod: 0.5084 - dur_error: 0.5508 - maestro_dur_loss: 0.0275 - val_loss: 0.1302 - val_f1_score_mod: 0.0209 - val_recall_mod: 0.0106 - val_precision_mod: 0.7688 - val_dur_error: 0.3948 - val_maestro_dur_loss: 0.0197\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13017\n",
      "25307/25307 - 148s - loss: 0.1428 - f1_score_mod: 0.0337 - recall_mod: 0.0174 - precision_mod: 0.5305 - dur_error: 0.5432 - maestro_dur_loss: 0.0272 - val_loss: 0.1303 - val_f1_score_mod: 0.0254 - val_recall_mod: 0.0129 - val_precision_mod: 0.7696 - val_dur_error: 0.4149 - val_maestro_dur_loss: 0.0207\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13017\n",
      "25307/25307 - 140s - loss: 0.1416 - f1_score_mod: 0.0378 - recall_mod: 0.0196 - precision_mod: 0.5479 - dur_error: 0.5387 - maestro_dur_loss: 0.0269 - val_loss: 0.1334 - val_f1_score_mod: 0.0398 - val_recall_mod: 0.0205 - val_precision_mod: 0.7244 - val_dur_error: 0.4920 - val_maestro_dur_loss: 0.0246\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.13017 to 0.12750, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 155s - loss: 0.1402 - f1_score_mod: 0.0448 - recall_mod: 0.0234 - precision_mod: 0.5774 - dur_error: 0.5294 - maestro_dur_loss: 0.0265 - val_loss: 0.1275 - val_f1_score_mod: 0.0369 - val_recall_mod: 0.0190 - val_precision_mod: 0.7300 - val_dur_error: 0.3828 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12750\n",
      "25307/25307 - 142s - loss: 0.1392 - f1_score_mod: 0.0472 - recall_mod: 0.0246 - precision_mod: 0.5753 - dur_error: 0.5214 - maestro_dur_loss: 0.0261 - val_loss: 0.1275 - val_f1_score_mod: 0.0481 - val_recall_mod: 0.0249 - val_precision_mod: 0.7251 - val_dur_error: 0.4035 - val_maestro_dur_loss: 0.0202\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.12750 to 0.12708, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 147s - loss: 0.1382 - f1_score_mod: 0.0512 - recall_mod: 0.0268 - precision_mod: 0.5875 - dur_error: 0.5156 - maestro_dur_loss: 0.0258 - val_loss: 0.1271 - val_f1_score_mod: 0.0470 - val_recall_mod: 0.0243 - val_precision_mod: 0.7853 - val_dur_error: 0.4019 - val_maestro_dur_loss: 0.0201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.12708 to 0.12526, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 143s - loss: 0.1372 - f1_score_mod: 0.0562 - recall_mod: 0.0296 - precision_mod: 0.6006 - dur_error: 0.5121 - maestro_dur_loss: 0.0256 - val_loss: 0.1253 - val_f1_score_mod: 0.0499 - val_recall_mod: 0.0258 - val_precision_mod: 0.7697 - val_dur_error: 0.3825 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.12526 to 0.12377, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1365 - f1_score_mod: 0.0607 - recall_mod: 0.0320 - precision_mod: 0.5923 - dur_error: 0.5069 - maestro_dur_loss: 0.0253 - val_loss: 0.1238 - val_f1_score_mod: 0.0658 - val_recall_mod: 0.0345 - val_precision_mod: 0.7413 - val_dur_error: 0.3621 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12377\n",
      "25307/25307 - 132s - loss: 0.1355 - f1_score_mod: 0.0627 - recall_mod: 0.0332 - precision_mod: 0.5938 - dur_error: 0.4995 - maestro_dur_loss: 0.0250 - val_loss: 0.1251 - val_f1_score_mod: 0.0743 - val_recall_mod: 0.0392 - val_precision_mod: 0.7477 - val_dur_error: 0.3943 - val_maestro_dur_loss: 0.0197\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.12377 to 0.12143, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 132s - loss: 0.1345 - f1_score_mod: 0.0660 - recall_mod: 0.0350 - precision_mod: 0.5943 - dur_error: 0.4920 - maestro_dur_loss: 0.0246 - val_loss: 0.1214 - val_f1_score_mod: 0.0638 - val_recall_mod: 0.0333 - val_precision_mod: 0.7842 - val_dur_error: 0.3307 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12143 to 0.12079, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1336 - f1_score_mod: 0.0689 - recall_mod: 0.0366 - precision_mod: 0.6043 - dur_error: 0.4842 - maestro_dur_loss: 0.0242 - val_loss: 0.1208 - val_f1_score_mod: 0.0756 - val_recall_mod: 0.0399 - val_precision_mod: 0.7426 - val_dur_error: 0.3258 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12079 to 0.12065, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 131s - loss: 0.1331 - f1_score_mod: 0.0741 - recall_mod: 0.0395 - precision_mod: 0.6146 - dur_error: 0.4802 - maestro_dur_loss: 0.0240 - val_loss: 0.1206 - val_f1_score_mod: 0.0750 - val_recall_mod: 0.0396 - val_precision_mod: 0.7877 - val_dur_error: 0.3328 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12065\n",
      "25307/25307 - 131s - loss: 0.1323 - f1_score_mod: 0.0787 - recall_mod: 0.0421 - precision_mod: 0.6275 - dur_error: 0.4760 - maestro_dur_loss: 0.0238 - val_loss: 0.1259 - val_f1_score_mod: 0.1018 - val_recall_mod: 0.0548 - val_precision_mod: 0.7392 - val_dur_error: 0.4438 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.12065 to 0.11895, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 133s - loss: 0.1318 - f1_score_mod: 0.0811 - recall_mod: 0.0434 - precision_mod: 0.6282 - dur_error: 0.4719 - maestro_dur_loss: 0.0236 - val_loss: 0.1189 - val_f1_score_mod: 0.0773 - val_recall_mod: 0.0408 - val_precision_mod: 0.7709 - val_dur_error: 0.3110 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11895\n",
      "25307/25307 - 146s - loss: 0.1309 - f1_score_mod: 0.0816 - recall_mod: 0.0438 - precision_mod: 0.6250 - dur_error: 0.4636 - maestro_dur_loss: 0.0232 - val_loss: 0.1262 - val_f1_score_mod: 0.1043 - val_recall_mod: 0.0562 - val_precision_mod: 0.7431 - val_dur_error: 0.4694 - val_maestro_dur_loss: 0.0235\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11895\n",
      "25307/25307 - 143s - loss: 0.1306 - f1_score_mod: 0.0902 - recall_mod: 0.0487 - precision_mod: 0.6226 - dur_error: 0.4669 - maestro_dur_loss: 0.0233 - val_loss: 0.1219 - val_f1_score_mod: 0.0995 - val_recall_mod: 0.0535 - val_precision_mod: 0.7560 - val_dur_error: 0.3933 - val_maestro_dur_loss: 0.0197\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.11895 to 0.11670, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 147s - loss: 0.1299 - f1_score_mod: 0.0900 - recall_mod: 0.0486 - precision_mod: 0.6246 - dur_error: 0.4592 - maestro_dur_loss: 0.0230 - val_loss: 0.1167 - val_f1_score_mod: 0.0985 - val_recall_mod: 0.0527 - val_precision_mod: 0.7811 - val_dur_error: 0.2903 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11670\n",
      "25307/25307 - 136s - loss: 0.1294 - f1_score_mod: 0.0940 - recall_mod: 0.0509 - precision_mod: 0.6355 - dur_error: 0.4530 - maestro_dur_loss: 0.0226 - val_loss: 0.1187 - val_f1_score_mod: 0.1076 - val_recall_mod: 0.0580 - val_precision_mod: 0.7689 - val_dur_error: 0.3404 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11670\n",
      "25307/25307 - 135s - loss: 0.1289 - f1_score_mod: 0.0992 - recall_mod: 0.0539 - precision_mod: 0.6391 - dur_error: 0.4520 - maestro_dur_loss: 0.0226 - val_loss: 0.1203 - val_f1_score_mod: 0.1111 - val_recall_mod: 0.0600 - val_precision_mod: 0.7790 - val_dur_error: 0.3791 - val_maestro_dur_loss: 0.0190\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11670\n",
      "25307/25307 - 135s - loss: 0.1284 - f1_score_mod: 0.0996 - recall_mod: 0.0542 - precision_mod: 0.6293 - dur_error: 0.4508 - maestro_dur_loss: 0.0225 - val_loss: 0.1222 - val_f1_score_mod: 0.1275 - val_recall_mod: 0.0698 - val_precision_mod: 0.7548 - val_dur_error: 0.4256 - val_maestro_dur_loss: 0.0213\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11670\n",
      "25307/25307 - 136s - loss: 0.1279 - f1_score_mod: 0.1042 - recall_mod: 0.0568 - precision_mod: 0.6479 - dur_error: 0.4493 - maestro_dur_loss: 0.0225 - val_loss: 0.1224 - val_f1_score_mod: 0.1328 - val_recall_mod: 0.0730 - val_precision_mod: 0.7438 - val_dur_error: 0.4308 - val_maestro_dur_loss: 0.0215\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.11670 to 0.11459, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1275 - f1_score_mod: 0.1092 - recall_mod: 0.0597 - precision_mod: 0.6520 - dur_error: 0.4467 - maestro_dur_loss: 0.0223 - val_loss: 0.1146 - val_f1_score_mod: 0.1226 - val_recall_mod: 0.0667 - val_precision_mod: 0.7721 - val_dur_error: 0.2828 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11459\n",
      "25307/25307 - 135s - loss: 0.1272 - f1_score_mod: 0.1108 - recall_mod: 0.0606 - precision_mod: 0.6616 - dur_error: 0.4454 - maestro_dur_loss: 0.0223 - val_loss: 0.1153 - val_f1_score_mod: 0.1314 - val_recall_mod: 0.0719 - val_precision_mod: 0.7630 - val_dur_error: 0.2972 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.11459 to 0.11459, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 135s - loss: 0.1267 - f1_score_mod: 0.1134 - recall_mod: 0.0623 - precision_mod: 0.6469 - dur_error: 0.4423 - maestro_dur_loss: 0.0221 - val_loss: 0.1146 - val_f1_score_mod: 0.1274 - val_recall_mod: 0.0694 - val_precision_mod: 0.7985 - val_dur_error: 0.2850 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11459\n",
      "25307/25307 - 139s - loss: 0.1265 - f1_score_mod: 0.1164 - recall_mod: 0.0639 - precision_mod: 0.6633 - dur_error: 0.4425 - maestro_dur_loss: 0.0221 - val_loss: 0.1167 - val_f1_score_mod: 0.1266 - val_recall_mod: 0.0691 - val_precision_mod: 0.7744 - val_dur_error: 0.3329 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.11459 to 0.11436, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 141s - loss: 0.1261 - f1_score_mod: 0.1162 - recall_mod: 0.0638 - precision_mod: 0.6594 - dur_error: 0.4380 - maestro_dur_loss: 0.0219 - val_loss: 0.1144 - val_f1_score_mod: 0.1435 - val_recall_mod: 0.0794 - val_precision_mod: 0.7631 - val_dur_error: 0.2950 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 40/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11436\n",
      "25307/25307 - 143s - loss: 0.1257 - f1_score_mod: 0.1244 - recall_mod: 0.0689 - precision_mod: 0.6582 - dur_error: 0.4390 - maestro_dur_loss: 0.0219 - val_loss: 0.1185 - val_f1_score_mod: 0.1647 - val_recall_mod: 0.0931 - val_precision_mod: 0.7284 - val_dur_error: 0.3786 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11436\n",
      "25307/25307 - 147s - loss: 0.1253 - f1_score_mod: 0.1245 - recall_mod: 0.0690 - precision_mod: 0.6566 - dur_error: 0.4345 - maestro_dur_loss: 0.0217 - val_loss: 0.1156 - val_f1_score_mod: 0.1643 - val_recall_mod: 0.0929 - val_precision_mod: 0.7380 - val_dur_error: 0.3301 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11436\n",
      "25307/25307 - 135s - loss: 0.1250 - f1_score_mod: 0.1286 - recall_mod: 0.0713 - precision_mod: 0.6706 - dur_error: 0.4348 - maestro_dur_loss: 0.0217 - val_loss: 0.1145 - val_f1_score_mod: 0.1462 - val_recall_mod: 0.0810 - val_precision_mod: 0.7663 - val_dur_error: 0.3148 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.11436 to 0.11254, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 134s - loss: 0.1246 - f1_score_mod: 0.1283 - recall_mod: 0.0711 - precision_mod: 0.6715 - dur_error: 0.4317 - maestro_dur_loss: 0.0216 - val_loss: 0.1125 - val_f1_score_mod: 0.1520 - val_recall_mod: 0.0848 - val_precision_mod: 0.7672 - val_dur_error: 0.2748 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.11254 to 0.11232, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 134s - loss: 0.1243 - f1_score_mod: 0.1307 - recall_mod: 0.0727 - precision_mod: 0.6600 - dur_error: 0.4295 - maestro_dur_loss: 0.0215 - val_loss: 0.1123 - val_f1_score_mod: 0.1533 - val_recall_mod: 0.0855 - val_precision_mod: 0.7556 - val_dur_error: 0.2781 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.11232 to 0.11217, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 143s - loss: 0.1240 - f1_score_mod: 0.1373 - recall_mod: 0.0766 - precision_mod: 0.6780 - dur_error: 0.4294 - maestro_dur_loss: 0.0215 - val_loss: 0.1122 - val_f1_score_mod: 0.1521 - val_recall_mod: 0.0845 - val_precision_mod: 0.7747 - val_dur_error: 0.2779 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11217\n",
      "25307/25307 - 225s - loss: 0.1237 - f1_score_mod: 0.1388 - recall_mod: 0.0777 - precision_mod: 0.6701 - dur_error: 0.4285 - maestro_dur_loss: 0.0214 - val_loss: 0.1142 - val_f1_score_mod: 0.1652 - val_recall_mod: 0.0930 - val_precision_mod: 0.7554 - val_dur_error: 0.3256 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11217\n",
      "25307/25307 - 213s - loss: 0.1235 - f1_score_mod: 0.1389 - recall_mod: 0.0777 - precision_mod: 0.6718 - dur_error: 0.4280 - maestro_dur_loss: 0.0214 - val_loss: 0.1136 - val_f1_score_mod: 0.1668 - val_recall_mod: 0.0942 - val_precision_mod: 0.7383 - val_dur_error: 0.3193 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11217\n",
      "25307/25307 - 160s - loss: 0.1228 - f1_score_mod: 0.1445 - recall_mod: 0.0812 - precision_mod: 0.6678 - dur_error: 0.4202 - maestro_dur_loss: 0.0210 - val_loss: 0.1127 - val_f1_score_mod: 0.1669 - val_recall_mod: 0.0940 - val_precision_mod: 0.7654 - val_dur_error: 0.3060 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11217\n",
      "25307/25307 - 145s - loss: 0.1229 - f1_score_mod: 0.1450 - recall_mod: 0.0815 - precision_mod: 0.6694 - dur_error: 0.4223 - maestro_dur_loss: 0.0211 - val_loss: 0.1122 - val_f1_score_mod: 0.1717 - val_recall_mod: 0.0973 - val_precision_mod: 0.7502 - val_dur_error: 0.3007 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11217 to 0.11153, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 143s - loss: 0.1225 - f1_score_mod: 0.1492 - recall_mod: 0.0841 - precision_mod: 0.6751 - dur_error: 0.4219 - maestro_dur_loss: 0.0211 - val_loss: 0.1115 - val_f1_score_mod: 0.1652 - val_recall_mod: 0.0931 - val_precision_mod: 0.7553 - val_dur_error: 0.2791 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11153\n",
      "25307/25307 - 144s - loss: 0.1223 - f1_score_mod: 0.1496 - recall_mod: 0.0843 - precision_mod: 0.6737 - dur_error: 0.4192 - maestro_dur_loss: 0.0210 - val_loss: 0.1134 - val_f1_score_mod: 0.1725 - val_recall_mod: 0.0976 - val_precision_mod: 0.7522 - val_dur_error: 0.3292 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.11153 to 0.11079, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 144s - loss: 0.1221 - f1_score_mod: 0.1531 - recall_mod: 0.0865 - precision_mod: 0.6776 - dur_error: 0.4176 - maestro_dur_loss: 0.0209 - val_loss: 0.1108 - val_f1_score_mod: 0.1751 - val_recall_mod: 0.0993 - val_precision_mod: 0.7619 - val_dur_error: 0.2777 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11079\n",
      "25307/25307 - 144s - loss: 0.1215 - f1_score_mod: 0.1532 - recall_mod: 0.0866 - precision_mod: 0.6765 - dur_error: 0.4159 - maestro_dur_loss: 0.0208 - val_loss: 0.1120 - val_f1_score_mod: 0.1895 - val_recall_mod: 0.1090 - val_precision_mod: 0.7372 - val_dur_error: 0.3046 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.11079 to 0.11054, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1213 - f1_score_mod: 0.1580 - recall_mod: 0.0896 - precision_mod: 0.6801 - dur_error: 0.4131 - maestro_dur_loss: 0.0207 - val_loss: 0.1105 - val_f1_score_mod: 0.1786 - val_recall_mod: 0.1012 - val_precision_mod: 0.7688 - val_dur_error: 0.2745 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11054\n",
      "25307/25307 - 146s - loss: 0.1212 - f1_score_mod: 0.1628 - recall_mod: 0.0928 - precision_mod: 0.6759 - dur_error: 0.4166 - maestro_dur_loss: 0.0208 - val_loss: 0.1111 - val_f1_score_mod: 0.1799 - val_recall_mod: 0.1023 - val_precision_mod: 0.7681 - val_dur_error: 0.2930 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11054\n",
      "25307/25307 - 145s - loss: 0.1211 - f1_score_mod: 0.1620 - recall_mod: 0.0921 - precision_mod: 0.6784 - dur_error: 0.4166 - maestro_dur_loss: 0.0208 - val_loss: 0.1116 - val_f1_score_mod: 0.1849 - val_recall_mod: 0.1055 - val_precision_mod: 0.7559 - val_dur_error: 0.3064 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11054\n",
      "25307/25307 - 145s - loss: 0.1207 - f1_score_mod: 0.1637 - recall_mod: 0.0932 - precision_mod: 0.6851 - dur_error: 0.4130 - maestro_dur_loss: 0.0206 - val_loss: 0.1131 - val_f1_score_mod: 0.2069 - val_recall_mod: 0.1212 - val_precision_mod: 0.7206 - val_dur_error: 0.3375 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11054\n",
      "25307/25307 - 145s - loss: 0.1203 - f1_score_mod: 0.1690 - recall_mod: 0.0966 - precision_mod: 0.6888 - dur_error: 0.4098 - maestro_dur_loss: 0.0205 - val_loss: 0.1114 - val_f1_score_mod: 0.1787 - val_recall_mod: 0.1015 - val_precision_mod: 0.7566 - val_dur_error: 0.3111 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11054\n",
      "25307/25307 - 145s - loss: 0.1202 - f1_score_mod: 0.1712 - recall_mod: 0.0979 - precision_mod: 0.6913 - dur_error: 0.4116 - maestro_dur_loss: 0.0206 - val_loss: 0.1133 - val_f1_score_mod: 0.2174 - val_recall_mod: 0.1289 - val_precision_mod: 0.7080 - val_dur_error: 0.3389 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.11054 to 0.10957, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1200 - f1_score_mod: 0.1724 - recall_mod: 0.0989 - precision_mod: 0.6826 - dur_error: 0.4109 - maestro_dur_loss: 0.0205 - val_loss: 0.1096 - val_f1_score_mod: 0.1798 - val_recall_mod: 0.1018 - val_precision_mod: 0.7775 - val_dur_error: 0.2715 - val_maestro_dur_loss: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.10957\n",
      "25307/25307 - 144s - loss: 0.1199 - f1_score_mod: 0.1723 - recall_mod: 0.0987 - precision_mod: 0.6892 - dur_error: 0.4085 - maestro_dur_loss: 0.0204 - val_loss: 0.1112 - val_f1_score_mod: 0.1931 - val_recall_mod: 0.1108 - val_precision_mod: 0.7651 - val_dur_error: 0.3136 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.10957\n",
      "25307/25307 - 144s - loss: 0.1195 - f1_score_mod: 0.1740 - recall_mod: 0.1000 - precision_mod: 0.6804 - dur_error: 0.4102 - maestro_dur_loss: 0.0205 - val_loss: 0.1158 - val_f1_score_mod: 0.2062 - val_recall_mod: 0.1201 - val_precision_mod: 0.7342 - val_dur_error: 0.4052 - val_maestro_dur_loss: 0.0203\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.10957\n",
      "25307/25307 - 144s - loss: 0.1194 - f1_score_mod: 0.1747 - recall_mod: 0.1004 - precision_mod: 0.6889 - dur_error: 0.4084 - maestro_dur_loss: 0.0204 - val_loss: 0.1114 - val_f1_score_mod: 0.1955 - val_recall_mod: 0.1123 - val_precision_mod: 0.7627 - val_dur_error: 0.3173 - val_maestro_dur_loss: 0.0159\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.10957\n",
      "25307/25307 - 144s - loss: 0.1194 - f1_score_mod: 0.1788 - recall_mod: 0.1029 - precision_mod: 0.6926 - dur_error: 0.4084 - maestro_dur_loss: 0.0204 - val_loss: 0.1118 - val_f1_score_mod: 0.2016 - val_recall_mod: 0.1166 - val_precision_mod: 0.7505 - val_dur_error: 0.3361 - val_maestro_dur_loss: 0.0168\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.10957\n",
      "25307/25307 - 144s - loss: 0.1189 - f1_score_mod: 0.1800 - recall_mod: 0.1037 - precision_mod: 0.6888 - dur_error: 0.4034 - maestro_dur_loss: 0.0202 - val_loss: 0.1122 - val_f1_score_mod: 0.2187 - val_recall_mod: 0.1290 - val_precision_mod: 0.7246 - val_dur_error: 0.3414 - val_maestro_dur_loss: 0.0171\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.10957 to 0.10863, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1189 - f1_score_mod: 0.1833 - recall_mod: 0.1057 - precision_mod: 0.6947 - dur_error: 0.4082 - maestro_dur_loss: 0.0204 - val_loss: 0.1086 - val_f1_score_mod: 0.2037 - val_recall_mod: 0.1179 - val_precision_mod: 0.7545 - val_dur_error: 0.2715 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 67/500\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.10863 to 0.10848, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1185 - f1_score_mod: 0.1839 - recall_mod: 0.1064 - precision_mod: 0.6897 - dur_error: 0.4028 - maestro_dur_loss: 0.0201 - val_loss: 0.1085 - val_f1_score_mod: 0.2121 - val_recall_mod: 0.1239 - val_precision_mod: 0.7421 - val_dur_error: 0.2701 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.10848\n",
      "25307/25307 - 145s - loss: 0.1187 - f1_score_mod: 0.1845 - recall_mod: 0.1065 - precision_mod: 0.6970 - dur_error: 0.4062 - maestro_dur_loss: 0.0203 - val_loss: 0.1090 - val_f1_score_mod: 0.2044 - val_recall_mod: 0.1186 - val_precision_mod: 0.7494 - val_dur_error: 0.2906 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.10848\n",
      "25307/25307 - 145s - loss: 0.1181 - f1_score_mod: 0.1859 - recall_mod: 0.1076 - precision_mod: 0.6923 - dur_error: 0.4002 - maestro_dur_loss: 0.0200 - val_loss: 0.1146 - val_f1_score_mod: 0.2182 - val_recall_mod: 0.1282 - val_precision_mod: 0.7418 - val_dur_error: 0.4017 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 70/500\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.10848 to 0.10822, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 146s - loss: 0.1180 - f1_score_mod: 0.1899 - recall_mod: 0.1103 - precision_mod: 0.6915 - dur_error: 0.4015 - maestro_dur_loss: 0.0201 - val_loss: 0.1082 - val_f1_score_mod: 0.2257 - val_recall_mod: 0.1337 - val_precision_mod: 0.7290 - val_dur_error: 0.2732 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.10822 to 0.10767, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1179 - f1_score_mod: 0.1928 - recall_mod: 0.1122 - precision_mod: 0.6929 - dur_error: 0.4026 - maestro_dur_loss: 0.0201 - val_loss: 0.1077 - val_f1_score_mod: 0.2197 - val_recall_mod: 0.1293 - val_precision_mod: 0.7410 - val_dur_error: 0.2683 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.10767\n",
      "25307/25307 - 146s - loss: 0.1175 - f1_score_mod: 0.1901 - recall_mod: 0.1106 - precision_mod: 0.6913 - dur_error: 0.3990 - maestro_dur_loss: 0.0200 - val_loss: 0.1085 - val_f1_score_mod: 0.2122 - val_recall_mod: 0.1236 - val_precision_mod: 0.7600 - val_dur_error: 0.2874 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.10767\n",
      "25307/25307 - 146s - loss: 0.1175 - f1_score_mod: 0.1916 - recall_mod: 0.1114 - precision_mod: 0.6961 - dur_error: 0.3985 - maestro_dur_loss: 0.0199 - val_loss: 0.1078 - val_f1_score_mod: 0.1987 - val_recall_mod: 0.1144 - val_precision_mod: 0.7710 - val_dur_error: 0.2734 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.10767\n",
      "25307/25307 - 145s - loss: 0.1174 - f1_score_mod: 0.1925 - recall_mod: 0.1119 - precision_mod: 0.6998 - dur_error: 0.3995 - maestro_dur_loss: 0.0200 - val_loss: 0.1100 - val_f1_score_mod: 0.2402 - val_recall_mod: 0.1441 - val_precision_mod: 0.7293 - val_dur_error: 0.3170 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.10767\n",
      "25307/25307 - 145s - loss: 0.1169 - f1_score_mod: 0.1983 - recall_mod: 0.1159 - precision_mod: 0.6960 - dur_error: 0.3967 - maestro_dur_loss: 0.0198 - val_loss: 0.1080 - val_f1_score_mod: 0.2328 - val_recall_mod: 0.1383 - val_precision_mod: 0.7422 - val_dur_error: 0.2816 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.10767 to 0.10681, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1168 - f1_score_mod: 0.1985 - recall_mod: 0.1159 - precision_mod: 0.6981 - dur_error: 0.3951 - maestro_dur_loss: 0.0198 - val_loss: 0.1068 - val_f1_score_mod: 0.2247 - val_recall_mod: 0.1323 - val_precision_mod: 0.7510 - val_dur_error: 0.2632 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.10681\n",
      "25307/25307 - 145s - loss: 0.1170 - f1_score_mod: 0.1996 - recall_mod: 0.1168 - precision_mod: 0.6960 - dur_error: 0.4011 - maestro_dur_loss: 0.0201 - val_loss: 0.1071 - val_f1_score_mod: 0.2334 - val_recall_mod: 0.1390 - val_precision_mod: 0.7457 - val_dur_error: 0.2687 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10681\n",
      "25307/25307 - 145s - loss: 0.1168 - f1_score_mod: 0.2010 - recall_mod: 0.1177 - precision_mod: 0.7014 - dur_error: 0.3970 - maestro_dur_loss: 0.0198 - val_loss: 0.1114 - val_f1_score_mod: 0.2311 - val_recall_mod: 0.1371 - val_precision_mod: 0.7429 - val_dur_error: 0.3592 - val_maestro_dur_loss: 0.0180\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.10681\n",
      "25307/25307 - 145s - loss: 0.1164 - f1_score_mod: 0.2039 - recall_mod: 0.1198 - precision_mod: 0.6968 - dur_error: 0.3936 - maestro_dur_loss: 0.0197 - val_loss: 0.1092 - val_f1_score_mod: 0.2466 - val_recall_mod: 0.1488 - val_precision_mod: 0.7297 - val_dur_error: 0.3136 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10681\n",
      "25307/25307 - 144s - loss: 0.1161 - f1_score_mod: 0.2070 - recall_mod: 0.1217 - precision_mod: 0.6985 - dur_error: 0.3918 - maestro_dur_loss: 0.0196 - val_loss: 0.1116 - val_f1_score_mod: 0.2375 - val_recall_mod: 0.1419 - val_precision_mod: 0.7346 - val_dur_error: 0.3677 - val_maestro_dur_loss: 0.0184\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.10681\n",
      "25307/25307 - 144s - loss: 0.1160 - f1_score_mod: 0.2061 - recall_mod: 0.1210 - precision_mod: 0.7019 - dur_error: 0.3942 - maestro_dur_loss: 0.0197 - val_loss: 0.1133 - val_f1_score_mod: 0.2434 - val_recall_mod: 0.1467 - val_precision_mod: 0.7270 - val_dur_error: 0.4011 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.10681 to 0.10639, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1160 - f1_score_mod: 0.2069 - recall_mod: 0.1216 - precision_mod: 0.7001 - dur_error: 0.3952 - maestro_dur_loss: 0.0198 - val_loss: 0.1064 - val_f1_score_mod: 0.2284 - val_recall_mod: 0.1351 - val_precision_mod: 0.7559 - val_dur_error: 0.2721 - val_maestro_dur_loss: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.10639 to 0.10609, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 144s - loss: 0.1158 - f1_score_mod: 0.2092 - recall_mod: 0.1232 - precision_mod: 0.7019 - dur_error: 0.3925 - maestro_dur_loss: 0.0196 - val_loss: 0.1061 - val_f1_score_mod: 0.2359 - val_recall_mod: 0.1407 - val_precision_mod: 0.7460 - val_dur_error: 0.2636 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10609\n",
      "25307/25307 - 144s - loss: 0.1155 - f1_score_mod: 0.2091 - recall_mod: 0.1231 - precision_mod: 0.7045 - dur_error: 0.3901 - maestro_dur_loss: 0.0195 - val_loss: 0.1086 - val_f1_score_mod: 0.2469 - val_recall_mod: 0.1489 - val_precision_mod: 0.7271 - val_dur_error: 0.3117 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.10609 to 0.10578, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 146s - loss: 0.1155 - f1_score_mod: 0.2115 - recall_mod: 0.1249 - precision_mod: 0.7015 - dur_error: 0.3917 - maestro_dur_loss: 0.0196 - val_loss: 0.1058 - val_f1_score_mod: 0.2264 - val_recall_mod: 0.1330 - val_precision_mod: 0.7671 - val_dur_error: 0.2628 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10578\n",
      "25307/25307 - 144s - loss: 0.1155 - f1_score_mod: 0.2101 - recall_mod: 0.1238 - precision_mod: 0.7024 - dur_error: 0.3922 - maestro_dur_loss: 0.0196 - val_loss: 0.1064 - val_f1_score_mod: 0.2393 - val_recall_mod: 0.1425 - val_precision_mod: 0.7541 - val_dur_error: 0.2783 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 87/500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.10578\n",
      "25307/25307 - 145s - loss: 0.1150 - f1_score_mod: 0.2135 - recall_mod: 0.1264 - precision_mod: 0.6976 - dur_error: 0.3883 - maestro_dur_loss: 0.0194 - val_loss: 0.1097 - val_f1_score_mod: 0.2521 - val_recall_mod: 0.1524 - val_precision_mod: 0.7393 - val_dur_error: 0.3461 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 88/500\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.10578\n",
      "25307/25307 - 144s - loss: 0.1149 - f1_score_mod: 0.2162 - recall_mod: 0.1281 - precision_mod: 0.7012 - dur_error: 0.3904 - maestro_dur_loss: 0.0195 - val_loss: 0.1060 - val_f1_score_mod: 0.2372 - val_recall_mod: 0.1409 - val_precision_mod: 0.7575 - val_dur_error: 0.2756 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 89/500\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.10578\n",
      "25307/25307 - 144s - loss: 0.1146 - f1_score_mod: 0.2165 - recall_mod: 0.1283 - precision_mod: 0.7015 - dur_error: 0.3856 - maestro_dur_loss: 0.0193 - val_loss: 0.1091 - val_f1_score_mod: 0.2464 - val_recall_mod: 0.1479 - val_precision_mod: 0.7439 - val_dur_error: 0.3386 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10578\n",
      "25307/25307 - 145s - loss: 0.1146 - f1_score_mod: 0.2218 - recall_mod: 0.1318 - precision_mod: 0.7083 - dur_error: 0.3887 - maestro_dur_loss: 0.0194 - val_loss: 0.1095 - val_f1_score_mod: 0.2506 - val_recall_mod: 0.1503 - val_precision_mod: 0.7581 - val_dur_error: 0.3520 - val_maestro_dur_loss: 0.0176\n",
      "Epoch 91/500\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.10578\n",
      "25307/25307 - 145s - loss: 0.1145 - f1_score_mod: 0.2229 - recall_mod: 0.1325 - precision_mod: 0.7084 - dur_error: 0.3893 - maestro_dur_loss: 0.0195 - val_loss: 0.1087 - val_f1_score_mod: 0.2378 - val_recall_mod: 0.1414 - val_precision_mod: 0.7616 - val_dur_error: 0.3325 - val_maestro_dur_loss: 0.0166\n",
      "Epoch 92/500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.10578\n",
      "25307/25307 - 144s - loss: 0.1144 - f1_score_mod: 0.2219 - recall_mod: 0.1318 - precision_mod: 0.7080 - dur_error: 0.3884 - maestro_dur_loss: 0.0194 - val_loss: 0.1069 - val_f1_score_mod: 0.2607 - val_recall_mod: 0.1584 - val_precision_mod: 0.7466 - val_dur_error: 0.2994 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.10578 to 0.10484, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 145s - loss: 0.1141 - f1_score_mod: 0.2252 - recall_mod: 0.1341 - precision_mod: 0.7111 - dur_error: 0.3840 - maestro_dur_loss: 0.0192 - val_loss: 0.1048 - val_f1_score_mod: 0.2472 - val_recall_mod: 0.1481 - val_precision_mod: 0.7540 - val_dur_error: 0.2623 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 94/500\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.10484\n",
      "25307/25307 - 145s - loss: 0.1140 - f1_score_mod: 0.2240 - recall_mod: 0.1333 - precision_mod: 0.7095 - dur_error: 0.3855 - maestro_dur_loss: 0.0193 - val_loss: 0.1053 - val_f1_score_mod: 0.2482 - val_recall_mod: 0.1482 - val_precision_mod: 0.7698 - val_dur_error: 0.2685 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 95/500\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.10484\n",
      "25307/25307 - 145s - loss: 0.1140 - f1_score_mod: 0.2284 - recall_mod: 0.1365 - precision_mod: 0.7086 - dur_error: 0.3873 - maestro_dur_loss: 0.0194 - val_loss: 0.1110 - val_f1_score_mod: 0.2601 - val_recall_mod: 0.1578 - val_precision_mod: 0.7448 - val_dur_error: 0.3816 - val_maestro_dur_loss: 0.0191\n",
      "Epoch 96/500\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.10484 to 0.10444, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 146s - loss: 0.1137 - f1_score_mod: 0.2305 - recall_mod: 0.1378 - precision_mod: 0.7120 - dur_error: 0.3859 - maestro_dur_loss: 0.0193 - val_loss: 0.1044 - val_f1_score_mod: 0.2521 - val_recall_mod: 0.1509 - val_precision_mod: 0.7728 - val_dur_error: 0.2575 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 97/500\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.10444\n",
      "25307/25307 - 145s - loss: 0.1136 - f1_score_mod: 0.2307 - recall_mod: 0.1381 - precision_mod: 0.7105 - dur_error: 0.3835 - maestro_dur_loss: 0.0192 - val_loss: 0.1066 - val_f1_score_mod: 0.2727 - val_recall_mod: 0.1681 - val_precision_mod: 0.7288 - val_dur_error: 0.3018 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 98/500\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.10444\n",
      "25307/25307 - 145s - loss: 0.1136 - f1_score_mod: 0.2320 - recall_mod: 0.1389 - precision_mod: 0.7096 - dur_error: 0.3849 - maestro_dur_loss: 0.0192 - val_loss: 0.1064 - val_f1_score_mod: 0.2483 - val_recall_mod: 0.1486 - val_precision_mod: 0.7704 - val_dur_error: 0.3005 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 99/500\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.10444\n",
      "25307/25307 - 145s - loss: 0.1132 - f1_score_mod: 0.2301 - recall_mod: 0.1374 - precision_mod: 0.7146 - dur_error: 0.3822 - maestro_dur_loss: 0.0191 - val_loss: 0.1057 - val_f1_score_mod: 0.2621 - val_recall_mod: 0.1587 - val_precision_mod: 0.7569 - val_dur_error: 0.2904 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 100/500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.10444\n",
      "25307/25307 - 146s - loss: 0.1134 - f1_score_mod: 0.2330 - recall_mod: 0.1396 - precision_mod: 0.7106 - dur_error: 0.3852 - maestro_dur_loss: 0.0193 - val_loss: 0.1057 - val_f1_score_mod: 0.2548 - val_recall_mod: 0.1536 - val_precision_mod: 0.7599 - val_dur_error: 0.2906 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 101/500\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.10444\n",
      "25307/25307 - 145s - loss: 0.1131 - f1_score_mod: 0.2352 - recall_mod: 0.1410 - precision_mod: 0.7160 - dur_error: 0.3853 - maestro_dur_loss: 0.0193 - val_loss: 0.1090 - val_f1_score_mod: 0.2687 - val_recall_mod: 0.1641 - val_precision_mod: 0.7468 - val_dur_error: 0.3567 - val_maestro_dur_loss: 0.0178\n",
      "Epoch 102/500\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.10444\n",
      "25307/25307 - 148s - loss: 0.1128 - f1_score_mod: 0.2351 - recall_mod: 0.1413 - precision_mod: 0.7084 - dur_error: 0.3788 - maestro_dur_loss: 0.0189 - val_loss: 0.1091 - val_f1_score_mod: 0.2614 - val_recall_mod: 0.1578 - val_precision_mod: 0.7661 - val_dur_error: 0.3632 - val_maestro_dur_loss: 0.0182\n",
      "Epoch 103/500\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.10444\n",
      "25307/25307 - 143s - loss: 0.1125 - f1_score_mod: 0.2386 - recall_mod: 0.1435 - precision_mod: 0.7143 - dur_error: 0.3779 - maestro_dur_loss: 0.0189 - val_loss: 0.1046 - val_f1_score_mod: 0.2649 - val_recall_mod: 0.1607 - val_precision_mod: 0.7575 - val_dur_error: 0.2722 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 104/500\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.10444 to 0.10370, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 144s - loss: 0.1127 - f1_score_mod: 0.2396 - recall_mod: 0.1445 - precision_mod: 0.7099 - dur_error: 0.3815 - maestro_dur_loss: 0.0191 - val_loss: 0.1037 - val_f1_score_mod: 0.2668 - val_recall_mod: 0.1622 - val_precision_mod: 0.7577 - val_dur_error: 0.2573 - val_maestro_dur_loss: 0.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.10370 to 0.10369, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 143s - loss: 0.1124 - f1_score_mod: 0.2425 - recall_mod: 0.1462 - precision_mod: 0.7171 - dur_error: 0.3798 - maestro_dur_loss: 0.0190 - val_loss: 0.1037 - val_f1_score_mod: 0.2738 - val_recall_mod: 0.1691 - val_precision_mod: 0.7267 - val_dur_error: 0.2589 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 106/500\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.10369\n",
      "25307/25307 - 144s - loss: 0.1124 - f1_score_mod: 0.2414 - recall_mod: 0.1457 - precision_mod: 0.7107 - dur_error: 0.3796 - maestro_dur_loss: 0.0190 - val_loss: 0.1107 - val_f1_score_mod: 0.2800 - val_recall_mod: 0.1730 - val_precision_mod: 0.7424 - val_dur_error: 0.3969 - val_maestro_dur_loss: 0.0198\n",
      "Epoch 107/500\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.10369\n",
      "25307/25307 - 143s - loss: 0.1123 - f1_score_mod: 0.2428 - recall_mod: 0.1467 - precision_mod: 0.7117 - dur_error: 0.3797 - maestro_dur_loss: 0.0190 - val_loss: 0.1076 - val_f1_score_mod: 0.2654 - val_recall_mod: 0.1604 - val_precision_mod: 0.7781 - val_dur_error: 0.3403 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 108/500\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.10369\n",
      "25307/25307 - 144s - loss: 0.1119 - f1_score_mod: 0.2437 - recall_mod: 0.1470 - precision_mod: 0.7208 - dur_error: 0.3772 - maestro_dur_loss: 0.0189 - val_loss: 0.1073 - val_f1_score_mod: 0.2889 - val_recall_mod: 0.1806 - val_precision_mod: 0.7246 - val_dur_error: 0.3269 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 109/500\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.10369\n",
      "25307/25307 - 143s - loss: 0.1120 - f1_score_mod: 0.2446 - recall_mod: 0.1479 - precision_mod: 0.7161 - dur_error: 0.3788 - maestro_dur_loss: 0.0189 - val_loss: 0.1041 - val_f1_score_mod: 0.2789 - val_recall_mod: 0.1718 - val_precision_mod: 0.7481 - val_dur_error: 0.2723 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 110/500\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.10369\n",
      "25307/25307 - 136s - loss: 0.1119 - f1_score_mod: 0.2445 - recall_mod: 0.1480 - precision_mod: 0.7109 - dur_error: 0.3796 - maestro_dur_loss: 0.0190 - val_loss: 0.1063 - val_f1_score_mod: 0.2726 - val_recall_mod: 0.1669 - val_precision_mod: 0.7537 - val_dur_error: 0.3195 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 111/500\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.10369\n",
      "25307/25307 - 136s - loss: 0.1118 - f1_score_mod: 0.2468 - recall_mod: 0.1495 - precision_mod: 0.7173 - dur_error: 0.3809 - maestro_dur_loss: 0.0190 - val_loss: 0.1039 - val_f1_score_mod: 0.2788 - val_recall_mod: 0.1710 - val_precision_mod: 0.7608 - val_dur_error: 0.2746 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 112/500\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.10369 to 0.10367, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1118 - f1_score_mod: 0.2482 - recall_mod: 0.1505 - precision_mod: 0.7170 - dur_error: 0.3807 - maestro_dur_loss: 0.0190 - val_loss: 0.1037 - val_f1_score_mod: 0.2877 - val_recall_mod: 0.1785 - val_precision_mod: 0.7559 - val_dur_error: 0.2724 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 113/500\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.10367\n",
      "25307/25307 - 136s - loss: 0.1114 - f1_score_mod: 0.2504 - recall_mod: 0.1523 - precision_mod: 0.7151 - dur_error: 0.3770 - maestro_dur_loss: 0.0188 - val_loss: 0.1046 - val_f1_score_mod: 0.2859 - val_recall_mod: 0.1770 - val_precision_mod: 0.7471 - val_dur_error: 0.2896 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 114/500\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.10367\n",
      "25307/25307 - 136s - loss: 0.1112 - f1_score_mod: 0.2518 - recall_mod: 0.1531 - precision_mod: 0.7211 - dur_error: 0.3762 - maestro_dur_loss: 0.0188 - val_loss: 0.1054 - val_f1_score_mod: 0.2793 - val_recall_mod: 0.1715 - val_precision_mod: 0.7585 - val_dur_error: 0.3066 - val_maestro_dur_loss: 0.0153\n",
      "Epoch 115/500\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.10367\n",
      "25307/25307 - 136s - loss: 0.1113 - f1_score_mod: 0.2557 - recall_mod: 0.1557 - precision_mod: 0.7222 - dur_error: 0.3779 - maestro_dur_loss: 0.0189 - val_loss: 0.1038 - val_f1_score_mod: 0.2749 - val_recall_mod: 0.1682 - val_precision_mod: 0.7616 - val_dur_error: 0.2757 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 116/500\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.10367 to 0.10251, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1112 - f1_score_mod: 0.2512 - recall_mod: 0.1527 - precision_mod: 0.7161 - dur_error: 0.3766 - maestro_dur_loss: 0.0188 - val_loss: 0.1025 - val_f1_score_mod: 0.2758 - val_recall_mod: 0.1685 - val_precision_mod: 0.7686 - val_dur_error: 0.2551 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 117/500\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.10251\n",
      "25307/25307 - 136s - loss: 0.1110 - f1_score_mod: 0.2541 - recall_mod: 0.1546 - precision_mod: 0.7200 - dur_error: 0.3776 - maestro_dur_loss: 0.0189 - val_loss: 0.1034 - val_f1_score_mod: 0.2849 - val_recall_mod: 0.1755 - val_precision_mod: 0.7630 - val_dur_error: 0.2755 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 118/500\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.10251\n",
      "25307/25307 - 137s - loss: 0.1110 - f1_score_mod: 0.2560 - recall_mod: 0.1559 - precision_mod: 0.7219 - dur_error: 0.3773 - maestro_dur_loss: 0.0189 - val_loss: 0.1062 - val_f1_score_mod: 0.2793 - val_recall_mod: 0.1706 - val_precision_mod: 0.7774 - val_dur_error: 0.3302 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 119/500\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.10251\n",
      "25307/25307 - 136s - loss: 0.1106 - f1_score_mod: 0.2564 - recall_mod: 0.1561 - precision_mod: 0.7235 - dur_error: 0.3747 - maestro_dur_loss: 0.0187 - val_loss: 0.1041 - val_f1_score_mod: 0.2827 - val_recall_mod: 0.1737 - val_precision_mod: 0.7687 - val_dur_error: 0.2929 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 120/500\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.10251\n",
      "25307/25307 - 137s - loss: 0.1106 - f1_score_mod: 0.2587 - recall_mod: 0.1579 - precision_mod: 0.7217 - dur_error: 0.3741 - maestro_dur_loss: 0.0187 - val_loss: 0.1033 - val_f1_score_mod: 0.2989 - val_recall_mod: 0.1878 - val_precision_mod: 0.7373 - val_dur_error: 0.2735 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 121/500\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.10251\n",
      "25307/25307 - 136s - loss: 0.1104 - f1_score_mod: 0.2591 - recall_mod: 0.1586 - precision_mod: 0.7185 - dur_error: 0.3747 - maestro_dur_loss: 0.0187 - val_loss: 0.1042 - val_f1_score_mod: 0.2932 - val_recall_mod: 0.1832 - val_precision_mod: 0.7397 - val_dur_error: 0.2908 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 122/500\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.10251\n",
      "25307/25307 - 137s - loss: 0.1102 - f1_score_mod: 0.2618 - recall_mod: 0.1603 - precision_mod: 0.7226 - dur_error: 0.3747 - maestro_dur_loss: 0.0187 - val_loss: 0.1032 - val_f1_score_mod: 0.2797 - val_recall_mod: 0.1711 - val_precision_mod: 0.7690 - val_dur_error: 0.2732 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 123/500\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.10251\n",
      "25307/25307 - 136s - loss: 0.1103 - f1_score_mod: 0.2617 - recall_mod: 0.1600 - precision_mod: 0.7234 - dur_error: 0.3742 - maestro_dur_loss: 0.0187 - val_loss: 0.1058 - val_f1_score_mod: 0.2938 - val_recall_mod: 0.1831 - val_precision_mod: 0.7477 - val_dur_error: 0.3267 - val_maestro_dur_loss: 0.0163\n",
      "Epoch 124/500\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.10251\n",
      "25307/25307 - 137s - loss: 0.1100 - f1_score_mod: 0.2637 - recall_mod: 0.1618 - precision_mod: 0.7201 - dur_error: 0.3720 - maestro_dur_loss: 0.0186 - val_loss: 0.1035 - val_f1_score_mod: 0.2860 - val_recall_mod: 0.1761 - val_precision_mod: 0.7647 - val_dur_error: 0.2826 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 125/500\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.10251\n",
      "25307/25307 - 136s - loss: 0.1100 - f1_score_mod: 0.2659 - recall_mod: 0.1633 - precision_mod: 0.7230 - dur_error: 0.3750 - maestro_dur_loss: 0.0188 - val_loss: 0.1035 - val_f1_score_mod: 0.2883 - val_recall_mod: 0.1780 - val_precision_mod: 0.7637 - val_dur_error: 0.2858 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 126/500\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.10251 to 0.10169, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 137s - loss: 0.1097 - f1_score_mod: 0.2655 - recall_mod: 0.1627 - precision_mod: 0.7268 - dur_error: 0.3725 - maestro_dur_loss: 0.0186 - val_loss: 0.1017 - val_f1_score_mod: 0.2918 - val_recall_mod: 0.1803 - val_precision_mod: 0.7678 - val_dur_error: 0.2520 - val_maestro_dur_loss: 0.0126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.10169\n",
      "25307/25307 - 136s - loss: 0.1095 - f1_score_mod: 0.2676 - recall_mod: 0.1646 - precision_mod: 0.7216 - dur_error: 0.3697 - maestro_dur_loss: 0.0185 - val_loss: 0.1019 - val_f1_score_mod: 0.2894 - val_recall_mod: 0.1794 - val_precision_mod: 0.7542 - val_dur_error: 0.2556 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 128/500\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.10169 to 0.10160, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1097 - f1_score_mod: 0.2690 - recall_mod: 0.1653 - precision_mod: 0.7281 - dur_error: 0.3728 - maestro_dur_loss: 0.0186 - val_loss: 0.1016 - val_f1_score_mod: 0.2965 - val_recall_mod: 0.1862 - val_precision_mod: 0.7378 - val_dur_error: 0.2530 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 129/500\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.10160\n",
      "25307/25307 - 136s - loss: 0.1093 - f1_score_mod: 0.2726 - recall_mod: 0.1681 - precision_mod: 0.7255 - dur_error: 0.3714 - maestro_dur_loss: 0.0186 - val_loss: 0.1022 - val_f1_score_mod: 0.2917 - val_recall_mod: 0.1804 - val_precision_mod: 0.7680 - val_dur_error: 0.2649 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 130/500\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.10160\n",
      "25307/25307 - 138s - loss: 0.1092 - f1_score_mod: 0.2699 - recall_mod: 0.1662 - precision_mod: 0.7231 - dur_error: 0.3706 - maestro_dur_loss: 0.0185 - val_loss: 0.1025 - val_f1_score_mod: 0.2981 - val_recall_mod: 0.1857 - val_precision_mod: 0.7568 - val_dur_error: 0.2696 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 131/500\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.10160\n",
      "25307/25307 - 136s - loss: 0.1090 - f1_score_mod: 0.2718 - recall_mod: 0.1675 - precision_mod: 0.7268 - dur_error: 0.3690 - maestro_dur_loss: 0.0185 - val_loss: 0.1022 - val_f1_score_mod: 0.3039 - val_recall_mod: 0.1920 - val_precision_mod: 0.7379 - val_dur_error: 0.2657 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 132/500\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.10160\n",
      "25307/25307 - 137s - loss: 0.1089 - f1_score_mod: 0.2739 - recall_mod: 0.1692 - precision_mod: 0.7273 - dur_error: 0.3676 - maestro_dur_loss: 0.0184 - val_loss: 0.1019 - val_f1_score_mod: 0.3105 - val_recall_mod: 0.1963 - val_precision_mod: 0.7475 - val_dur_error: 0.2659 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 133/500\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.10160\n",
      "25307/25307 - 137s - loss: 0.1091 - f1_score_mod: 0.2734 - recall_mod: 0.1690 - precision_mod: 0.7230 - dur_error: 0.3706 - maestro_dur_loss: 0.0185 - val_loss: 0.1022 - val_f1_score_mod: 0.3019 - val_recall_mod: 0.1904 - val_precision_mod: 0.7375 - val_dur_error: 0.2698 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 134/500\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.10160\n",
      "25307/25307 - 137s - loss: 0.1087 - f1_score_mod: 0.2746 - recall_mod: 0.1696 - precision_mod: 0.7275 - dur_error: 0.3669 - maestro_dur_loss: 0.0183 - val_loss: 0.1033 - val_f1_score_mod: 0.3045 - val_recall_mod: 0.1928 - val_precision_mod: 0.7341 - val_dur_error: 0.2941 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 135/500\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.10160 to 0.10105, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 137s - loss: 0.1085 - f1_score_mod: 0.2759 - recall_mod: 0.1705 - precision_mod: 0.7302 - dur_error: 0.3673 - maestro_dur_loss: 0.0184 - val_loss: 0.1010 - val_f1_score_mod: 0.3147 - val_recall_mod: 0.2004 - val_precision_mod: 0.7354 - val_dur_error: 0.2517 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 136/500\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.10105\n",
      "25307/25307 - 137s - loss: 0.1089 - f1_score_mod: 0.2766 - recall_mod: 0.1713 - precision_mod: 0.7247 - dur_error: 0.3717 - maestro_dur_loss: 0.0186 - val_loss: 0.1044 - val_f1_score_mod: 0.2973 - val_recall_mod: 0.1848 - val_precision_mod: 0.7679 - val_dur_error: 0.3212 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 137/500\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.10105\n",
      "25307/25307 - 136s - loss: 0.1085 - f1_score_mod: 0.2786 - recall_mod: 0.1723 - precision_mod: 0.7321 - dur_error: 0.3684 - maestro_dur_loss: 0.0184 - val_loss: 0.1014 - val_f1_score_mod: 0.3071 - val_recall_mod: 0.1937 - val_precision_mod: 0.7460 - val_dur_error: 0.2571 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 138/500\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.10105\n",
      "25307/25307 - 136s - loss: 0.1084 - f1_score_mod: 0.2812 - recall_mod: 0.1745 - precision_mod: 0.7290 - dur_error: 0.3690 - maestro_dur_loss: 0.0184 - val_loss: 0.1032 - val_f1_score_mod: 0.3072 - val_recall_mod: 0.1928 - val_precision_mod: 0.7608 - val_dur_error: 0.2969 - val_maestro_dur_loss: 0.0148\n",
      "Epoch 139/500\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.10105 to 0.10093, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1083 - f1_score_mod: 0.2814 - recall_mod: 0.1746 - precision_mod: 0.7308 - dur_error: 0.3682 - maestro_dur_loss: 0.0184 - val_loss: 0.1009 - val_f1_score_mod: 0.2969 - val_recall_mod: 0.1850 - val_precision_mod: 0.7576 - val_dur_error: 0.2535 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 140/500\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.10093 to 0.10071, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 135s - loss: 0.1080 - f1_score_mod: 0.2808 - recall_mod: 0.1740 - precision_mod: 0.7331 - dur_error: 0.3668 - maestro_dur_loss: 0.0183 - val_loss: 0.1007 - val_f1_score_mod: 0.2997 - val_recall_mod: 0.1870 - val_precision_mod: 0.7661 - val_dur_error: 0.2500 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 141/500\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.10071\n",
      "25307/25307 - 136s - loss: 0.1079 - f1_score_mod: 0.2813 - recall_mod: 0.1743 - precision_mod: 0.7340 - dur_error: 0.3656 - maestro_dur_loss: 0.0183 - val_loss: 0.1016 - val_f1_score_mod: 0.3017 - val_recall_mod: 0.1892 - val_precision_mod: 0.7493 - val_dur_error: 0.2701 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 142/500\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.10071\n",
      "25307/25307 - 137s - loss: 0.1081 - f1_score_mod: 0.2814 - recall_mod: 0.1748 - precision_mod: 0.7264 - dur_error: 0.3690 - maestro_dur_loss: 0.0184 - val_loss: 0.1014 - val_f1_score_mod: 0.3053 - val_recall_mod: 0.1908 - val_precision_mod: 0.7663 - val_dur_error: 0.2621 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 143/500\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.10071\n",
      "25307/25307 - 136s - loss: 0.1077 - f1_score_mod: 0.2849 - recall_mod: 0.1773 - precision_mod: 0.7309 - dur_error: 0.3653 - maestro_dur_loss: 0.0183 - val_loss: 0.1043 - val_f1_score_mod: 0.3087 - val_recall_mod: 0.1939 - val_precision_mod: 0.7622 - val_dur_error: 0.3283 - val_maestro_dur_loss: 0.0164\n",
      "Epoch 144/500\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.10071\n",
      "25307/25307 - 137s - loss: 0.1077 - f1_score_mod: 0.2867 - recall_mod: 0.1789 - precision_mod: 0.7273 - dur_error: 0.3656 - maestro_dur_loss: 0.0183 - val_loss: 0.1008 - val_f1_score_mod: 0.3215 - val_recall_mod: 0.2052 - val_precision_mod: 0.7457 - val_dur_error: 0.2554 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 145/500\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.10071\n",
      "25307/25307 - 136s - loss: 0.1075 - f1_score_mod: 0.2841 - recall_mod: 0.1766 - precision_mod: 0.7332 - dur_error: 0.3646 - maestro_dur_loss: 0.0182 - val_loss: 0.1033 - val_f1_score_mod: 0.3157 - val_recall_mod: 0.2007 - val_precision_mod: 0.7459 - val_dur_error: 0.3119 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 146/500\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.10071\n",
      "25307/25307 - 136s - loss: 0.1073 - f1_score_mod: 0.2884 - recall_mod: 0.1799 - precision_mod: 0.7323 - dur_error: 0.3667 - maestro_dur_loss: 0.0183 - val_loss: 0.1007 - val_f1_score_mod: 0.3200 - val_recall_mod: 0.2040 - val_precision_mod: 0.7468 - val_dur_error: 0.2578 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 147/500\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.10071\n",
      "25307/25307 - 136s - loss: 0.1071 - f1_score_mod: 0.2932 - recall_mod: 0.1834 - precision_mod: 0.7368 - dur_error: 0.3641 - maestro_dur_loss: 0.0182 - val_loss: 0.1021 - val_f1_score_mod: 0.3081 - val_recall_mod: 0.1946 - val_precision_mod: 0.7541 - val_dur_error: 0.2879 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 148/500\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.10071\n",
      "25307/25307 - 136s - loss: 0.1071 - f1_score_mod: 0.2910 - recall_mod: 0.1817 - precision_mod: 0.7365 - dur_error: 0.3622 - maestro_dur_loss: 0.0181 - val_loss: 0.1020 - val_f1_score_mod: 0.3151 - val_recall_mod: 0.2001 - val_precision_mod: 0.7453 - val_dur_error: 0.2839 - val_maestro_dur_loss: 0.0142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/500\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.10071\n",
      "25307/25307 - 136s - loss: 0.1070 - f1_score_mod: 0.2914 - recall_mod: 0.1824 - precision_mod: 0.7301 - dur_error: 0.3632 - maestro_dur_loss: 0.0182 - val_loss: 0.1036 - val_f1_score_mod: 0.3106 - val_recall_mod: 0.1954 - val_precision_mod: 0.7638 - val_dur_error: 0.3239 - val_maestro_dur_loss: 0.0162\n",
      "Epoch 150/500\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.10071 to 0.10034, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1069 - f1_score_mod: 0.2925 - recall_mod: 0.1830 - precision_mod: 0.7336 - dur_error: 0.3651 - maestro_dur_loss: 0.0183 - val_loss: 0.1003 - val_f1_score_mod: 0.3055 - val_recall_mod: 0.1908 - val_precision_mod: 0.7706 - val_dur_error: 0.2568 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 151/500\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.10034\n",
      "25307/25307 - 136s - loss: 0.1068 - f1_score_mod: 0.2934 - recall_mod: 0.1840 - precision_mod: 0.7301 - dur_error: 0.3632 - maestro_dur_loss: 0.0182 - val_loss: 0.1019 - val_f1_score_mod: 0.3244 - val_recall_mod: 0.2077 - val_precision_mod: 0.7479 - val_dur_error: 0.2904 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 152/500\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.10034 to 0.10008, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 135s - loss: 0.1068 - f1_score_mod: 0.2930 - recall_mod: 0.1837 - precision_mod: 0.7307 - dur_error: 0.3636 - maestro_dur_loss: 0.0182 - val_loss: 0.1001 - val_f1_score_mod: 0.3214 - val_recall_mod: 0.2051 - val_precision_mod: 0.7466 - val_dur_error: 0.2558 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 153/500\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.10008\n",
      "25307/25307 - 135s - loss: 0.1066 - f1_score_mod: 0.2969 - recall_mod: 0.1862 - precision_mod: 0.7384 - dur_error: 0.3642 - maestro_dur_loss: 0.0182 - val_loss: 0.1043 - val_f1_score_mod: 0.3171 - val_recall_mod: 0.2008 - val_precision_mod: 0.7578 - val_dur_error: 0.3372 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 154/500\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.10008\n",
      "25307/25307 - 135s - loss: 0.1065 - f1_score_mod: 0.2982 - recall_mod: 0.1872 - precision_mod: 0.7366 - dur_error: 0.3636 - maestro_dur_loss: 0.0182 - val_loss: 0.1003 - val_f1_score_mod: 0.3254 - val_recall_mod: 0.2091 - val_precision_mod: 0.7396 - val_dur_error: 0.2603 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 155/500\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.10008 to 0.09974, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1064 - f1_score_mod: 0.2981 - recall_mod: 0.1874 - precision_mod: 0.7351 - dur_error: 0.3632 - maestro_dur_loss: 0.0182 - val_loss: 0.0997 - val_f1_score_mod: 0.3214 - val_recall_mod: 0.2045 - val_precision_mod: 0.7577 - val_dur_error: 0.2489 - val_maestro_dur_loss: 0.0124\n",
      "Epoch 156/500\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.09974\n",
      "25307/25307 - 136s - loss: 0.1063 - f1_score_mod: 0.2975 - recall_mod: 0.1873 - precision_mod: 0.7311 - dur_error: 0.3630 - maestro_dur_loss: 0.0181 - val_loss: 0.1003 - val_f1_score_mod: 0.3167 - val_recall_mod: 0.2004 - val_precision_mod: 0.7605 - val_dur_error: 0.2625 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 157/500\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.09974 to 0.09972, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 136s - loss: 0.1061 - f1_score_mod: 0.3009 - recall_mod: 0.1894 - precision_mod: 0.7362 - dur_error: 0.3613 - maestro_dur_loss: 0.0181 - val_loss: 0.0997 - val_f1_score_mod: 0.3164 - val_recall_mod: 0.2002 - val_precision_mod: 0.7585 - val_dur_error: 0.2529 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 158/500\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.09972\n",
      "25307/25307 - 135s - loss: 0.1060 - f1_score_mod: 0.3008 - recall_mod: 0.1895 - precision_mod: 0.7340 - dur_error: 0.3625 - maestro_dur_loss: 0.0181 - val_loss: 0.1005 - val_f1_score_mod: 0.3199 - val_recall_mod: 0.2024 - val_precision_mod: 0.7684 - val_dur_error: 0.2714 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 159/500\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.09972\n",
      "25307/25307 - 136s - loss: 0.1060 - f1_score_mod: 0.2994 - recall_mod: 0.1884 - precision_mod: 0.7349 - dur_error: 0.3640 - maestro_dur_loss: 0.0182 - val_loss: 0.1003 - val_f1_score_mod: 0.3295 - val_recall_mod: 0.2120 - val_precision_mod: 0.7466 - val_dur_error: 0.2670 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 160/500\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.09972\n",
      "25307/25307 - 136s - loss: 0.1059 - f1_score_mod: 0.3029 - recall_mod: 0.1911 - precision_mod: 0.7354 - dur_error: 0.3620 - maestro_dur_loss: 0.0181 - val_loss: 0.0998 - val_f1_score_mod: 0.3292 - val_recall_mod: 0.2103 - val_precision_mod: 0.7596 - val_dur_error: 0.2584 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 161/500\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.09972\n",
      "25307/25307 - 136s - loss: 0.1057 - f1_score_mod: 0.3046 - recall_mod: 0.1929 - precision_mod: 0.7305 - dur_error: 0.3613 - maestro_dur_loss: 0.0181 - val_loss: 0.1027 - val_f1_score_mod: 0.3236 - val_recall_mod: 0.2067 - val_precision_mod: 0.7502 - val_dur_error: 0.3163 - val_maestro_dur_loss: 0.0158\n",
      "Epoch 162/500\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.09972\n",
      "25307/25307 - 136s - loss: 0.1057 - f1_score_mod: 0.3061 - recall_mod: 0.1932 - precision_mod: 0.7430 - dur_error: 0.3622 - maestro_dur_loss: 0.0181 - val_loss: 0.1007 - val_f1_score_mod: 0.3300 - val_recall_mod: 0.2114 - val_precision_mod: 0.7560 - val_dur_error: 0.2788 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 163/500\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.09972\n",
      "25307/25307 - 136s - loss: 0.1055 - f1_score_mod: 0.3063 - recall_mod: 0.1937 - precision_mod: 0.7383 - dur_error: 0.3601 - maestro_dur_loss: 0.0180 - val_loss: 0.1007 - val_f1_score_mod: 0.3342 - val_recall_mod: 0.2160 - val_precision_mod: 0.7430 - val_dur_error: 0.2836 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 164/500\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.09972\n",
      "25307/25307 - 137s - loss: 0.1054 - f1_score_mod: 0.3096 - recall_mod: 0.1964 - precision_mod: 0.7356 - dur_error: 0.3623 - maestro_dur_loss: 0.0181 - val_loss: 0.1040 - val_f1_score_mod: 0.3268 - val_recall_mod: 0.2089 - val_precision_mod: 0.7576 - val_dur_error: 0.3455 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 165/500\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.09972 to 0.09958, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 165s - loss: 0.1052 - f1_score_mod: 0.3071 - recall_mod: 0.1944 - precision_mod: 0.7366 - dur_error: 0.3569 - maestro_dur_loss: 0.0178 - val_loss: 0.0996 - val_f1_score_mod: 0.3351 - val_recall_mod: 0.2165 - val_precision_mod: 0.7454 - val_dur_error: 0.2580 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 166/500\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.09958\n",
      "25307/25307 - 163s - loss: 0.1054 - f1_score_mod: 0.3093 - recall_mod: 0.1963 - precision_mod: 0.7341 - dur_error: 0.3630 - maestro_dur_loss: 0.0182 - val_loss: 0.1011 - val_f1_score_mod: 0.3325 - val_recall_mod: 0.2148 - val_precision_mod: 0.7425 - val_dur_error: 0.2928 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 167/500\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.09958\n",
      "25307/25307 - 167s - loss: 0.1050 - f1_score_mod: 0.3092 - recall_mod: 0.1960 - precision_mod: 0.7365 - dur_error: 0.3594 - maestro_dur_loss: 0.0180 - val_loss: 0.1000 - val_f1_score_mod: 0.3463 - val_recall_mod: 0.2271 - val_precision_mod: 0.7400 - val_dur_error: 0.2705 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 168/500\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.09958 to 0.09917, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 172s - loss: 0.1049 - f1_score_mod: 0.3110 - recall_mod: 0.1977 - precision_mod: 0.7352 - dur_error: 0.3578 - maestro_dur_loss: 0.0179 - val_loss: 0.0992 - val_f1_score_mod: 0.3181 - val_recall_mod: 0.2008 - val_precision_mod: 0.7732 - val_dur_error: 0.2495 - val_maestro_dur_loss: 0.0125\n",
      "Epoch 169/500\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.09917\n",
      "25307/25307 - 169s - loss: 0.1048 - f1_score_mod: 0.3128 - recall_mod: 0.1989 - precision_mod: 0.7389 - dur_error: 0.3579 - maestro_dur_loss: 0.0179 - val_loss: 0.0995 - val_f1_score_mod: 0.3354 - val_recall_mod: 0.2176 - val_precision_mod: 0.7349 - val_dur_error: 0.2604 - val_maestro_dur_loss: 0.0130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.09917\n",
      "25307/25307 - 156s - loss: 0.1047 - f1_score_mod: 0.3153 - recall_mod: 0.2008 - precision_mod: 0.7385 - dur_error: 0.3581 - maestro_dur_loss: 0.0179 - val_loss: 0.1012 - val_f1_score_mod: 0.3444 - val_recall_mod: 0.2250 - val_precision_mod: 0.7368 - val_dur_error: 0.3011 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 171/500\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.09917 to 0.09916, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 158s - loss: 0.1047 - f1_score_mod: 0.3134 - recall_mod: 0.1996 - precision_mod: 0.7348 - dur_error: 0.3612 - maestro_dur_loss: 0.0181 - val_loss: 0.0992 - val_f1_score_mod: 0.3310 - val_recall_mod: 0.2112 - val_precision_mod: 0.7698 - val_dur_error: 0.2566 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 172/500\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.09916 to 0.09906, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 160s - loss: 0.1046 - f1_score_mod: 0.3142 - recall_mod: 0.1999 - precision_mod: 0.7381 - dur_error: 0.3585 - maestro_dur_loss: 0.0179 - val_loss: 0.0991 - val_f1_score_mod: 0.3456 - val_recall_mod: 0.2271 - val_precision_mod: 0.7287 - val_dur_error: 0.2551 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 173/500\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.09906 to 0.09890, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 149s - loss: 0.1042 - f1_score_mod: 0.3175 - recall_mod: 0.2025 - precision_mod: 0.7382 - dur_error: 0.3560 - maestro_dur_loss: 0.0178 - val_loss: 0.0989 - val_f1_score_mod: 0.3363 - val_recall_mod: 0.2178 - val_precision_mod: 0.7435 - val_dur_error: 0.2548 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 174/500\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.09890\n",
      "25307/25307 - 151s - loss: 0.1043 - f1_score_mod: 0.3167 - recall_mod: 0.2020 - precision_mod: 0.7370 - dur_error: 0.3592 - maestro_dur_loss: 0.0180 - val_loss: 0.0994 - val_f1_score_mod: 0.3312 - val_recall_mod: 0.2121 - val_precision_mod: 0.7591 - val_dur_error: 0.2677 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 175/500\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.09890\n",
      "25307/25307 - 161s - loss: 0.1042 - f1_score_mod: 0.3179 - recall_mod: 0.2027 - precision_mod: 0.7394 - dur_error: 0.3576 - maestro_dur_loss: 0.0179 - val_loss: 0.0994 - val_f1_score_mod: 0.3439 - val_recall_mod: 0.2249 - val_precision_mod: 0.7352 - val_dur_error: 0.2714 - val_maestro_dur_loss: 0.0136\n",
      "Epoch 176/500\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.09890\n",
      "25307/25307 - 160s - loss: 0.1040 - f1_score_mod: 0.3200 - recall_mod: 0.2047 - precision_mod: 0.7383 - dur_error: 0.3565 - maestro_dur_loss: 0.0178 - val_loss: 0.0997 - val_f1_score_mod: 0.3482 - val_recall_mod: 0.2301 - val_precision_mod: 0.7188 - val_dur_error: 0.2769 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 177/500\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.09890\n",
      "25307/25307 - 154s - loss: 0.1041 - f1_score_mod: 0.3213 - recall_mod: 0.2058 - precision_mod: 0.7372 - dur_error: 0.3590 - maestro_dur_loss: 0.0180 - val_loss: 0.1006 - val_f1_score_mod: 0.3400 - val_recall_mod: 0.2199 - val_precision_mod: 0.7524 - val_dur_error: 0.2921 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 178/500\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.09890\n",
      "25307/25307 - 162s - loss: 0.1036 - f1_score_mod: 0.3227 - recall_mod: 0.2065 - precision_mod: 0.7429 - dur_error: 0.3555 - maestro_dur_loss: 0.0178 - val_loss: 0.0989 - val_f1_score_mod: 0.3429 - val_recall_mod: 0.2222 - val_precision_mod: 0.7583 - val_dur_error: 0.2659 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 179/500\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.09890 to 0.09829, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 159s - loss: 0.1037 - f1_score_mod: 0.3226 - recall_mod: 0.2067 - precision_mod: 0.7392 - dur_error: 0.3570 - maestro_dur_loss: 0.0179 - val_loss: 0.0983 - val_f1_score_mod: 0.3460 - val_recall_mod: 0.2253 - val_precision_mod: 0.7505 - val_dur_error: 0.2539 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 180/500\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.09829\n",
      "25307/25307 - 155s - loss: 0.1035 - f1_score_mod: 0.3257 - recall_mod: 0.2090 - precision_mod: 0.7418 - dur_error: 0.3578 - maestro_dur_loss: 0.0179 - val_loss: 0.0999 - val_f1_score_mod: 0.3420 - val_recall_mod: 0.2214 - val_precision_mod: 0.7595 - val_dur_error: 0.2855 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 181/500\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.09829\n",
      "25307/25307 - 150s - loss: 0.1033 - f1_score_mod: 0.3277 - recall_mod: 0.2108 - precision_mod: 0.7403 - dur_error: 0.3553 - maestro_dur_loss: 0.0178 - val_loss: 0.0989 - val_f1_score_mod: 0.3438 - val_recall_mod: 0.2228 - val_precision_mod: 0.7577 - val_dur_error: 0.2690 - val_maestro_dur_loss: 0.0135\n",
      "Epoch 182/500\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.09829 to 0.09817, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 152s - loss: 0.1032 - f1_score_mod: 0.3284 - recall_mod: 0.2112 - precision_mod: 0.7427 - dur_error: 0.3557 - maestro_dur_loss: 0.0178 - val_loss: 0.0982 - val_f1_score_mod: 0.3511 - val_recall_mod: 0.2303 - val_precision_mod: 0.7472 - val_dur_error: 0.2514 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 183/500\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.09817\n",
      "25307/25307 - 154s - loss: 0.1035 - f1_score_mod: 0.3251 - recall_mod: 0.2088 - precision_mod: 0.7378 - dur_error: 0.3585 - maestro_dur_loss: 0.0179 - val_loss: 0.1006 - val_f1_score_mod: 0.3548 - val_recall_mod: 0.2337 - val_precision_mod: 0.7399 - val_dur_error: 0.2988 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 184/500\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.09817\n",
      "25307/25307 - 154s - loss: 0.1029 - f1_score_mod: 0.3264 - recall_mod: 0.2097 - precision_mod: 0.7411 - dur_error: 0.3490 - maestro_dur_loss: 0.0175 - val_loss: 0.0992 - val_f1_score_mod: 0.3489 - val_recall_mod: 0.2273 - val_precision_mod: 0.7546 - val_dur_error: 0.2768 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 185/500\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.09817\n",
      "25307/25307 - 151s - loss: 0.1029 - f1_score_mod: 0.3294 - recall_mod: 0.2121 - precision_mod: 0.7406 - dur_error: 0.3538 - maestro_dur_loss: 0.0177 - val_loss: 0.0984 - val_f1_score_mod: 0.3539 - val_recall_mod: 0.2321 - val_precision_mod: 0.7485 - val_dur_error: 0.2604 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 186/500\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.09817\n",
      "25307/25307 - 162s - loss: 0.1029 - f1_score_mod: 0.3303 - recall_mod: 0.2127 - precision_mod: 0.7414 - dur_error: 0.3568 - maestro_dur_loss: 0.0178 - val_loss: 0.1009 - val_f1_score_mod: 0.3558 - val_recall_mod: 0.2353 - val_precision_mod: 0.7352 - val_dur_error: 0.3147 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 187/500\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.09817\n",
      "25307/25307 - 154s - loss: 0.1027 - f1_score_mod: 0.3331 - recall_mod: 0.2151 - precision_mod: 0.7417 - dur_error: 0.3544 - maestro_dur_loss: 0.0177 - val_loss: 0.0986 - val_f1_score_mod: 0.3568 - val_recall_mod: 0.2354 - val_precision_mod: 0.7391 - val_dur_error: 0.2646 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 188/500\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.09817 to 0.09787, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 167s - loss: 0.1027 - f1_score_mod: 0.3353 - recall_mod: 0.2169 - precision_mod: 0.7424 - dur_error: 0.3550 - maestro_dur_loss: 0.0177 - val_loss: 0.0979 - val_f1_score_mod: 0.3554 - val_recall_mod: 0.2331 - val_precision_mod: 0.7507 - val_dur_error: 0.2526 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 189/500\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.09787\n",
      "25307/25307 - 152s - loss: 0.1028 - f1_score_mod: 0.3343 - recall_mod: 0.2161 - precision_mod: 0.7419 - dur_error: 0.3571 - maestro_dur_loss: 0.0179 - val_loss: 0.1025 - val_f1_score_mod: 0.3538 - val_recall_mod: 0.2330 - val_precision_mod: 0.7402 - val_dur_error: 0.3462 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 190/500\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.09787\n",
      "25307/25307 - 160s - loss: 0.1022 - f1_score_mod: 0.3383 - recall_mod: 0.2192 - precision_mod: 0.7451 - dur_error: 0.3524 - maestro_dur_loss: 0.0176 - val_loss: 0.1002 - val_f1_score_mod: 0.3534 - val_recall_mod: 0.2326 - val_precision_mod: 0.7411 - val_dur_error: 0.3027 - val_maestro_dur_loss: 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.09787\n",
      "25307/25307 - 159s - loss: 0.1024 - f1_score_mod: 0.3380 - recall_mod: 0.2194 - precision_mod: 0.7409 - dur_error: 0.3540 - maestro_dur_loss: 0.0177 - val_loss: 0.0986 - val_f1_score_mod: 0.3617 - val_recall_mod: 0.2399 - val_precision_mod: 0.7377 - val_dur_error: 0.2743 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 192/500\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.09787\n",
      "25307/25307 - 155s - loss: 0.1020 - f1_score_mod: 0.3381 - recall_mod: 0.2191 - precision_mod: 0.7449 - dur_error: 0.3514 - maestro_dur_loss: 0.0176 - val_loss: 0.0987 - val_f1_score_mod: 0.3584 - val_recall_mod: 0.2362 - val_precision_mod: 0.7449 - val_dur_error: 0.2747 - val_maestro_dur_loss: 0.0137\n",
      "Epoch 193/500\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.09787\n",
      "25307/25307 - 152s - loss: 0.1021 - f1_score_mod: 0.3355 - recall_mod: 0.2174 - precision_mod: 0.7382 - dur_error: 0.3518 - maestro_dur_loss: 0.0176 - val_loss: 0.0986 - val_f1_score_mod: 0.3658 - val_recall_mod: 0.2437 - val_precision_mod: 0.7384 - val_dur_error: 0.2689 - val_maestro_dur_loss: 0.0134\n",
      "Epoch 194/500\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.09787 to 0.09734, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 155s - loss: 0.1019 - f1_score_mod: 0.3402 - recall_mod: 0.2209 - precision_mod: 0.7448 - dur_error: 0.3513 - maestro_dur_loss: 0.0176 - val_loss: 0.0973 - val_f1_score_mod: 0.3455 - val_recall_mod: 0.2234 - val_precision_mod: 0.7677 - val_dur_error: 0.2489 - val_maestro_dur_loss: 0.0124\n",
      "Epoch 195/500\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.09734\n",
      "25307/25307 - 150s - loss: 0.1017 - f1_score_mod: 0.3417 - recall_mod: 0.2219 - precision_mod: 0.7471 - dur_error: 0.3505 - maestro_dur_loss: 0.0175 - val_loss: 0.1003 - val_f1_score_mod: 0.3576 - val_recall_mod: 0.2344 - val_precision_mod: 0.7569 - val_dur_error: 0.3117 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 196/500\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.09734 to 0.09717, saving model to ../models/best_maestro_model_2_1_512_0pt4_lr_1e-04_cv_0pt2.h5\n",
      "25307/25307 - 151s - loss: 0.1016 - f1_score_mod: 0.3440 - recall_mod: 0.2240 - precision_mod: 0.7448 - dur_error: 0.3514 - maestro_dur_loss: 0.0176 - val_loss: 0.0972 - val_f1_score_mod: 0.3619 - val_recall_mod: 0.2395 - val_precision_mod: 0.7427 - val_dur_error: 0.2510 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 197/500\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.09717\n",
      "25307/25307 - 151s - loss: 0.1018 - f1_score_mod: 0.3433 - recall_mod: 0.2235 - precision_mod: 0.7437 - dur_error: 0.3542 - maestro_dur_loss: 0.0177 - val_loss: 0.1004 - val_f1_score_mod: 0.3641 - val_recall_mod: 0.2419 - val_precision_mod: 0.7423 - val_dur_error: 0.3143 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 198/500\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0001, clipvalue = 0.2, epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
