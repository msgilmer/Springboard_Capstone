{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this [tutorial](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import tempfile\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../train_and_val/X_train_ext.npy')\n",
    "X_val = np.load('../train_and_val/X_val_ext.npy')\n",
    "y_train = np.load('../train_and_val/y_train_ext.npy')\n",
    "y_val = np.load('../train_and_val/y_val_ext.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Scaled Duration for X_train: 0.860215053763441\n",
      "Maximum Scaled Duration for X_val: 0.5516975308641976\n",
      "Maximum Scaled Duration for y_train: 1.0\n",
      "Maximum Scaled Duration for y_val: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Maximum Scaled Duration for X_train: {}'.format(X_train[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for X_val: {}'.format(X_val[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for y_train: {}'.format(y_train[:, -1].max()))\n",
    "print('Maximum Scaled Duration for y_val: {}'.format(y_val[:, -1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Ratio of the Mean of the Scaled Duration:  0.9898257021922111\n",
      "Train-Validation Ratio of the Stdv of the Scaled Duration:  0.9209054376139967\n"
     ]
    }
   ],
   "source": [
    "print('Train-Validation Ratio of the Mean of the Scaled Duration: ', y_train[:, -1].mean() / y_val[:, -1].mean())\n",
    "print('Train-Validation Ratio of the Stdv of the Scaled Duration: ', y_train[:, -1].std() / y_val[:, -1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maestro_loss_wr(harshness, n_dur_nodes): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last n_dur_nodes elements (whose mean represents\n",
    "    the duration). The proportionality constant is the 'harshness' of the maestro in \n",
    "    regards to timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-n_dur_nodes] * K.log(ypred[:, :-n_dur_nodes]) + \\\n",
    "                            (1 - ytrue[:, :-n_dur_nodes]) * K.log(1 - ypred[:, :-n_dur_nodes]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs(K.mean(ytrue[:, -n_dur_nodes:], axis = 1) - \\\n",
    "                                                K.mean(ypred[:, -n_dur_nodes:], axis = 1)) / \\\n",
    "                                      (K.mean(ytrue[:, -n_dur_nodes:], axis = 1) + \\\n",
    "                                       K.mean(ypred[:, -n_dur_nodes:], axis = 1) + K.epsilon()))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, :-n_dur_nodes] elements will be zero\n",
    "            return bce_loss * 2     # (for a rest). This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "def precision_mod_wr(n_dur_nodes):\n",
    "    def precision_mod(ytrue, ypred):\n",
    "        \"\"\"Just a modified precision excluding the last n_dur_nodes elements (which are not\n",
    "        classification nodes)\"\"\"\n",
    "\n",
    "        true_positives = K.sum(K.round(ytrue[:, :-n_dur_nodes] * ypred[:, :-n_dur_nodes]))\n",
    "        pred_positives = K.sum(K.round(ypred[:, :-n_dur_nodes]))\n",
    "        return true_positives / (pred_positives + K.epsilon())\n",
    "    return precision_mod\n",
    "\n",
    "def recall_mod_wr(n_dur_nodes):\n",
    "    def recall_mod(ytrue, ypred):\n",
    "        \"\"\"Just a modified recall excluding the last n_dur_nodes elements (which are not\n",
    "        classification nodes)\"\"\"\n",
    "\n",
    "        true_positives = K.sum(K.round(ytrue[:, :-n_dur_nodes] * ypred[:, :-n_dur_nodes]))\n",
    "        poss_positives = K.sum(ytrue[:, :-n_dur_nodes])\n",
    "        return true_positives / (poss_positives + K.epsilon())\n",
    "    return recall_mod\n",
    "\n",
    "def f1_score_mod_wr(n_dur_nodes):\n",
    "    def f1_score_mod(ytrue, ypred):\n",
    "        \"\"\"Just a modified f1_score excluding the last n_dur_nodes elements (which are not\n",
    "        classification nodes)\"\"\"\n",
    "\n",
    "        precision = precision_mod_wr(n_dur_nodes)(ytrue, ypred)\n",
    "        recall = recall_mod_wr(n_dur_nodes)(ytrue, ypred)   \n",
    "        return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_score_mod\n",
    "\n",
    "def dur_error_wr(n_dur_nodes):\n",
    "    def dur_error(ytrue, ypred):\n",
    "        \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "        return 2 * K.mean(K.abs((K.mean(ytrue[:, -n_dur_nodes:], axis = 1) - \\\n",
    "                   K.mean(ypred[:, -n_dur_nodes:], axis = 1)) / (K.mean(ytrue[:, -n_dur_nodes:], \\\n",
    "                    axis = 1) + K.mean(ypred[:, -n_dur_nodes:], axis = 1) + K.epsilon())))\n",
    "    return dur_error\n",
    "\n",
    "def maestro_dur_loss_wr(harshness, n_dur_nodes):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((K.mean(ytrue[:, -n_dur_nodes:], axis = 1) - \\\n",
    "                                      K.mean(ypred[:, -n_dur_nodes:], axis = 1)) / \\\n",
    "                                      (K.mean(ytrue[:, -n_dur_nodes:], axis = 1) + \\\n",
    "                                      K.mean(ypred[:, -n_dur_nodes:], axis = 1) + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "harshness = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(file_path, harshness = harshness, n_dur_nodes = 20):\n",
    "    \n",
    "    custom_objects = {'maestro_loss': maestro_loss_wr(harshness, \\\n",
    "        n_dur_nodes), 'f1_score_mod': f1_score_mod_wr(n_dur_nodes), \\\n",
    "        'recall_mod': recall_mod_wr(n_dur_nodes), 'precision_mod': \\\n",
    "        precision_mod_wr(n_dur_nodes), 'dur_error': \\\n",
    "        dur_error_wr(n_dur_nodes), 'maestro_dur_loss': \\\n",
    "        maestro_dur_loss_wr(harshness, n_dur_nodes)}\n",
    "\n",
    "    return load_model(file_path, custom_objects = custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_with_checkpoint(model, filename = 'best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5', \\\n",
    "                                harshness = 0.05, n_dur_nodes = 20, batch_size = 512, epochs = 2, \\\n",
    "                                initial_sparsity = 0.5, final_sparsity = 0.8):\n",
    "    \n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    end_step = np.ceil(X_train.shape[0] / batch_size) * epochs\n",
    "    \n",
    "    # Define model for pruning.\n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "              initial_sparsity = initial_sparsity, final_sparsity = final_sparsity,\n",
    "              begin_step=0, end_step=end_step)\n",
    "    }\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "    \n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(loss = maestro_loss_wr(harshness, n_dur_nodes), \n",
    "                          optimizer = opt, \n",
    "                          metrics = [f1_score_mod_wr(n_dur_nodes), recall_mod_wr(n_dur_nodes), \\\n",
    "                                     precision_mod_wr(n_dur_nodes), dur_error_wr(n_dur_nodes), \\\n",
    "                                     maestro_dur_loss_wr(harshness, n_dur_nodes)])\n",
    "\n",
    "    model_for_pruning.summary()\n",
    "    \n",
    "    logdir = tempfile.mkdtemp()\n",
    "\n",
    "    mc = ModelCheckpoint('../models/' + filename, monitor = 'val_loss', mode = 'min', \\\n",
    "                                                            save_best_only = True, verbose = 1)\n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "      tfmot.sparsity.keras.PruningSummaries(log_dir = logdir),\n",
    "      mc, \n",
    "      TerminateOnNaN()\n",
    "    ]\n",
    "\n",
    "    model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n",
    "                      validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_lstm_4 ( (None, 16, 1024)          9277443   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 16, 1024)          1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_lstm_5 ( (None, 1024)              16781315  \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 1024)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_4  (None, 512)               1049090   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_5  (None, 108)               110702    \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 108)               1         \n",
      "=================================================================\n",
      "Total params: 27,218,555\n",
      "Trainable params: 13,613,676\n",
      "Non-trainable params: 13,604,879\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07691, saving model to ../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-feb6692b4220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/best_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpruned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_model_with_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-7c814fc8a57f>\u001b[0m in \u001b[0;36mprune_model_with_checkpoint\u001b[0;34m(model, filename, harshness, n_dur_nodes, batch_size, epochs, initial_sparsity, final_sparsity)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n\u001b[0;32m---> 37\u001b[0;31m                       validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     filepath, overwrite=True, options=self._options)\n\u001b[1;32m   1300\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m       \u001b[0mparam_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5')\n",
    "pruned_model = prune_model_with_checkpoint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, filename = 'best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5', harshness = 0.05, \\\n",
    "                n_dur_nodes = 20, batch_size = 512, epochs = 50, initial_sparsity = 0.5, final_sparsity = 0.8):\n",
    "    \n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    end_step = np.ceil(X_train.shape[0] / batch_size) * epochs\n",
    "    \n",
    "    # Define model for pruning.\n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "              initial_sparsity = initial_sparsity, final_sparsity = final_sparsity,\n",
    "              begin_step=0, end_step=end_step)\n",
    "    }\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "    \n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(loss = maestro_loss_wr(harshness, n_dur_nodes), \n",
    "                          optimizer = opt, \n",
    "                          metrics = [f1_score_mod_wr(n_dur_nodes), recall_mod_wr(n_dur_nodes), \\\n",
    "                                     precision_mod_wr(n_dur_nodes), dur_error_wr(n_dur_nodes), \\\n",
    "                                     maestro_dur_loss_wr(harshness, n_dur_nodes)])\n",
    "\n",
    "    #model_for_pruning.summary()\n",
    "    \n",
    "    logdir = tempfile.mkdtemp()\n",
    "\n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "      tfmot.sparsity.keras.PruningSummaries(log_dir = logdir),\n",
    "      TerminateOnNaN()\n",
    "    ]\n",
    "    filepath = '../models/' + filename\n",
    "    # ModelCheckpoint is giving a funny error (RuntimeError: Unable to create link (name already exists), \n",
    "    # so here is my workaround:\n",
    "    print('Epoch 1/{}'.format(epochs))\n",
    "    history = model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = 1, \n",
    "                validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n",
    "    if (np.isnan(history.history['val_loss'][0])): # NaN failure in first epoch\n",
    "        return model\n",
    "    else:\n",
    "        min_val_loss = history.history['val_loss'][0]\n",
    "        print('val_loss is {a:2.5f}, saving model to {b}'.format(a = min_val_loss, b = filepath))\n",
    "        model.save(filepath, save_format = 'h5')\n",
    "        \n",
    "    for i in range(epochs - 1):\n",
    "        print('Epoch {}/{}'.format(i + 2, epochs))\n",
    "        history = model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = 1, \n",
    "                      validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n",
    "        if (np.isnan(history.history['val_loss'][0])): # NaN failure  \n",
    "            break\n",
    "        else:\n",
    "            if (history.history['val_loss'][0] < min_val_loss):\n",
    "                print('val_loss improved from {a:2.5f} to {b:2.5f}, saving model to {c}'.format(\\\n",
    "                            a = min_val_loss, b = history.history['val_loss'][0], c = filepath))\n",
    "                model.save(filepath, save_format = 'h5')\n",
    "                min_val_loss = history.history['val_loss'][0]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 - 579s - loss: 0.0578 - f1_score_mod: 0.6506 - recall_mod: 0.5454 - precision_mod: 0.8072 - dur_error: 0.1638 - maestro_dur_loss: 0.0082 - val_loss: 0.0774 - val_f1_score_mod: 0.5764 - val_recall_mod: 0.4769 - val_precision_mod: 0.7287 - val_dur_error: 0.1716 - val_maestro_dur_loss: 0.0086\n",
      "val_loss is 0.07739, saving model to ../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5\n",
      "Epoch 2/20\n",
      "50/50 - 578s - loss: 0.0640 - f1_score_mod: 0.5692 - recall_mod: 0.4340 - precision_mod: 0.8344 - dur_error: 0.1823 - maestro_dur_loss: 0.0091 - val_loss: 0.0764 - val_f1_score_mod: 0.5611 - val_recall_mod: 0.4450 - val_precision_mod: 0.7595 - val_dur_error: 0.1676 - val_maestro_dur_loss: 0.0084\n",
      "val_loss improved from 0.07739 to 0.07642, saving model to ../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5\n",
      "Epoch 3/20\n",
      "50/50 - 587s - loss: 0.0559 - f1_score_mod: 0.6445 - recall_mod: 0.5240 - precision_mod: 0.8377 - dur_error: 0.1619 - maestro_dur_loss: 0.0081 - val_loss: 0.0761 - val_f1_score_mod: 0.5767 - val_recall_mod: 0.4715 - val_precision_mod: 0.7424 - val_dur_error: 0.1656 - val_maestro_dur_loss: 0.0083\n",
      "val_loss improved from 0.07642 to 0.07611, saving model to ../models/best_pruned_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5\n",
      "Epoch 4/20\n",
      "50/50 - 564s - loss: 0.0566 - f1_score_mod: 0.6547 - recall_mod: 0.5515 - precision_mod: 0.8061 - dur_error: 0.1621 - maestro_dur_loss: 0.0081 - val_loss: 0.0777 - val_f1_score_mod: 0.5821 - val_recall_mod: 0.4846 - val_precision_mod: 0.7289 - val_dur_error: 0.1646 - val_maestro_dur_loss: 0.0082\n",
      "Epoch 5/20\n",
      "50/50 - 508s - loss: 0.0527 - f1_score_mod: 0.6764 - recall_mod: 0.5715 - precision_mod: 0.8289 - dur_error: 0.1572 - maestro_dur_loss: 0.0079 - val_loss: 0.0774 - val_f1_score_mod: 0.5903 - val_recall_mod: 0.4977 - val_precision_mod: 0.7254 - val_dur_error: 0.1643 - val_maestro_dur_loss: 0.0082\n",
      "Epoch 6/20\n",
      "50/50 - 461s - loss: 0.0559 - f1_score_mod: 0.6422 - recall_mod: 0.5251 - precision_mod: 0.8284 - dur_error: 0.1630 - maestro_dur_loss: 0.0082 - val_loss: 0.0782 - val_f1_score_mod: 0.5818 - val_recall_mod: 0.4810 - val_precision_mod: 0.7363 - val_dur_error: 0.1648 - val_maestro_dur_loss: 0.0082\n",
      "Epoch 7/20\n",
      "50/50 - 475s - loss: 0.0514 - f1_score_mod: 0.6819 - recall_mod: 0.5760 - precision_mod: 0.8359 - dur_error: 0.1560 - maestro_dur_loss: 0.0078 - val_loss: 0.0781 - val_f1_score_mod: 0.5917 - val_recall_mod: 0.5013 - val_precision_mod: 0.7222 - val_dur_error: 0.1647 - val_maestro_dur_loss: 0.0082\n",
      "Epoch 8/20\n",
      "50/50 - 505s - loss: 0.0545 - f1_score_mod: 0.6618 - recall_mod: 0.5574 - precision_mod: 0.8149 - dur_error: 0.1604 - maestro_dur_loss: 0.0080 - val_loss: 0.0789 - val_f1_score_mod: 0.5852 - val_recall_mod: 0.4935 - val_precision_mod: 0.7190 - val_dur_error: 0.1653 - val_maestro_dur_loss: 0.0083\n",
      "Epoch 9/20\n",
      "50/50 - 518s - loss: 0.0505 - f1_score_mod: 0.6902 - recall_mod: 0.5884 - precision_mod: 0.8350 - dur_error: 0.1555 - maestro_dur_loss: 0.0078 - val_loss: 0.0791 - val_f1_score_mod: 0.5917 - val_recall_mod: 0.5042 - val_precision_mod: 0.7162 - val_dur_error: 0.1623 - val_maestro_dur_loss: 0.0081\n",
      "Epoch 10/20\n",
      "50/50 - 487s - loss: 0.0535 - f1_score_mod: 0.6656 - recall_mod: 0.5566 - precision_mod: 0.8283 - dur_error: 0.1597 - maestro_dur_loss: 0.0080 - val_loss: 0.0788 - val_f1_score_mod: 0.5884 - val_recall_mod: 0.4954 - val_precision_mod: 0.7246 - val_dur_error: 0.1651 - val_maestro_dur_loss: 0.0083\n",
      "Epoch 11/20\n",
      "50/50 - 466s - loss: 0.0498 - f1_score_mod: 0.6929 - recall_mod: 0.5917 - precision_mod: 0.8362 - dur_error: 0.1542 - maestro_dur_loss: 0.0077 - val_loss: 0.0793 - val_f1_score_mod: 0.5975 - val_recall_mod: 0.5143 - val_precision_mod: 0.7129 - val_dur_error: 0.1635 - val_maestro_dur_loss: 0.0082\n",
      "Epoch 12/20\n",
      "50/50 - 463s - loss: 0.0512 - f1_score_mod: 0.6847 - recall_mod: 0.5832 - precision_mod: 0.8292 - dur_error: 0.1576 - maestro_dur_loss: 0.0079 - val_loss: 0.0800 - val_f1_score_mod: 0.5942 - val_recall_mod: 0.5036 - val_precision_mod: 0.7248 - val_dur_error: 0.1622 - val_maestro_dur_loss: 0.0081\n",
      "Epoch 13/20\n",
      "50/50 - 451s - loss: 0.0484 - f1_score_mod: 0.7036 - recall_mod: 0.6055 - precision_mod: 0.8401 - dur_error: 0.1532 - maestro_dur_loss: 0.0077 - val_loss: 0.0800 - val_f1_score_mod: 0.6009 - val_recall_mod: 0.5173 - val_precision_mod: 0.7170 - val_dur_error: 0.1634 - val_maestro_dur_loss: 0.0082\n",
      "Epoch 14/20\n",
      "50/50 - 464s - loss: 0.0486 - f1_score_mod: 0.7028 - recall_mod: 0.6060 - precision_mod: 0.8368 - dur_error: 0.1539 - maestro_dur_loss: 0.0077 - val_loss: 0.0809 - val_f1_score_mod: 0.5996 - val_recall_mod: 0.5174 - val_precision_mod: 0.7131 - val_dur_error: 0.1628 - val_maestro_dur_loss: 0.0081\n",
      "Epoch 15/20\n",
      "50/50 - 587s - loss: 0.0463 - f1_score_mod: 0.7211 - recall_mod: 0.6295 - precision_mod: 0.8443 - dur_error: 0.1525 - maestro_dur_loss: 0.0076 - val_loss: 0.0814 - val_f1_score_mod: 0.6034 - val_recall_mod: 0.5260 - val_precision_mod: 0.7076 - val_dur_error: 0.1620 - val_maestro_dur_loss: 0.0081\n",
      "Epoch 16/20\n",
      "50/50 - 468s - loss: 0.0459 - f1_score_mod: 0.7260 - recall_mod: 0.6373 - precision_mod: 0.8437 - dur_error: 0.1504 - maestro_dur_loss: 0.0075 - val_loss: 0.0817 - val_f1_score_mod: 0.6073 - val_recall_mod: 0.5312 - val_precision_mod: 0.7092 - val_dur_error: 0.1616 - val_maestro_dur_loss: 0.0081\n",
      "Epoch 17/20\n",
      "50/50 - 437s - loss: 0.0445 - f1_score_mod: 0.7374 - recall_mod: 0.6514 - precision_mod: 0.8499 - dur_error: 0.1497 - maestro_dur_loss: 0.0075 - val_loss: 0.0825 - val_f1_score_mod: 0.6089 - val_recall_mod: 0.5346 - val_precision_mod: 0.7073 - val_dur_error: 0.1608 - val_maestro_dur_loss: 0.0080\n",
      "Epoch 18/20\n",
      "Batch 0: Invalid loss, terminating training\n",
      "Batch 1: Invalid loss, terminating training\n",
      "Batch 2: Invalid loss, terminating training\n",
      "Batch 3: Invalid loss, terminating training\n",
      "Batch 4: Invalid loss, terminating training\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Batch 6: Invalid loss, terminating training\n",
      "Batch 7: Invalid loss, terminating training\n",
      "Batch 8: Invalid loss, terminating training\n",
      "Batch 9: Invalid loss, terminating training\n",
      "Batch 10: Invalid loss, terminating training\n",
      "Batch 11: Invalid loss, terminating training\n",
      "Batch 12: Invalid loss, terminating training\n",
      "Batch 13: Invalid loss, terminating training\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "50/50 - 557s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_ext20_2_1_1024_0pt4_mnv_2.h5')\n",
    "pruned_model = prune_model(model, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 - 257s - loss: 0.1844 - f1_score_mod: 0.1412 - recall_mod: 0.0783 - precision_mod: 0.7431 - dur_error: 1.7618 - maestro_dur_loss: 0.0881 - val_loss: 0.1770 - val_f1_score_mod: 0.1812 - val_recall_mod: 0.1016 - val_precision_mod: 0.8396 - val_dur_error: 1.7588 - val_maestro_dur_loss: 0.0879\n",
      "val_loss is 0.17695, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 2/50\n",
      "50/50 - 247s - loss: 0.1776 - f1_score_mod: 0.1847 - recall_mod: 0.1053 - precision_mod: 0.7681 - dur_error: 1.7587 - maestro_dur_loss: 0.0879 - val_loss: 0.1734 - val_f1_score_mod: 0.2140 - val_recall_mod: 0.1230 - val_precision_mod: 0.8271 - val_dur_error: 1.7559 - val_maestro_dur_loss: 0.0878\n",
      "val_loss improved from 0.17695 to 0.17344, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 3/50\n",
      "50/50 - 246s - loss: 0.1745 - f1_score_mod: 0.2019 - recall_mod: 0.1162 - precision_mod: 0.7764 - dur_error: 1.7562 - maestro_dur_loss: 0.0878 - val_loss: 0.1715 - val_f1_score_mod: 0.2321 - val_recall_mod: 0.1350 - val_precision_mod: 0.8311 - val_dur_error: 1.7535 - val_maestro_dur_loss: 0.0877\n",
      "val_loss improved from 0.17344 to 0.17146, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 4/50\n",
      "50/50 - 270s - loss: 0.1713 - f1_score_mod: 0.2174 - recall_mod: 0.1267 - precision_mod: 0.7714 - dur_error: 1.7540 - maestro_dur_loss: 0.0877 - val_loss: 0.1698 - val_f1_score_mod: 0.2309 - val_recall_mod: 0.1337 - val_precision_mod: 0.8499 - val_dur_error: 1.7523 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.17146 to 0.16976, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 5/50\n",
      "50/50 - 217s - loss: 0.1687 - f1_score_mod: 0.2288 - recall_mod: 0.1346 - precision_mod: 0.7684 - dur_error: 1.7534 - maestro_dur_loss: 0.0877 - val_loss: 0.1684 - val_f1_score_mod: 0.2583 - val_recall_mod: 0.1533 - val_precision_mod: 0.8216 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16976 to 0.16837, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 6/50\n",
      "50/50 - 134s - loss: 0.1669 - f1_score_mod: 0.2399 - recall_mod: 0.1423 - precision_mod: 0.7695 - dur_error: 1.7528 - maestro_dur_loss: 0.0876 - val_loss: 0.1678 - val_f1_score_mod: 0.2690 - val_recall_mod: 0.1615 - val_precision_mod: 0.8067 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16837 to 0.16781, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 7/50\n",
      "50/50 - 133s - loss: 0.1656 - f1_score_mod: 0.2462 - recall_mod: 0.1471 - precision_mod: 0.7592 - dur_error: 1.7526 - maestro_dur_loss: 0.0876 - val_loss: 0.1670 - val_f1_score_mod: 0.2792 - val_recall_mod: 0.1689 - val_precision_mod: 0.8064 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16781 to 0.16701, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 8/50\n",
      "50/50 - 142s - loss: 0.1629 - f1_score_mod: 0.2549 - recall_mod: 0.1531 - precision_mod: 0.7649 - dur_error: 1.7528 - maestro_dur_loss: 0.0876 - val_loss: 0.1666 - val_f1_score_mod: 0.2843 - val_recall_mod: 0.1725 - val_precision_mod: 0.8095 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16701 to 0.16656, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 9/50\n",
      "50/50 - 131s - loss: 0.1627 - f1_score_mod: 0.2593 - recall_mod: 0.1567 - precision_mod: 0.7549 - dur_error: 1.7527 - maestro_dur_loss: 0.0876 - val_loss: 0.1661 - val_f1_score_mod: 0.2880 - val_recall_mod: 0.1753 - val_precision_mod: 0.8097 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16656 to 0.16608, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 10/50\n",
      "50/50 - 139s - loss: 0.1609 - f1_score_mod: 0.2653 - recall_mod: 0.1610 - precision_mod: 0.7574 - dur_error: 1.7527 - maestro_dur_loss: 0.0876 - val_loss: 0.1658 - val_f1_score_mod: 0.2969 - val_recall_mod: 0.1825 - val_precision_mod: 0.7975 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16608 to 0.16579, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 11/50\n",
      "50/50 - 139s - loss: 0.1608 - f1_score_mod: 0.2608 - recall_mod: 0.1581 - precision_mod: 0.7502 - dur_error: 1.7529 - maestro_dur_loss: 0.0876 - val_loss: 0.1658 - val_f1_score_mod: 0.3004 - val_recall_mod: 0.1853 - val_precision_mod: 0.7943 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16579 to 0.16577, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 12/50\n",
      "50/50 - 151s - loss: 0.1589 - f1_score_mod: 0.2696 - recall_mod: 0.1643 - precision_mod: 0.7574 - dur_error: 1.7530 - maestro_dur_loss: 0.0876 - val_loss: 0.1653 - val_f1_score_mod: 0.3037 - val_recall_mod: 0.1879 - val_precision_mod: 0.7928 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16577 to 0.16530, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 13/50\n",
      "50/50 - 262s - loss: 0.1588 - f1_score_mod: 0.2722 - recall_mod: 0.1663 - precision_mod: 0.7533 - dur_error: 1.7528 - maestro_dur_loss: 0.0876 - val_loss: 0.1653 - val_f1_score_mod: 0.3013 - val_recall_mod: 0.1861 - val_precision_mod: 0.7929 - val_dur_error: 1.7514 - val_maestro_dur_loss: 0.0876\n",
      "val_loss improved from 0.16530 to 0.16530, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 14/50\n",
      "Batch 8: Invalid loss, terminating training\n",
      "Batch 9: Invalid loss, terminating training\n",
      "Batch 10: Invalid loss, terminating training\n",
      "Batch 11: Invalid loss, terminating training\n",
      "Batch 12: Invalid loss, terminating training\n",
      "Batch 13: Invalid loss, terminating training\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "50/50 - 260s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5')\n",
    "pruned_model = prune_model(model, epochs = 50, initial_sparsity = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 - 289s - loss: 0.0865 - f1_score_mod: 0.4790 - recall_mod: 0.3529 - precision_mod: 0.7469 - dur_error: 0.3443 - maestro_dur_loss: 0.0172 - val_loss: 0.0920 - val_f1_score_mod: 0.4464 - val_recall_mod: 0.3265 - val_precision_mod: 0.7063 - val_dur_error: 0.2490 - val_maestro_dur_loss: 0.0125\n",
      "val_loss is 0.09196, saving model to ../models/best_maestro_model_pruned_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "Epoch 2/50\n",
      "50/50 - 277s - loss: 0.1139 - f1_score_mod: 0.3872 - recall_mod: 0.2791 - precision_mod: 0.7536 - dur_error: 0.7666 - maestro_dur_loss: 0.0383 - val_loss: 0.1787 - val_f1_score_mod: 0.1993 - val_recall_mod: 0.1135 - val_precision_mod: 0.8234 - val_dur_error: 1.7600 - val_maestro_dur_loss: 0.0880\n",
      "Epoch 3/50\n",
      "50/50 - 277s - loss: 0.1747 - f1_score_mod: 0.2171 - recall_mod: 0.1267 - precision_mod: 0.7701 - dur_error: 1.7603 - maestro_dur_loss: 0.0880 - val_loss: 0.1700 - val_f1_score_mod: 0.2602 - val_recall_mod: 0.1542 - val_precision_mod: 0.8345 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 4/50\n",
      "50/50 - 274s - loss: 0.1661 - f1_score_mod: 0.2605 - recall_mod: 0.1566 - precision_mod: 0.7794 - dur_error: 1.7599 - maestro_dur_loss: 0.0880 - val_loss: 0.1666 - val_f1_score_mod: 0.2921 - val_recall_mod: 0.1780 - val_precision_mod: 0.8151 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 5/50\n",
      "50/50 - 321s - loss: 0.1613 - f1_score_mod: 0.2852 - recall_mod: 0.1748 - precision_mod: 0.7804 - dur_error: 1.7602 - maestro_dur_loss: 0.0880 - val_loss: 0.1649 - val_f1_score_mod: 0.3116 - val_recall_mod: 0.1930 - val_precision_mod: 0.8091 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 6/50\n",
      "50/50 - 299s - loss: 0.1577 - f1_score_mod: 0.3075 - recall_mod: 0.1921 - precision_mod: 0.7739 - dur_error: 1.7600 - maestro_dur_loss: 0.0880 - val_loss: 0.1639 - val_f1_score_mod: 0.3304 - val_recall_mod: 0.2083 - val_precision_mod: 0.8002 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 7/50\n",
      "50/50 - 248s - loss: 0.1551 - f1_score_mod: 0.3188 - recall_mod: 0.2016 - precision_mod: 0.7648 - dur_error: 1.7599 - maestro_dur_loss: 0.0880 - val_loss: 0.1635 - val_f1_score_mod: 0.3408 - val_recall_mod: 0.2172 - val_precision_mod: 0.7930 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 8/50\n",
      "50/50 - 248s - loss: 0.1530 - f1_score_mod: 0.3367 - recall_mod: 0.2155 - precision_mod: 0.7726 - dur_error: 1.7597 - maestro_dur_loss: 0.0880 - val_loss: 0.1630 - val_f1_score_mod: 0.3602 - val_recall_mod: 0.2348 - val_precision_mod: 0.7741 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 9/50\n",
      "50/50 - 248s - loss: 0.1510 - f1_score_mod: 0.3493 - recall_mod: 0.2268 - precision_mod: 0.7615 - dur_error: 1.7597 - maestro_dur_loss: 0.0880 - val_loss: 0.1620 - val_f1_score_mod: 0.3676 - val_recall_mod: 0.2405 - val_precision_mod: 0.7808 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 10/50\n",
      "50/50 - 250s - loss: 0.1494 - f1_score_mod: 0.3589 - recall_mod: 0.2347 - precision_mod: 0.7647 - dur_error: 1.7599 - maestro_dur_loss: 0.0880 - val_loss: 0.1619 - val_f1_score_mod: 0.3807 - val_recall_mod: 0.2539 - val_precision_mod: 0.7626 - val_dur_error: 1.7585 - val_maestro_dur_loss: 0.0879\n",
      "Epoch 11/50\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5')\n",
    "pruned_model = prune_model(model, epochs = 50, initial_sparsity = 0.6, final_sparsity = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
