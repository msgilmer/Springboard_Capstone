{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this [tutorial](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import tempfile\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../train_and_val/X_train.npy')\n",
    "X_val = np.load('../train_and_val/X_val.npy')\n",
    "y_train = np.load('../train_and_val/y_train.npy')\n",
    "y_val = np.load('../train_and_val/y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Scaled Duration for X_train: 0.860215053763441\n",
      "Maximum Scaled Duration for X_val: 0.5516975308641976\n",
      "Maximum Scaled Duration for y_train: 1.0\n",
      "Maximum Scaled Duration for y_val: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Maximum Scaled Duration for X_train: {}'.format(X_train[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for X_val: {}'.format(X_val[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for y_train: {}'.format(y_train[:, -1].max()))\n",
    "print('Maximum Scaled Duration for y_val: {}'.format(y_val[:, -1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Ratio of the Mean of the Scaled Duration:  0.9898257021922111\n",
      "Train-Validation Ratio of the Stdv of the Scaled Duration:  0.9209054376139967\n"
     ]
    }
   ],
   "source": [
    "print('Train-Validation Ratio of the Mean of the Scaled Duration: ', y_train[:, -1].mean() / y_val[:, -1].mean())\n",
    "print('Train-Validation Ratio of the Stdv of the Scaled Duration: ', y_train[:, -1].std() / y_val[:, -1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maestro_loss_wr(harshness): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last element (which repesents the duration). \n",
    "    The proportionality constant is the 'harshness' of the maestro in regards to\n",
    "    timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-1] * K.log(ypred[:, :-1]) + (1 - ytrue[:, :-1]) * \\\n",
    "                     K.log(1 - ypred[:, :-1]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, -1] elements will be zero\n",
    "            return bce_loss * 2     # This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "\n",
    "def precision_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified precision excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    pred_positives = K.sum(K.round(ypred[:, :-1]))\n",
    "    return true_positives / (pred_positives + K.epsilon())\n",
    "\n",
    "def recall_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified recall excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    poss_positives = K.sum(ytrue[:, :-1])\n",
    "    return true_positives / (poss_positives + K.epsilon())\n",
    "\n",
    "def f1_score_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified f1_score excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    precision = precision_mod(ytrue, ypred)\n",
    "    recall = recall_mod(ytrue, ypred)   \n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def dur_error(ytrue, ypred):\n",
    "    \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "    return 2 * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / (ytrue[:, -1] + ypred[:, -1] + \\\n",
    "                                                         K.epsilon())))\n",
    "\n",
    "def maestro_dur_loss_wr(harshness):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "harshness = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(file_path, harshness = harshness):\n",
    "    \n",
    "    custom_objects = { 'maestro_loss': maestro_loss_wr(harshness), \\\n",
    "        'f1_score_mod': f1_score_mod, 'recall_mod': recall_mod, \\\n",
    "        'precision_mod': precision_mod, 'dur_error': dur_error, \\\n",
    "        'maestro_dur_loss': maestro_dur_loss_wr(harshness)}\n",
    "\n",
    "    return load_model(file_path, custom_objects = custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RMSprop(lr = 0.0005, clipvalue = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, batch_size = 512, epochs = 2, initial_sparsity = 0.5, final_sparsity = 0.8):\n",
    "    \n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    end_step = np.ceil(X_train.shape[0] / batch_size) * epochs\n",
    "    \n",
    "    # Define model for pruning.\n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "              initial_sparsity = initial_sparsity, final_sparsity = final_sparsity,\n",
    "              begin_step=0, end_step=end_step)\n",
    "    }\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "    \n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(loss = maestro_loss_wr(harshness), \n",
    "                          optimizer = opt, \n",
    "                          metrics = [f1_score_mod, recall_mod, precision_mod, \\\n",
    "                                     dur_error, maestro_dur_loss_wr(harshness)])\n",
    "\n",
    "    model_for_pruning.summary()\n",
    "    \n",
    "    logdir = tempfile.mkdtemp()\n",
    "\n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "      tfmot.sparsity.keras.PruningSummaries(log_dir = logdir),\n",
    "    ]\n",
    "\n",
    "    model_for_pruning.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \n",
    "                      validation_data = (X_val, y_val), verbose = 2, callbacks = callbacks)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:200: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_lstm_20  (None, 16, 512)           2463747   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 16, 512)           1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_lstm_21  (None, 512)               4196355   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_20 (None, 256)               262402    \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_21 (None, 89)                45659     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 89)                1         \n",
      "=================================================================\n",
      "Total params: 6,968,168\n",
      "Trainable params: 3,486,297\n",
      "Non-trainable params: 3,481,871\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "50/50 - 266s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 2/2\n",
      "50/50 - 258s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5')\n",
    "pruned_model = prune_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_lstm_20  (None, 16, 512)           2463747   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 16, 512)           1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_lstm_21  (None, 512)               4196355   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_20 (None, 256)               262402    \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_21 (None, 89)                45659     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_activati (None, 89)                1         \n",
      "=================================================================\n",
      "Total params: 6,968,168\n",
      "Trainable params: 3,486,297\n",
      "Non-trainable params: 3,481,871\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "50/50 - 263s - loss: 0.1608 - f1_score_mod: 0.3118 - recall_mod: 0.1958 - precision_mod: 0.7730 - dur_error: 1.7640 - maestro_dur_loss: 0.0882 - val_loss: 0.1628 - val_f1_score_mod: 0.3577 - val_recall_mod: 0.2316 - val_precision_mod: 0.7870 - val_dur_error: 1.7626 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 2/25\n",
      "50/50 - 258s - loss: 0.1516 - f1_score_mod: 0.3615 - recall_mod: 0.2359 - precision_mod: 0.7773 - dur_error: 1.7637 - maestro_dur_loss: 0.0882 - val_loss: 0.1613 - val_f1_score_mod: 0.3839 - val_recall_mod: 0.2568 - val_precision_mod: 0.7611 - val_dur_error: 1.7626 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 3/25\n",
      "50/50 - 258s - loss: 0.1572 - f1_score_mod: 0.3446 - recall_mod: 0.2246 - precision_mod: 0.7426 - dur_error: 1.7638 - maestro_dur_loss: 0.0882 - val_loss: 0.1632 - val_f1_score_mod: 0.3714 - val_recall_mod: 0.2459 - val_precision_mod: 0.7600 - val_dur_error: 1.7626 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 4/25\n",
      "50/50 - 259s - loss: 0.1511 - f1_score_mod: 0.3702 - recall_mod: 0.2449 - precision_mod: 0.7615 - dur_error: 1.7639 - maestro_dur_loss: 0.0882 - val_loss: 0.1619 - val_f1_score_mod: 0.3808 - val_recall_mod: 0.2539 - val_precision_mod: 0.7625 - val_dur_error: 1.7626 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 5/25\n",
      "50/50 - 257s - loss: 0.1621 - f1_score_mod: 0.3057 - recall_mod: 0.1931 - precision_mod: 0.7406 - dur_error: 1.7638 - maestro_dur_loss: 0.0882 - val_loss: 0.1639 - val_f1_score_mod: 0.3350 - val_recall_mod: 0.2130 - val_precision_mod: 0.7859 - val_dur_error: 1.7622 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 6/25\n",
      "50/50 - 258s - loss: 0.1538 - f1_score_mod: 0.3400 - recall_mod: 0.2190 - precision_mod: 0.7637 - dur_error: 1.7638 - maestro_dur_loss: 0.0882 - val_loss: 0.1627 - val_f1_score_mod: 0.3538 - val_recall_mod: 0.2288 - val_precision_mod: 0.7817 - val_dur_error: 1.7622 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 7/25\n",
      "50/50 - 257s - loss: 0.1650 - f1_score_mod: 0.2926 - recall_mod: 0.1830 - precision_mod: 0.7342 - dur_error: 1.7629 - maestro_dur_loss: 0.0881 - val_loss: 0.1655 - val_f1_score_mod: 0.3263 - val_recall_mod: 0.2063 - val_precision_mod: 0.7812 - val_dur_error: 1.7615 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 8/25\n",
      "50/50 - 250s - loss: 0.1568 - f1_score_mod: 0.3178 - recall_mod: 0.2012 - precision_mod: 0.7597 - dur_error: 1.7631 - maestro_dur_loss: 0.0882 - val_loss: 0.1637 - val_f1_score_mod: 0.3413 - val_recall_mod: 0.2181 - val_precision_mod: 0.7857 - val_dur_error: 1.7615 - val_maestro_dur_loss: 0.0881\n",
      "Epoch 9/25\n",
      "50/50 - 248s - loss: 0.1674 - f1_score_mod: 0.2627 - recall_mod: 0.1600 - precision_mod: 0.7392 - dur_error: 1.7624 - maestro_dur_loss: 0.0881 - val_loss: 0.1665 - val_f1_score_mod: 0.3076 - val_recall_mod: 0.1910 - val_precision_mod: 0.7901 - val_dur_error: 1.7608 - val_maestro_dur_loss: 0.0880\n",
      "Epoch 10/25\n",
      "50/50 - 248s - loss: 0.1587 - f1_score_mod: 0.2957 - recall_mod: 0.1838 - precision_mod: 0.7601 - dur_error: 1.7624 - maestro_dur_loss: 0.0881 - val_loss: 0.1647 - val_f1_score_mod: 0.3215 - val_recall_mod: 0.2018 - val_precision_mod: 0.7917 - val_dur_error: 1.7608 - val_maestro_dur_loss: 0.0880\n",
      "Epoch 11/25\n",
      "50/50 - 248s - loss: 0.1668 - f1_score_mod: 0.2502 - recall_mod: 0.1508 - precision_mod: 0.7397 - dur_error: 1.7615 - maestro_dur_loss: 0.0881 - val_loss: 0.1669 - val_f1_score_mod: 0.2834 - val_recall_mod: 0.1721 - val_precision_mod: 0.8054 - val_dur_error: 1.7601 - val_maestro_dur_loss: 0.0880\n",
      "Epoch 12/25\n",
      "50/50 - 247s - loss: 0.1603 - f1_score_mod: 0.2684 - recall_mod: 0.1634 - precision_mod: 0.7543 - dur_error: 1.7616 - maestro_dur_loss: 0.0881 - val_loss: 0.1657 - val_f1_score_mod: 0.3023 - val_recall_mod: 0.1871 - val_precision_mod: 0.7897 - val_dur_error: 1.7601 - val_maestro_dur_loss: 0.0880\n",
      "Epoch 13/25\n",
      "50/50 - 248s - loss: 0.1656 - f1_score_mod: 0.2352 - recall_mod: 0.1398 - precision_mod: 0.7452 - dur_error: 1.7613 - maestro_dur_loss: 0.0881 - val_loss: 0.1672 - val_f1_score_mod: 0.2750 - val_recall_mod: 0.1666 - val_precision_mod: 0.7901 - val_dur_error: 1.7597 - val_maestro_dur_loss: 0.0880\n",
      "Epoch 14/25\n",
      "50/50 - 248s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 15/25\n",
      "50/50 - 250s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 16/25\n",
      "50/50 - 248s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 17/25\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 18/25\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 19/25\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 20/25\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 21/25\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 22/25\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 23/25\n",
      "50/50 - 245s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25\n",
      "50/50 - 244s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n",
      "Epoch 25/25\n",
      "50/50 - 246s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_file('../models/best_maestro_model_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5')\n",
    "pruned_model = prune_model(model, epochs = 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
