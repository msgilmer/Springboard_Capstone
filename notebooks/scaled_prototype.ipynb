{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Up to handle a dataset of size 1 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sys import getsizeof\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from decimal import Decimal\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in transposed_chopin_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_chopin_sequences = np.load('../train_and_val/transposed_chopin_sequences.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_inputs(sequences, window_size = 16):\n",
    "    \"\"\"Apply a window function of size window_size across the dataset to create X. The next \n",
    "    vector in the sequence is appended to y for each window. Return X, y.\"\"\"\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(sequences)):\n",
    "        if (len(sequences[i]) < window_size + 1):\n",
    "            print(\"Skipping index \", i, \" because the song is too short. Try a shorter window_size to include it.\")\n",
    "            continue\n",
    "        for j in range(len(sequences[i]) - window_size):\n",
    "            X.append(sequences[i][j:j + window_size])\n",
    "            y.append(sequences[i][j + window_size])\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate them exactly as has been done in ./data_read_and_process.ipynb to verify that the checked statistics remain the same. This is meant to verify that transposed_chopin_sequences is unchanged by saving and loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sequences_to_inputs(transposed_chopin_sequences)\n",
    "X, y = shuffle(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Duration = 6.545454545454545\n"
     ]
    }
   ],
   "source": [
    "maximum_duration = max(X[:, :, -1].max(), y[:, -1].max())\n",
    "print('Maximum Duration = {}'.format(maximum_duration))\n",
    "# Keep variable so one can multiply the durations by this after music generation\n",
    "# to convert back into seconds\n",
    "X[:, :, -1] /= maximum_duration\n",
    "y[:, -1] /= maximum_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36153, 89)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36153, 16, 89)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size is 411.855104 MB\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size is {} MB'.format(getsizeof(X) / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Scaled Duration for X_train: 0.860215053763441\n",
      "Maximum Scaled Duration for X_val: 0.5516975308641976\n",
      "Maximum Scaled Duration for y_train: 1.0\n",
      "Maximum Scaled Duration for y_val: 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Maximum Scaled Duration for X_train: {}'.format(X_train[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for X_val: {}'.format(X_val[:, :, -1].max()))\n",
    "print('Maximum Scaled Duration for y_train: {}'.format(y_train[:, -1].max()))\n",
    "print('Maximum Scaled Duration for y_val: {}'.format(y_val[:, -1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Ratio of the Mean of the Scaled Duration:  0.9898257021922111\n",
      "Train-Validation Ratio of the Stdv of the Scaled Duration:  0.9209054376139967\n"
     ]
    }
   ],
   "source": [
    "print('Train-Validation Ratio of the Mean of the Scaled Duration: ', y_train[:, -1].mean() / y_val[:, -1].mean())\n",
    "print('Train-Validation Ratio of the Stdv of the Scaled Duration: ', y_train[:, -1].std() / y_val[:, -1].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, they are the same. Now, we want to scale to 1 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping index  24  because the song is too short. Try a shorter window_size to include it.\n"
     ]
    }
   ],
   "source": [
    "X, y = sequences_to_inputs(transposed_chopin_sequences, window_size = 42)\n",
    "X, y = shuffle(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Duration = 6.545454545454545\n"
     ]
    }
   ],
   "source": [
    "maximum_duration = max(X[:, :, -1].max(), y[:, -1].max())\n",
    "print('Maximum Duration = {}'.format(maximum_duration))\n",
    "# Keep variable so one can multiply the durations by this after music generation\n",
    "# to convert back into seconds\n",
    "X[:, :, -1] /= maximum_duration\n",
    "y[:, -1] /= maximum_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34260, 89)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34260, 42, 89)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size is 1024.511168 MB\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size is {} MB'.format(getsizeof(X) / 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below functions are the same ones used in ./model_training.ipynb which are necessary to train and save a model (along with performance data). The only difference is in the base filename in the function 'train_lstm_model' is changed from 'best_maestro_model' to 'best_maestro_model_scaled'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4):\n",
    "    \"\"\"Generate a keras Sequential model of the form as described in Figure 16 of\n",
    "    https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_lstm_nodes, return_sequences = True, input_shape = (16, 89,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(1, n_lstm_layers - 1):\n",
    "        model.add(LSTM(n_lstm_nodes, return_sequences = True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(n_lstm_nodes))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_lstm_nodes // 2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(n_dense_layers - 1):\n",
    "        model.add(Dense(n_lstm_nodes // 2))\n",
    "        model.add(Dropout(0.6))\n",
    "    model.add(Dense(89))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'RMSProp')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maestro_loss_wr(harshness): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last element (which repesents the duration). \n",
    "    The proportionality constant is the 'harshness' of the maestro in regards to\n",
    "    timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-1] * K.log(ypred[:, :-1]) + (1 - ytrue[:, :-1]) * \\\n",
    "                     K.log(1 - ypred[:, :-1]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, -1] elements will be zero\n",
    "            return bce_loss * 2     # This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "\n",
    "def precision_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified precision excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    pred_positives = K.sum(K.round(ypred[:, :-1]))\n",
    "    return true_positives / (pred_positives + K.epsilon())\n",
    "\n",
    "def recall_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified recall excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    poss_positives = K.sum(ytrue[:, :-1])\n",
    "    return true_positives / (poss_positives + K.epsilon())\n",
    "\n",
    "def f1_score_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified f1_score excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    precision = precision_mod(ytrue, ypred)\n",
    "    recall = recall_mod(ytrue, ypred)   \n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def dur_error(ytrue, ypred):\n",
    "    \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "    return 2 * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / (ytrue[:, -1] + ypred[:, -1] + \\\n",
    "                                                         K.epsilon())))\n",
    "\n",
    "def maestro_dur_loss_wr(harshness):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cols_dict(history):\n",
    "    \"\"\"return a mapping of desired column names to the corresponding columns in the\n",
    "    history dictionary (previously history.history where history is the return value\n",
    "    of model.train)\"\"\"\n",
    "    return {'maestro_loss': history['loss'], 'f1_score': history['f1_score_mod'], \\\n",
    " 'precision': history['precision_mod'], 'recall': history['recall_mod'], \\\n",
    " 'dur_error': history['dur_error'], 'dur_loss': history['maestro_dur_loss'], \\\n",
    " 'val_maestro_loss': history['val_loss'], 'val_f1_score': history['val_f1_score_mod'], \\\n",
    " 'val_precision': history['val_precision_mod'], 'val_recall': history['val_recall_mod'], \\\n",
    " 'val_dur_error': history['val_dur_error'], 'val_dur_loss': history['val_maestro_dur_loss']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4, \\\n",
    "                     batch_size = 512, harshness = 0.05, lr = None, clipnorm = None, clipvalue = None, \\\n",
    "                     epochs = 150):\n",
    "    \"\"\"Train a model using the passed parameters, the data, and using the RMSprop optimizer. Write the\n",
    "    best model as a .h5 and a .csv containing columns for the training and validation custom loss and\n",
    "    metrics. Returns nothing.\"\"\"\n",
    "    model = lstm(n_lstm_layers = n_lstm_layers, n_dense_layers = n_dense_layers, \\\n",
    "                 n_lstm_nodes = n_lstm_nodes, dropout_rate = dropout_rate)\n",
    "\n",
    "    if (lr or clipnorm or clipvalue):\n",
    "        if (lr):          # It's required that the first argument to RMSprop is not None\n",
    "            opt = RMSprop(lr = lr, clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        elif (clipnorm):\n",
    "            opt = RMSprop(clipnorm = clipnorm, clipvalue = clipvalue)\n",
    "        else: # clipvalue\n",
    "            opt = RMSprop(clipvalue = clipvalue)\n",
    "    else:\n",
    "        opt = RMSprop()   # TypeError when all are None, so do this instead\n",
    "        \n",
    "    model.compile(loss = maestro_loss_wr(harshness), optimizer = opt, metrics = [f1_score_mod, recall_mod, \\\n",
    "                                                precision_mod, dur_error, maestro_dur_loss_wr(harshness)])\n",
    "    \n",
    "    filename = 'best_maestro_model_scaled_{0}_{1}_{2}_{3}'.format(n_lstm_layers, n_dense_layers, n_lstm_nodes, \\\n",
    "                                                          str(dropout_rate).replace('.', 'pt'))\n",
    "    if (lr):\n",
    "        filename += '_lr_{}'.format('%.0e' % Decimal(lr))\n",
    "    if (clipnorm):\n",
    "        filename += '_cn_{}'.format(str(clipnorm).replace('.', 'pt'))     \n",
    "    if (clipvalue):\n",
    "        filename += '_cv_{}'.format(str(clipvalue).replace('.', 'pt'))\n",
    "                                   \n",
    "    mc = ModelCheckpoint('../models/' + filename + '.h5', monitor = 'val_loss', mode = 'min', \\\n",
    "                                                         save_best_only = True, verbose = 1)\n",
    "                                   \n",
    "    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "    \n",
    "    # In most preliminary tests model training has failed at some point when the loss becomes NaN during\n",
    "    # validation\n",
    "    if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "        for key, value in history.history.items():\n",
    "            if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "                value.append(np.nan)\n",
    "                \n",
    "    df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "    df.index.name = 'Epochs'\n",
    "    df.to_csv('../model_data/' + filename + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model on this scaled version of the dataset using the same hyperparameters that gave us our best model in ./model_training.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14477, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 150s - loss: 0.2414 - f1_score_mod: 0.0272 - recall_mod: 0.0392 - precision_mod: 0.0857 - dur_error: 1.0064 - maestro_dur_loss: 0.0503 - val_loss: 0.1448 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4532 - val_maestro_dur_loss: 0.0227\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14477 to 0.13937, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 165s - loss: 0.1663 - f1_score_mod: 0.0070 - recall_mod: 0.0036 - precision_mod: 0.2045 - dur_error: 0.7137 - maestro_dur_loss: 0.0357 - val_loss: 0.1394 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.4507 - val_maestro_dur_loss: 0.0225\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13937\n",
      "50/50 - 140s - loss: 0.1539 - f1_score_mod: 0.0159 - recall_mod: 0.0082 - precision_mod: 0.3746 - dur_error: 0.6443 - maestro_dur_loss: 0.0322 - val_loss: 0.1434 - val_f1_score_mod: 0.0190 - val_recall_mod: 0.0097 - val_precision_mod: 0.6865 - val_dur_error: 0.6118 - val_maestro_dur_loss: 0.0306\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13937 to 0.13106, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 145s - loss: 0.1468 - f1_score_mod: 0.0336 - recall_mod: 0.0175 - precision_mod: 0.5195 - dur_error: 0.5978 - maestro_dur_loss: 0.0299 - val_loss: 0.1311 - val_f1_score_mod: 0.0272 - val_recall_mod: 0.0138 - val_precision_mod: 0.8097 - val_dur_error: 0.4276 - val_maestro_dur_loss: 0.0214\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13106\n",
      "50/50 - 139s - loss: 0.1420 - f1_score_mod: 0.0556 - recall_mod: 0.0293 - precision_mod: 0.5814 - dur_error: 0.5741 - maestro_dur_loss: 0.0287 - val_loss: 0.1389 - val_f1_score_mod: 0.0533 - val_recall_mod: 0.0277 - val_precision_mod: 0.7822 - val_dur_error: 0.6516 - val_maestro_dur_loss: 0.0326\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13106 to 0.12592, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 152s - loss: 0.1386 - f1_score_mod: 0.0735 - recall_mod: 0.0393 - precision_mod: 0.5999 - dur_error: 0.5609 - maestro_dur_loss: 0.0280 - val_loss: 0.1259 - val_f1_score_mod: 0.0721 - val_recall_mod: 0.0379 - val_precision_mod: 0.7755 - val_dur_error: 0.4233 - val_maestro_dur_loss: 0.0212\n",
      "Epoch 7/150\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12592 to 0.12357, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 147s - loss: 0.1359 - f1_score_mod: 0.0897 - recall_mod: 0.0485 - precision_mod: 0.6223 - dur_error: 0.5490 - maestro_dur_loss: 0.0274 - val_loss: 0.1236 - val_f1_score_mod: 0.0893 - val_recall_mod: 0.0475 - val_precision_mod: 0.7435 - val_dur_error: 0.4024 - val_maestro_dur_loss: 0.0201\n",
      "Epoch 8/150\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12357\n",
      "50/50 - 152s - loss: 0.1333 - f1_score_mod: 0.1028 - recall_mod: 0.0561 - precision_mod: 0.6398 - dur_error: 0.5374 - maestro_dur_loss: 0.0269 - val_loss: 0.1262 - val_f1_score_mod: 0.1383 - val_recall_mod: 0.0769 - val_precision_mod: 0.6946 - val_dur_error: 0.4977 - val_maestro_dur_loss: 0.0249\n",
      "Epoch 9/150\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12357\n",
      "50/50 - 147s - loss: 0.1315 - f1_score_mod: 0.1190 - recall_mod: 0.0658 - precision_mod: 0.6388 - dur_error: 0.5296 - maestro_dur_loss: 0.0265 - val_loss: 0.1283 - val_f1_score_mod: 0.1427 - val_recall_mod: 0.0795 - val_precision_mod: 0.7047 - val_dur_error: 0.5371 - val_maestro_dur_loss: 0.0269\n",
      "Epoch 10/150\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12357 to 0.12323, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 155s - loss: 0.1297 - f1_score_mod: 0.1324 - recall_mod: 0.0738 - precision_mod: 0.6591 - dur_error: 0.5217 - maestro_dur_loss: 0.0261 - val_loss: 0.1232 - val_f1_score_mod: 0.1632 - val_recall_mod: 0.0920 - val_precision_mod: 0.7345 - val_dur_error: 0.4810 - val_maestro_dur_loss: 0.0240\n",
      "Epoch 11/150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12323 to 0.12128, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 151s - loss: 0.1277 - f1_score_mod: 0.1460 - recall_mod: 0.0822 - precision_mod: 0.6718 - dur_error: 0.5061 - maestro_dur_loss: 0.0253 - val_loss: 0.1213 - val_f1_score_mod: 0.1631 - val_recall_mod: 0.0915 - val_precision_mod: 0.7585 - val_dur_error: 0.4696 - val_maestro_dur_loss: 0.0235\n",
      "Epoch 12/150\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.12128 to 0.11954, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 142s - loss: 0.1262 - f1_score_mod: 0.1549 - recall_mod: 0.0877 - precision_mod: 0.6697 - dur_error: 0.4965 - maestro_dur_loss: 0.0248 - val_loss: 0.1195 - val_f1_score_mod: 0.1848 - val_recall_mod: 0.1059 - val_precision_mod: 0.7295 - val_dur_error: 0.4445 - val_maestro_dur_loss: 0.0222\n",
      "Epoch 13/150\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11954 to 0.11614, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 142s - loss: 0.1248 - f1_score_mod: 0.1661 - recall_mod: 0.0950 - precision_mod: 0.6775 - dur_error: 0.4849 - maestro_dur_loss: 0.0242 - val_loss: 0.1161 - val_f1_score_mod: 0.2078 - val_recall_mod: 0.1217 - val_precision_mod: 0.7156 - val_dur_error: 0.3951 - val_maestro_dur_loss: 0.0198\n",
      "Epoch 14/150\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11614 to 0.11264, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 141s - loss: 0.1235 - f1_score_mod: 0.1738 - recall_mod: 0.0998 - precision_mod: 0.6874 - dur_error: 0.4751 - maestro_dur_loss: 0.0238 - val_loss: 0.1126 - val_f1_score_mod: 0.1913 - val_recall_mod: 0.1099 - val_precision_mod: 0.7454 - val_dur_error: 0.3374 - val_maestro_dur_loss: 0.0169\n",
      "Epoch 15/150\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11264 to 0.11036, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 153s - loss: 0.1222 - f1_score_mod: 0.1833 - recall_mod: 0.1062 - precision_mod: 0.6847 - dur_error: 0.4657 - maestro_dur_loss: 0.0233 - val_loss: 0.1104 - val_f1_score_mod: 0.1983 - val_recall_mod: 0.1142 - val_precision_mod: 0.7577 - val_dur_error: 0.3035 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 16/150\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11036 to 0.11027, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 152s - loss: 0.1210 - f1_score_mod: 0.1892 - recall_mod: 0.1100 - precision_mod: 0.6882 - dur_error: 0.4542 - maestro_dur_loss: 0.0227 - val_loss: 0.1103 - val_f1_score_mod: 0.2122 - val_recall_mod: 0.1236 - val_precision_mod: 0.7551 - val_dur_error: 0.3196 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 17/150\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11027 to 0.10861, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 157s - loss: 0.1203 - f1_score_mod: 0.1959 - recall_mod: 0.1145 - precision_mod: 0.6863 - dur_error: 0.4533 - maestro_dur_loss: 0.0227 - val_loss: 0.1086 - val_f1_score_mod: 0.2107 - val_recall_mod: 0.1228 - val_precision_mod: 0.7486 - val_dur_error: 0.2883 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 18/150\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10861\n",
      "50/50 - 166s - loss: 0.1192 - f1_score_mod: 0.2030 - recall_mod: 0.1194 - precision_mod: 0.6911 - dur_error: 0.4458 - maestro_dur_loss: 0.0223 - val_loss: 0.1142 - val_f1_score_mod: 0.2172 - val_recall_mod: 0.1267 - val_precision_mod: 0.7679 - val_dur_error: 0.4114 - val_maestro_dur_loss: 0.0206\n",
      "Epoch 19/150\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10861\n",
      "50/50 - 152s - loss: 0.1185 - f1_score_mod: 0.2104 - recall_mod: 0.1243 - precision_mod: 0.6954 - dur_error: 0.4399 - maestro_dur_loss: 0.0220 - val_loss: 0.1110 - val_f1_score_mod: 0.2179 - val_recall_mod: 0.1266 - val_precision_mod: 0.7897 - val_dur_error: 0.3620 - val_maestro_dur_loss: 0.0181\n",
      "Epoch 20/150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10861\n",
      "50/50 - 163s - loss: 0.1175 - f1_score_mod: 0.2183 - recall_mod: 0.1295 - precision_mod: 0.7036 - dur_error: 0.4355 - maestro_dur_loss: 0.0218 - val_loss: 0.1121 - val_f1_score_mod: 0.2347 - val_recall_mod: 0.1394 - val_precision_mod: 0.7491 - val_dur_error: 0.3918 - val_maestro_dur_loss: 0.0196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10861 to 0.10718, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 160s - loss: 0.1169 - f1_score_mod: 0.2199 - recall_mod: 0.1310 - precision_mod: 0.7028 - dur_error: 0.4329 - maestro_dur_loss: 0.0216 - val_loss: 0.1072 - val_f1_score_mod: 0.2444 - val_recall_mod: 0.1459 - val_precision_mod: 0.7559 - val_dur_error: 0.3046 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 22/150\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10718\n",
      "50/50 - 157s - loss: 0.1160 - f1_score_mod: 0.2316 - recall_mod: 0.1388 - precision_mod: 0.7085 - dur_error: 0.4288 - maestro_dur_loss: 0.0214 - val_loss: 0.1073 - val_f1_score_mod: 0.2527 - val_recall_mod: 0.1517 - val_precision_mod: 0.7608 - val_dur_error: 0.3091 - val_maestro_dur_loss: 0.0155\n",
      "Epoch 23/150\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10718\n",
      "50/50 - 158s - loss: 0.1154 - f1_score_mod: 0.2370 - recall_mod: 0.1426 - precision_mod: 0.7095 - dur_error: 0.4273 - maestro_dur_loss: 0.0214 - val_loss: 0.1145 - val_f1_score_mod: 0.2705 - val_recall_mod: 0.1659 - val_precision_mod: 0.7339 - val_dur_error: 0.4609 - val_maestro_dur_loss: 0.0230\n",
      "Epoch 24/150\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10718\n",
      "50/50 - 150s - loss: 0.1146 - f1_score_mod: 0.2419 - recall_mod: 0.1462 - precision_mod: 0.7093 - dur_error: 0.4217 - maestro_dur_loss: 0.0211 - val_loss: 0.1149 - val_f1_score_mod: 0.2683 - val_recall_mod: 0.1638 - val_precision_mod: 0.7442 - val_dur_error: 0.4691 - val_maestro_dur_loss: 0.0235\n",
      "Epoch 25/150\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10718\n",
      "50/50 - 154s - loss: 0.1142 - f1_score_mod: 0.2514 - recall_mod: 0.1530 - precision_mod: 0.7138 - dur_error: 0.4216 - maestro_dur_loss: 0.0211 - val_loss: 0.1096 - val_f1_score_mod: 0.2581 - val_recall_mod: 0.1550 - val_precision_mod: 0.7758 - val_dur_error: 0.3789 - val_maestro_dur_loss: 0.0189\n",
      "Epoch 26/150\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10718 to 0.10438, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 157s - loss: 0.1135 - f1_score_mod: 0.2543 - recall_mod: 0.1549 - precision_mod: 0.7185 - dur_error: 0.4175 - maestro_dur_loss: 0.0209 - val_loss: 0.1044 - val_f1_score_mod: 0.2698 - val_recall_mod: 0.1643 - val_precision_mod: 0.7572 - val_dur_error: 0.2806 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 27/150\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10438\n",
      "50/50 - 154s - loss: 0.1129 - f1_score_mod: 0.2568 - recall_mod: 0.1569 - precision_mod: 0.7175 - dur_error: 0.4167 - maestro_dur_loss: 0.0208 - val_loss: 0.1109 - val_f1_score_mod: 0.2963 - val_recall_mod: 0.1866 - val_precision_mod: 0.7215 - val_dur_error: 0.4175 - val_maestro_dur_loss: 0.0209\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10438 to 0.10372, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 237s - loss: 0.1122 - f1_score_mod: 0.2631 - recall_mod: 0.1617 - precision_mod: 0.7133 - dur_error: 0.4119 - maestro_dur_loss: 0.0206 - val_loss: 0.1037 - val_f1_score_mod: 0.2822 - val_recall_mod: 0.1737 - val_precision_mod: 0.7562 - val_dur_error: 0.2871 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 29/150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10372\n",
      "50/50 - 280s - loss: 0.1116 - f1_score_mod: 0.2689 - recall_mod: 0.1658 - precision_mod: 0.7225 - dur_error: 0.4095 - maestro_dur_loss: 0.0205 - val_loss: 0.1038 - val_f1_score_mod: 0.2738 - val_recall_mod: 0.1665 - val_precision_mod: 0.7725 - val_dur_error: 0.2874 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 30/150\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10372 to 0.10257, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 199s - loss: 0.1109 - f1_score_mod: 0.2755 - recall_mod: 0.1705 - precision_mod: 0.7253 - dur_error: 0.4073 - maestro_dur_loss: 0.0204 - val_loss: 0.1026 - val_f1_score_mod: 0.2937 - val_recall_mod: 0.1827 - val_precision_mod: 0.7511 - val_dur_error: 0.2801 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 31/150\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10257\n",
      "50/50 - 132s - loss: 0.1107 - f1_score_mod: 0.2776 - recall_mod: 0.1722 - precision_mod: 0.7246 - dur_error: 0.4097 - maestro_dur_loss: 0.0205 - val_loss: 0.1078 - val_f1_score_mod: 0.3104 - val_recall_mod: 0.1979 - val_precision_mod: 0.7228 - val_dur_error: 0.3723 - val_maestro_dur_loss: 0.0186\n",
      "Epoch 32/150\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10257 to 0.10254, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 136s - loss: 0.1102 - f1_score_mod: 0.2827 - recall_mod: 0.1759 - precision_mod: 0.7254 - dur_error: 0.4045 - maestro_dur_loss: 0.0202 - val_loss: 0.1025 - val_f1_score_mod: 0.2995 - val_recall_mod: 0.1867 - val_precision_mod: 0.7610 - val_dur_error: 0.2859 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 33/150\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10254\n",
      "50/50 - 135s - loss: 0.1093 - f1_score_mod: 0.2870 - recall_mod: 0.1794 - precision_mod: 0.7226 - dur_error: 0.4001 - maestro_dur_loss: 0.0200 - val_loss: 0.1040 - val_f1_score_mod: 0.3155 - val_recall_mod: 0.2010 - val_precision_mod: 0.7346 - val_dur_error: 0.3194 - val_maestro_dur_loss: 0.0160\n",
      "Epoch 34/150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10254\n",
      "50/50 - 137s - loss: 0.1089 - f1_score_mod: 0.2922 - recall_mod: 0.1832 - precision_mod: 0.7260 - dur_error: 0.4014 - maestro_dur_loss: 0.0201 - val_loss: 0.1046 - val_f1_score_mod: 0.3040 - val_recall_mod: 0.1898 - val_precision_mod: 0.7668 - val_dur_error: 0.3437 - val_maestro_dur_loss: 0.0172\n",
      "Epoch 35/150\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.10254 to 0.10089, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 138s - loss: 0.1084 - f1_score_mod: 0.2972 - recall_mod: 0.1871 - precision_mod: 0.7277 - dur_error: 0.3983 - maestro_dur_loss: 0.0199 - val_loss: 0.1009 - val_f1_score_mod: 0.3061 - val_recall_mod: 0.1917 - val_precision_mod: 0.7626 - val_dur_error: 0.2773 - val_maestro_dur_loss: 0.0139\n",
      "Epoch 36/150\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10089\n",
      "50/50 - 138s - loss: 0.1077 - f1_score_mod: 0.2974 - recall_mod: 0.1873 - precision_mod: 0.7287 - dur_error: 0.3938 - maestro_dur_loss: 0.0197 - val_loss: 0.1010 - val_f1_score_mod: 0.3088 - val_recall_mod: 0.1936 - val_precision_mod: 0.7662 - val_dur_error: 0.2827 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 37/150\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10089\n",
      "50/50 - 133s - loss: 0.1073 - f1_score_mod: 0.3063 - recall_mod: 0.1940 - precision_mod: 0.7330 - dur_error: 0.3951 - maestro_dur_loss: 0.0198 - val_loss: 0.1024 - val_f1_score_mod: 0.3283 - val_recall_mod: 0.2119 - val_precision_mod: 0.7304 - val_dur_error: 0.3079 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10089 to 0.10027, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 135s - loss: 0.1068 - f1_score_mod: 0.3102 - recall_mod: 0.1976 - precision_mod: 0.7280 - dur_error: 0.3935 - maestro_dur_loss: 0.0197 - val_loss: 0.1003 - val_f1_score_mod: 0.3323 - val_recall_mod: 0.2149 - val_precision_mod: 0.7346 - val_dur_error: 0.2752 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 39/150\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.10027\n",
      "50/50 - 217s - loss: 0.1062 - f1_score_mod: 0.3146 - recall_mod: 0.2008 - precision_mod: 0.7328 - dur_error: 0.3887 - maestro_dur_loss: 0.0194 - val_loss: 0.1005 - val_f1_score_mod: 0.3367 - val_recall_mod: 0.2171 - val_precision_mod: 0.7519 - val_dur_error: 0.2900 - val_maestro_dur_loss: 0.0145\n",
      "Epoch 40/150\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10027\n",
      "50/50 - 251s - loss: 0.1058 - f1_score_mod: 0.3197 - recall_mod: 0.2049 - precision_mod: 0.7324 - dur_error: 0.3886 - maestro_dur_loss: 0.0194 - val_loss: 0.1035 - val_f1_score_mod: 0.3358 - val_recall_mod: 0.2170 - val_precision_mod: 0.7442 - val_dur_error: 0.3461 - val_maestro_dur_loss: 0.0173\n",
      "Epoch 41/150\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10027\n",
      "50/50 - 196s - loss: 0.1052 - f1_score_mod: 0.3254 - recall_mod: 0.2092 - precision_mod: 0.7365 - dur_error: 0.3878 - maestro_dur_loss: 0.0194 - val_loss: 0.1059 - val_f1_score_mod: 0.3408 - val_recall_mod: 0.2210 - val_precision_mod: 0.7466 - val_dur_error: 0.3987 - val_maestro_dur_loss: 0.0199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.10027 to 0.09955, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 134s - loss: 0.1047 - f1_score_mod: 0.3306 - recall_mod: 0.2136 - precision_mod: 0.7356 - dur_error: 0.3868 - maestro_dur_loss: 0.0193 - val_loss: 0.0996 - val_f1_score_mod: 0.3446 - val_recall_mod: 0.2235 - val_precision_mod: 0.7540 - val_dur_error: 0.2837 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 43/150\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09955\n",
      "50/50 - 143s - loss: 0.1042 - f1_score_mod: 0.3311 - recall_mod: 0.2139 - precision_mod: 0.7375 - dur_error: 0.3854 - maestro_dur_loss: 0.0193 - val_loss: 0.1005 - val_f1_score_mod: 0.3497 - val_recall_mod: 0.2291 - val_precision_mod: 0.7407 - val_dur_error: 0.3074 - val_maestro_dur_loss: 0.0154\n",
      "Epoch 44/150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09955\n",
      "50/50 - 138s - loss: 0.1037 - f1_score_mod: 0.3353 - recall_mod: 0.2174 - precision_mod: 0.7371 - dur_error: 0.3826 - maestro_dur_loss: 0.0191 - val_loss: 0.1000 - val_f1_score_mod: 0.3536 - val_recall_mod: 0.2332 - val_precision_mod: 0.7330 - val_dur_error: 0.2948 - val_maestro_dur_loss: 0.0147\n",
      "Epoch 45/150\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09955 to 0.09783, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 148s - loss: 0.1030 - f1_score_mod: 0.3432 - recall_mod: 0.2236 - precision_mod: 0.7425 - dur_error: 0.3809 - maestro_dur_loss: 0.0190 - val_loss: 0.0978 - val_f1_score_mod: 0.3613 - val_recall_mod: 0.2385 - val_precision_mod: 0.7460 - val_dur_error: 0.2658 - val_maestro_dur_loss: 0.0133\n",
      "Epoch 46/150\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09783\n",
      "50/50 - 168s - loss: 0.1026 - f1_score_mod: 0.3475 - recall_mod: 0.2276 - precision_mod: 0.7377 - dur_error: 0.3816 - maestro_dur_loss: 0.0191 - val_loss: 0.0981 - val_f1_score_mod: 0.3556 - val_recall_mod: 0.2327 - val_precision_mod: 0.7556 - val_dur_error: 0.2757 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 47/150\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09783\n",
      "50/50 - 168s - loss: 0.1020 - f1_score_mod: 0.3525 - recall_mod: 0.2313 - precision_mod: 0.7442 - dur_error: 0.3802 - maestro_dur_loss: 0.0190 - val_loss: 0.1011 - val_f1_score_mod: 0.3700 - val_recall_mod: 0.2472 - val_precision_mod: 0.7370 - val_dur_error: 0.3393 - val_maestro_dur_loss: 0.0170\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09783\n",
      "50/50 - 240s - loss: 0.1017 - f1_score_mod: 0.3571 - recall_mod: 0.2359 - precision_mod: 0.7384 - dur_error: 0.3806 - maestro_dur_loss: 0.0190 - val_loss: 0.0980 - val_f1_score_mod: 0.3615 - val_recall_mod: 0.2386 - val_precision_mod: 0.7475 - val_dur_error: 0.2815 - val_maestro_dur_loss: 0.0141\n",
      "Epoch 49/150\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09783\n",
      "50/50 - 255s - loss: 0.1009 - f1_score_mod: 0.3621 - recall_mod: 0.2399 - precision_mod: 0.7412 - dur_error: 0.3749 - maestro_dur_loss: 0.0187 - val_loss: 0.0980 - val_f1_score_mod: 0.3675 - val_recall_mod: 0.2433 - val_precision_mod: 0.7521 - val_dur_error: 0.2916 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 50/150\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09783\n",
      "50/50 - 236s - loss: 0.1006 - f1_score_mod: 0.3662 - recall_mod: 0.2432 - precision_mod: 0.7448 - dur_error: 0.3761 - maestro_dur_loss: 0.0188 - val_loss: 0.0999 - val_f1_score_mod: 0.3671 - val_recall_mod: 0.2431 - val_precision_mod: 0.7511 - val_dur_error: 0.3299 - val_maestro_dur_loss: 0.0165\n",
      "Epoch 51/150\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09783 to 0.09621, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 257s - loss: 0.0999 - f1_score_mod: 0.3666 - recall_mod: 0.2435 - precision_mod: 0.7438 - dur_error: 0.3732 - maestro_dur_loss: 0.0187 - val_loss: 0.0962 - val_f1_score_mod: 0.3777 - val_recall_mod: 0.2536 - val_precision_mod: 0.7416 - val_dur_error: 0.2566 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 52/150\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09621 to 0.09599, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 257s - loss: 0.0994 - f1_score_mod: 0.3684 - recall_mod: 0.2456 - precision_mod: 0.7404 - dur_error: 0.3728 - maestro_dur_loss: 0.0186 - val_loss: 0.0960 - val_f1_score_mod: 0.3769 - val_recall_mod: 0.2524 - val_precision_mod: 0.7450 - val_dur_error: 0.2621 - val_maestro_dur_loss: 0.0131\n",
      "Epoch 53/150\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09599 to 0.09599, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 259s - loss: 0.0990 - f1_score_mod: 0.3740 - recall_mod: 0.2502 - precision_mod: 0.7435 - dur_error: 0.3712 - maestro_dur_loss: 0.0186 - val_loss: 0.0960 - val_f1_score_mod: 0.3830 - val_recall_mod: 0.2572 - val_precision_mod: 0.7522 - val_dur_error: 0.2631 - val_maestro_dur_loss: 0.0132\n",
      "Epoch 54/150\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09599 to 0.09579, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 257s - loss: 0.0984 - f1_score_mod: 0.3787 - recall_mod: 0.2543 - precision_mod: 0.7447 - dur_error: 0.3669 - maestro_dur_loss: 0.0183 - val_loss: 0.0958 - val_f1_score_mod: 0.3858 - val_recall_mod: 0.2608 - val_precision_mod: 0.7425 - val_dur_error: 0.2606 - val_maestro_dur_loss: 0.0130\n",
      "Epoch 55/150\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09579\n",
      "50/50 - 256s - loss: 0.0978 - f1_score_mod: 0.3826 - recall_mod: 0.2581 - precision_mod: 0.7425 - dur_error: 0.3646 - maestro_dur_loss: 0.0182 - val_loss: 0.0969 - val_f1_score_mod: 0.3741 - val_recall_mod: 0.2470 - val_precision_mod: 0.7725 - val_dur_error: 0.2763 - val_maestro_dur_loss: 0.0138\n",
      "Epoch 56/150\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09579 to 0.09525, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 258s - loss: 0.0977 - f1_score_mod: 0.3840 - recall_mod: 0.2598 - precision_mod: 0.7393 - dur_error: 0.3688 - maestro_dur_loss: 0.0184 - val_loss: 0.0952 - val_f1_score_mod: 0.3916 - val_recall_mod: 0.2664 - val_precision_mod: 0.7401 - val_dur_error: 0.2566 - val_maestro_dur_loss: 0.0128\n",
      "Epoch 57/150\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.09525\n",
      "50/50 - 254s - loss: 0.0971 - f1_score_mod: 0.3898 - recall_mod: 0.2641 - precision_mod: 0.7473 - dur_error: 0.3680 - maestro_dur_loss: 0.0184 - val_loss: 0.0968 - val_f1_score_mod: 0.3971 - val_recall_mod: 0.2729 - val_precision_mod: 0.7298 - val_dur_error: 0.2874 - val_maestro_dur_loss: 0.0144\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.09525\n",
      "50/50 - 248s - loss: 0.0965 - f1_score_mod: 0.3943 - recall_mod: 0.2681 - precision_mod: 0.7480 - dur_error: 0.3644 - maestro_dur_loss: 0.0182 - val_loss: 0.0965 - val_f1_score_mod: 0.3911 - val_recall_mod: 0.2650 - val_precision_mod: 0.7475 - val_dur_error: 0.2918 - val_maestro_dur_loss: 0.0146\n",
      "Epoch 59/150\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09525\n",
      "50/50 - 248s - loss: 0.0958 - f1_score_mod: 0.4000 - recall_mod: 0.2740 - precision_mod: 0.7444 - dur_error: 0.3629 - maestro_dur_loss: 0.0181 - val_loss: 0.0971 - val_f1_score_mod: 0.4001 - val_recall_mod: 0.2749 - val_precision_mod: 0.7361 - val_dur_error: 0.3015 - val_maestro_dur_loss: 0.0151\n",
      "Epoch 60/150\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09525\n",
      "50/50 - 247s - loss: 0.0956 - f1_score_mod: 0.4013 - recall_mod: 0.2755 - precision_mod: 0.7410 - dur_error: 0.3629 - maestro_dur_loss: 0.0181 - val_loss: 0.0960 - val_f1_score_mod: 0.3929 - val_recall_mod: 0.2666 - val_precision_mod: 0.7474 - val_dur_error: 0.2837 - val_maestro_dur_loss: 0.0142\n",
      "Epoch 61/150\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.09525\n",
      "50/50 - 247s - loss: 0.0948 - f1_score_mod: 0.4064 - recall_mod: 0.2798 - precision_mod: 0.7445 - dur_error: 0.3615 - maestro_dur_loss: 0.0181 - val_loss: 0.0972 - val_f1_score_mod: 0.4028 - val_recall_mod: 0.2766 - val_precision_mod: 0.7426 - val_dur_error: 0.3219 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 62/150\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09525 to 0.09458, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 248s - loss: 0.0943 - f1_score_mod: 0.4109 - recall_mod: 0.2841 - precision_mod: 0.7463 - dur_error: 0.3594 - maestro_dur_loss: 0.0180 - val_loss: 0.0946 - val_f1_score_mod: 0.4039 - val_recall_mod: 0.2782 - val_precision_mod: 0.7386 - val_dur_error: 0.2656 - val_maestro_dur_loss: 0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.09458\n",
      "50/50 - 248s - loss: 0.0938 - f1_score_mod: 0.4152 - recall_mod: 0.2879 - precision_mod: 0.7471 - dur_error: 0.3576 - maestro_dur_loss: 0.0179 - val_loss: 0.0956 - val_f1_score_mod: 0.4171 - val_recall_mod: 0.2935 - val_precision_mod: 0.7217 - val_dur_error: 0.2868 - val_maestro_dur_loss: 0.0143\n",
      "Epoch 64/150\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.09458\n",
      "50/50 - 248s - loss: 0.0934 - f1_score_mod: 0.4193 - recall_mod: 0.2915 - precision_mod: 0.7485 - dur_error: 0.3591 - maestro_dur_loss: 0.0180 - val_loss: 0.0996 - val_f1_score_mod: 0.4179 - val_recall_mod: 0.2967 - val_precision_mod: 0.7077 - val_dur_error: 0.3675 - val_maestro_dur_loss: 0.0184\n",
      "Epoch 65/150\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09458 to 0.09402, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 250s - loss: 0.0928 - f1_score_mod: 0.4242 - recall_mod: 0.2973 - precision_mod: 0.7428 - dur_error: 0.3557 - maestro_dur_loss: 0.0178 - val_loss: 0.0940 - val_f1_score_mod: 0.4203 - val_recall_mod: 0.2985 - val_precision_mod: 0.7110 - val_dur_error: 0.2533 - val_maestro_dur_loss: 0.0127\n",
      "Epoch 66/150\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09402 to 0.09366, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 246s - loss: 0.0922 - f1_score_mod: 0.4291 - recall_mod: 0.3013 - precision_mod: 0.7465 - dur_error: 0.3541 - maestro_dur_loss: 0.0177 - val_loss: 0.0937 - val_f1_score_mod: 0.4152 - val_recall_mod: 0.2903 - val_precision_mod: 0.7298 - val_dur_error: 0.2579 - val_maestro_dur_loss: 0.0129\n",
      "Epoch 67/150\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09366\n",
      "50/50 - 245s - loss: 0.0918 - f1_score_mod: 0.4295 - recall_mod: 0.3019 - precision_mod: 0.7457 - dur_error: 0.3531 - maestro_dur_loss: 0.0177 - val_loss: 0.0958 - val_f1_score_mod: 0.4202 - val_recall_mod: 0.2971 - val_precision_mod: 0.7184 - val_dur_error: 0.2985 - val_maestro_dur_loss: 0.0149\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09366\n",
      "50/50 - 246s - loss: 0.0911 - f1_score_mod: 0.4388 - recall_mod: 0.3107 - precision_mod: 0.7494 - dur_error: 0.3527 - maestro_dur_loss: 0.0176 - val_loss: 0.0945 - val_f1_score_mod: 0.4262 - val_recall_mod: 0.3032 - val_precision_mod: 0.7185 - val_dur_error: 0.2797 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 69/150\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09366\n",
      "50/50 - 246s - loss: 0.0904 - f1_score_mod: 0.4413 - recall_mod: 0.3138 - precision_mod: 0.7448 - dur_error: 0.3497 - maestro_dur_loss: 0.0175 - val_loss: 0.0945 - val_f1_score_mod: 0.4217 - val_recall_mod: 0.2966 - val_precision_mod: 0.7303 - val_dur_error: 0.2808 - val_maestro_dur_loss: 0.0140\n",
      "Epoch 70/150\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09366\n",
      "50/50 - 246s - loss: 0.0901 - f1_score_mod: 0.4472 - recall_mod: 0.3189 - precision_mod: 0.7503 - dur_error: 0.3496 - maestro_dur_loss: 0.0175 - val_loss: 0.0960 - val_f1_score_mod: 0.4266 - val_recall_mod: 0.3014 - val_precision_mod: 0.7306 - val_dur_error: 0.3110 - val_maestro_dur_loss: 0.0156\n",
      "Epoch 71/150\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09366\n",
      "50/50 - 245s - loss: 0.0894 - f1_score_mod: 0.4489 - recall_mod: 0.3208 - precision_mod: 0.7498 - dur_error: 0.3478 - maestro_dur_loss: 0.0174 - val_loss: 0.0953 - val_f1_score_mod: 0.4325 - val_recall_mod: 0.3115 - val_precision_mod: 0.7084 - val_dur_error: 0.3006 - val_maestro_dur_loss: 0.0150\n",
      "Epoch 72/150\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09366\n",
      "50/50 - 245s - loss: 0.0889 - f1_score_mod: 0.4561 - recall_mod: 0.3279 - precision_mod: 0.7501 - dur_error: 0.3473 - maestro_dur_loss: 0.0174 - val_loss: 0.0960 - val_f1_score_mod: 0.4367 - val_recall_mod: 0.3167 - val_precision_mod: 0.7045 - val_dur_error: 0.3223 - val_maestro_dur_loss: 0.0161\n",
      "Epoch 73/150\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09366\n",
      "50/50 - 245s - loss: 0.0883 - f1_score_mod: 0.4623 - recall_mod: 0.3343 - precision_mod: 0.7511 - dur_error: 0.3463 - maestro_dur_loss: 0.0173 - val_loss: 0.0951 - val_f1_score_mod: 0.4333 - val_recall_mod: 0.3085 - val_precision_mod: 0.7283 - val_dur_error: 0.3034 - val_maestro_dur_loss: 0.0152\n",
      "Epoch 74/150\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.09366\n",
      "50/50 - 245s - loss: 0.0880 - f1_score_mod: 0.4657 - recall_mod: 0.3386 - precision_mod: 0.7490 - dur_error: 0.3471 - maestro_dur_loss: 0.0174 - val_loss: 0.0956 - val_f1_score_mod: 0.4365 - val_recall_mod: 0.3124 - val_precision_mod: 0.7255 - val_dur_error: 0.3134 - val_maestro_dur_loss: 0.0157\n",
      "Epoch 75/150\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09366 to 0.09256, saving model to ../models/best_maestro_model_scaled_2_1_512_0pt4_lr_5e-04_cv_0pt2.h5\n",
      "50/50 - 199s - loss: 0.0871 - f1_score_mod: 0.4703 - recall_mod: 0.3431 - precision_mod: 0.7492 - dur_error: 0.3418 - maestro_dur_loss: 0.0171 - val_loss: 0.0926 - val_f1_score_mod: 0.4423 - val_recall_mod: 0.3222 - val_precision_mod: 0.7060 - val_dur_error: 0.2511 - val_maestro_dur_loss: 0.0126\n",
      "Epoch 76/150\n",
      "Batch 6: Invalid loss, terminating training\n",
      "Batch 7: Invalid loss, terminating training\n",
      "Batch 8: Invalid loss, terminating training\n",
      "Batch 9: Invalid loss, terminating training\n",
      "Batch 10: Invalid loss, terminating training\n",
      "Batch 11: Invalid loss, terminating training\n",
      "Batch 12: Invalid loss, terminating training\n",
      "Batch 13: Invalid loss, terminating training\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "Batch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09256\n",
      "50/50 - 170s - loss: nan - f1_score_mod: nan - recall_mod: nan - precision_mod: nan - dur_error: nan - maestro_dur_loss: nan - val_loss: nan - val_f1_score_mod: nan - val_recall_mod: nan - val_precision_mod: nan - val_dur_error: nan - val_maestro_dur_loss: nan\n"
     ]
    }
   ],
   "source": [
    "train_lstm_model(lr = 0.0005, clipvalue = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
