{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, key, tempo, duration, stream\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir, path\n",
    "from sys import maxsize, getsizeof\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation, LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to read MIDI files\n",
    "def read_midi(file, time_tol = 1.e-3):\n",
    "    \"\"\"This function reads a midi file for the notes, offsets, and durations for each piano part, separately.\n",
    "    The data needs to be further processed because there are many notes occurring at identical offsets (meaning \n",
    "    they start at the same time) but have different durations. I convert this sequence into one in which there is\n",
    "    no overlap, i.e. any time there is a change in the state of the piano keyboard, a new element is created \n",
    "    describing this state along with a duration for that state. This format will be much easier to use for encoding \n",
    "    the data into the neural network input later on. These sequences, separated by part, are  returned along with \n",
    "    the musical key of the song (we need this because we will eventually transpose everything to the key of C.)\"\"\"\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    notes_by_part = []          # MIDI files can contain multiple parts. I will focus on piano, but even here\n",
    "                                # there may be different files for the right hand and left hand, for example\n",
    "\n",
    "    midi = converter.parse(file)\n",
    "    parts = instrument.partitionByInstrument(midi)  # will extract the parsed data all instrument instructions\n",
    "                                                    # separately\n",
    "    keys_by_song = []           # An array, to be returned, containing elements of keys (below) for each song\n",
    "    notes_by_part = {}          # A dict for storing the processes sequences by part (the keys of the dict are\n",
    "                                # the part names)\n",
    "\n",
    "    for part in parts:\n",
    "        notes_to_parse = part.recurse()\n",
    "        notes = []        # This list will form the values of notes_by_part\n",
    "        keys = []         # A list will contain the musical keys (one element per part)\n",
    "        \n",
    "        bpm = None        # Beats per minute, this changes a lot throughout the song.\n",
    "                          # will use to scale durations. Actually, to keep it simpler, we\n",
    "                          # will ignore this for now. The durations will just be in units\n",
    "                          # of quarter notes (AKA beats)\n",
    "                    \n",
    "        offset = 0        # the offset is the number (float value) of beats into a song\n",
    "        last_offset = 0   # the notes we are currently reading in start at.\n",
    "        \n",
    "        all_notes_ato = []  # All notes At This Offset (ATO). Tracks the notes that need to\n",
    "                            # be added in between two offsets\n",
    "        for element in notes_to_parse:\n",
    "            if (isinstance(element, instrument.Piano) or isinstance(element, instrument.Instrument)):\n",
    "                continue\n",
    "            if (isinstance(element, tempo.MetronomeMark)):    # Update bpm\n",
    "                    bpm = element.number\n",
    "            elif (isinstance(element, key.Key)):    # Musical Key\n",
    "                keys.append(str(element)) \n",
    "            elif (isinstance(element, note.Rest)):  # Ignore rests in the file, I will infer them from the\n",
    "                continue                            # offsets and durations\n",
    "            else:\n",
    "                if (bpm is None):\n",
    "                    print('bpm is None before first note, skipping part')\n",
    "                    break\n",
    "                    \n",
    "                if (element.offset == last_offset):   # We're still at this offset, so keep adding to all_notes_ato\n",
    "                    if (isinstance(element, note.Note)):     # Note\n",
    "                        all_notes_ato.append((str(element.pitch), \\\n",
    "                                                         element.duration.quarterLength))\n",
    "                    elif (isinstance(element, chord.Chord)):\n",
    "                        all_notes_ato.append(('.'.join(str(n) for n in element.pitches), \\\n",
    "                                                           element.duration.quarterLength))                                   \n",
    "                else:    # a new offset, we need to write all the different piano states\n",
    "                         # that occurred in this offset interval, and add a rest if the\n",
    "                         # offset interval is longer than the durations of these distinct states\n",
    "                    offset = element.offset\n",
    "                    cur_offset = last_offset\n",
    "                    if (all_notes_ato):   # We have notes to write at this offset\n",
    "                        all_notes_ato.sort(key = lambda x: x[1])\n",
    "                        while(cur_offset < offset):  \n",
    "                            shortest_duration = all_notes_ato[0][1]\n",
    "                            if (shortest_duration < (offset - cur_offset)):    # write some intermediate\n",
    "                                                                               # lines, for those notes\n",
    "                                                                               # whose durations fall in\n",
    "                                                                               # this offset interval\n",
    "                                notes.append(('.'.join(n[0] for n in all_notes_ato), \\\n",
    "                                            all_notes_ato[0][1]))\n",
    "                                cur_offset += shortest_duration\n",
    "                                while(all_notes_ato and all_notes_ato[0][1] == shortest_duration):\n",
    "                                    all_notes_ato.pop(0)\n",
    "                                if (not all_notes_ato):\n",
    "                                    notes.append(('rest', offset - cur_offset))\n",
    "                                    cur_offset = offset\n",
    "                                    break\n",
    "                            elif (all_notes_ato[0][1] > ((offset - cur_offset) + time_tol)):  \n",
    "                                # All notes leftover should be transferred, but with \n",
    "                                # their durations shortened.\n",
    "                                # Added tolerance because of rounding errors.\n",
    "                                corrected = []\n",
    "                                for i in range(len(all_notes_ato)):\n",
    "                                    corrected.append((all_notes_ato[i][0], all_notes_ato[i][1] \\\n",
    "                                                    - (offset - cur_offset)))\n",
    "                                all_notes_ato = corrected\n",
    "                                cur_offset = offset\n",
    "                            else:  # they are equal (or close enough!)\n",
    "                                cur_offset = offset\n",
    "                                notes.append(('.'.join(n[0] for n in all_notes_ato), \\\n",
    "                                                all_notes_ato[0][1]))\n",
    "                                all_notes_ato.clear()  # get ready for next offset interval\n",
    "                    if (isinstance(element, note.Note)):\n",
    "                        all_notes_ato.append((str(element.pitch), element.duration.quarterLength))\n",
    "                    elif (isinstance(element, chord.Chord)):\n",
    "                        all_notes_ato.append(('.'.join(str(n) for n in element.pitches), \\\n",
    "                                                           element.duration.quarterLength))\n",
    "                    last_offset = element.offset\n",
    "\n",
    "        # Add info from that song to the return variables\n",
    "        notes_by_part[part.partName] = notes    \n",
    "        keys_by_song.append(list(set(keys)))\n",
    "    \n",
    "    return keys_by_song, notes_by_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: ./composers/chopin/chpn_op23.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p19.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op7_2.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p18.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p24.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op7_1.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p23.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p9.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p8.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p22.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p20.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p21.mid\n",
      "Loading Music File: ./composers/chopin/chp_op18.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op35_4.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op33_2.mid\n",
      "Loading Music File: ./composers/chopin/chp_op31.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op25_e4.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op35_2.mid\n",
      "bpm is None before first note, skipping part\n",
      "Loading Music File: ./composers/chopin/chpn_op25_e1.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op33_4.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op53.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op35_3.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op35_1.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op25_e2.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op25_e3.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op10_e12.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op10_e05.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op10_e01.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op66.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p10.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op25_e12.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p6.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p7.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p11.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p13.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op25_e11.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p5.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p4.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p12.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p16.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op27_2.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p1.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p17.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p15.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p3.mid\n",
      "Loading Music File: ./composers/chopin/chpn_op27_1.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p2.mid\n",
      "Loading Music File: ./composers/chopin/chpn-p14.mid\n"
     ]
    }
   ],
   "source": [
    "directory = './composers'\n",
    "sub_dir = directory + '/chopin'\n",
    "songs = []\n",
    "for filename in listdir(sub_dir):\n",
    "    file = path.join(sub_dir, filename)\n",
    "    songs.append(read_midi(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[], ['B- major'], []]\n",
      "1 [[], ['E- major'], []]\n",
      "2 [[], ['A major', 'C major'], []]\n",
      "3 [[], ['A- major'], []]\n",
      "4 [[], ['F major'], []]\n",
      "5 [[], ['B- major'], []]\n",
      "6 [[], ['F major'], []]\n",
      "7 [[], ['E major'], [], []]\n",
      "8 [[], ['A major'], [], []]\n",
      "9 [[], ['B- major'], []]\n",
      "10 [[], ['E- major'], []]\n",
      "11 [[], ['B- major'], []]\n",
      "12 [[], ['D- major', 'E- major'], []]\n",
      "13 [[], ['D- major'], []]\n",
      "14 [[], ['D major', 'B- major'], []]\n",
      "15 [[], ['A major', 'D- major'], []]\n",
      "16 [[], ['C major'], []]\n",
      "17 [['G- major'], [], ['G- major'], []]\n",
      "18 [[], ['A- major'], [], []]\n",
      "19 [[], ['D major', 'B major', 'B- major'], []]\n",
      "20 [[], ['A- major', 'E major'], []]\n",
      "21 [[], ['D- major'], []]\n",
      "22 [[], ['D- major', 'B- major'], []]\n",
      "23 [[], ['A- major'], []]\n",
      "24 [[], ['B major', 'F major'], []]\n",
      "25 [[], ['E- major'], []]\n",
      "26 [[], ['G- major'], []]\n",
      "27 [[], ['C major'], []]\n",
      "28 [[], ['D- major', 'E major'], []]\n",
      "29 [[], ['E major'], []]\n",
      "30 [[], ['E- major'], []]\n",
      "31 [[], ['D major'], []]\n",
      "32 [[], ['A major'], []]\n",
      "33 [[], ['B major'], []]\n",
      "34 [[], ['F# major'], []]\n",
      "35 [[], ['C major'], []]\n",
      "36 [[], ['D major'], []]\n",
      "37 [[], ['G major'], []]\n",
      "38 [[], ['B major'], []]\n",
      "39 [[], ['D- major'], []]\n",
      "40 [[], ['D- major'], []]\n",
      "41 [[], ['C major'], []]\n",
      "42 [[], ['A- major'], []]\n",
      "43 [[], ['D- major', 'E major'], []]\n",
      "44 [[], ['G major'], []]\n",
      "45 [[], ['A- major', 'E major'], []]\n",
      "46 [[], ['C major'], []]\n",
      "47 [[], ['G- major'], []]\n",
      "[2, 12, 14, 15, 19, 20, 22, 24, 28, 43, 45]\n",
      "['B- major', 'E- major', 'A- major', 'F major', 'B- major', 'F major', 'E major', 'A major', 'B- major', 'E- major', 'B- major', 'D- major', 'C major', 'G- major', 'A- major', 'D- major', 'A- major', 'E- major', 'G- major', 'C major', 'E major', 'E- major', 'D major', 'A major', 'B major', 'F# major', 'C major', 'D major', 'G major', 'B major', 'D- major', 'D- major', 'C major', 'A- major', 'G major', 'C major', 'G- major']\n"
     ]
    }
   ],
   "source": [
    "drop_indices = []   # Drop songs where there is a key change\n",
    "\n",
    "keys_by_song = []\n",
    "notes_by_song = []\n",
    "for i in range(len(songs)):\n",
    "    keys, notes_by_part = songs[i]\n",
    "    print(i, keys)\n",
    "    new_keys = None\n",
    "    for k in keys:\n",
    "        if (k):\n",
    "            new_keys = k\n",
    "            break\n",
    "    if (len(new_keys) > 1):\n",
    "        drop_indices.append(i)\n",
    "    else:\n",
    "        keys_by_song.append(new_keys[0])\n",
    "        cur_notes_by_part = []\n",
    "        for notes in notes_by_part.values():\n",
    "            if (notes):\n",
    "                cur_notes_by_part.append(notes)   # We won't label the parts, they are\n",
    "        notes_by_song.append(cur_notes_by_part)   # all piano and we will merge them\n",
    "                                                  # as if they were all played on one piano\n",
    "print(drop_indices)\n",
    "print(keys_by_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693.0 905.5000000000001\n"
     ]
    }
   ],
   "source": [
    "# For index 17, now index 13 (after dropping those songs with multiple keys)\n",
    "# i may need to merge the two different parts:\n",
    "# Let's add up the durations to see if they're equal\n",
    "# Wow, purty close. I wonder why. Maybe need to look at offsets\n",
    "index = 13\n",
    "sum_0 = 0\n",
    "for i in range(len(notes_by_song[index][0])):\n",
    "    sum_0 += notes_by_song[index][0][i][1]\n",
    "sum_1 = 0\n",
    "for i in range(len(notes_by_song[index][1])):\n",
    "    sum_1 += notes_by_song[index][1][i][1]\n",
    "print(sum_0, sum_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this because i left out all the rests? I thought inferring the rests from offset and \n",
    "# durations would work\n",
    "# Let's drop it for now and see if we have done the reading/outputting correctly by comparing\n",
    "# the input and output midis. Update: It works!\n",
    "notes_by_song.pop(index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('C3.C2', 2.5),\n",
       "  ('E-3.E-2', 0.5),\n",
       "  ('G#3.G#2', 0.5),\n",
       "  ('B-3.B-2', 0.5),\n",
       "  ('C4.C3', 0.5),\n",
       "  ('G#3.G#2', 0.5),\n",
       "  ('E-4.E-3', 0.5),\n",
       "  ('B-4.B-3', 0.5),\n",
       "  ('C5.C4', 0.5),\n",
       "  ('G#4.G#3', 0.5),\n",
       "  ('E-5.E-4', 0.5),\n",
       "  ('B-5.B-4', 0.5),\n",
       "  ('C6.C5', 0.5),\n",
       "  ('G5.G4', 0.5),\n",
       "  ('B-5.B-4', 0.5),\n",
       "  ('G#5.G#4', 0.5),\n",
       "  ('G5.G4', 1.0),\n",
       "  ('F#5.F#4', 0.5),\n",
       "  ('rest', 1.0),\n",
       "  ('F#5.F#4', 0.5),\n",
       "  ('G5.G4', 0.5),\n",
       "  ('F#5.F#4', 0.5),\n",
       "  ('F5.F4', 0.5),\n",
       "  ('F#5.F#4', 0.5),\n",
       "  ('A5.A4', Fraction(1, 3)),\n",
       "  ('rest', 0.0),\n",
       "  ('G5.G4', Fraction(1, 3)),\n",
       "  ('E-5.E-4', Fraction(1, 3)),\n",
       "  ('rest', 0.0),\n",
       "  ('E-5.E-4', 0.75),\n",
       "  ('D5.D4', 0.25),\n",
       "  ('F5.F4', Fraction(1, 3)),\n",
       "  ('E-5.E-4', Fraction(1, 3)),\n",
       "  ('D5.D4', Fraction(1, 3)),\n",
       "  ('D5.D4', 1.0),\n",
       "  ('rest', 3.0),\n",
       "  ('C5.G3.E-3.C4', 1.5),\n",
       "  ('G4', 0.5),\n",
       "  ('D3.G3.E-4.B-4', 3.5),\n",
       "  ('D2', 0.5),\n",
       "  ('F#4.C4.D4', 0.5),\n",
       "  ('B-4', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('B-3.D4.G2.G4', 1.0),\n",
       "  ('B-3.D4.G2', 1.0),\n",
       "  ('D4.G4.B-3.D5', 1.0),\n",
       "  ('D4.G4.B-3', 1.0),\n",
       "  ('C4.E-4.G4.A3.C5', 1.0),\n",
       "  ('C4.E-4.G4.A3', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('D2.C4.D4', 0.5),\n",
       "  ('F#4', 0.5),\n",
       "  ('B-4.D2', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('B-3.D4.G2.G4', 1.0),\n",
       "  ('B-3.D4.G2', 1.0),\n",
       "  ('A3.G3.C#4.A2.E4', 1.0),\n",
       "  ('A3.G3.C#4.A2', 1.0),\n",
       "  ('F#3.A3.D4.D2.F#4', 1.0),\n",
       "  ('F#3.A3.D4.D2', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('D3.F#4.C4.D4', 0.5),\n",
       "  ('B-4.D3', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('B-3.D4.G2.G4', 1.0),\n",
       "  ('B-3.D4.G2', 1.0),\n",
       "  ('B-4.G4.C#5.E4.G5', 1.0),\n",
       "  ('B-4.G4.C#5.E4', 1.0),\n",
       "  ('B-4.C5.F4.F5', 1.0),\n",
       "  ('B-4.C5.F4', 1.0),\n",
       "  ('F4.E-4', 0.5),\n",
       "  ('F2.E-5.A4', 0.5),\n",
       "  ('D5.F2', 0.5),\n",
       "  ('C#5.F2', 0.25),\n",
       "  ('E-4.F4.A4.B-2.C#5', 1.0),\n",
       "  ('E-4.F4.A4.B-2', 1.0),\n",
       "  ('F4.D4.B-4.B-3.D5', 1.0),\n",
       "  ('F4.D4.B-4.B-3', 1.0),\n",
       "  ('E-4.G4.A3.C5', 1.0),\n",
       "  ('E-4.G4.A3', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('D2.F#4.C4.D4', 0.5),\n",
       "  ('B-4.D2', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('B-3.D4.G2.G4', 1.0),\n",
       "  ('B-3.D4.G2', 1.0),\n",
       "  ('D4.G4.B-3.D5', 1.0),\n",
       "  ('D4.G4.B-3', 1.0),\n",
       "  ('C4.E-4.G4.A3.C5', 1.0),\n",
       "  ('C4.E-4.G4.A3', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('D2.F#4.C4.D4', 0.5),\n",
       "  ('B-4.D2', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('B-3.D4.G2.G4', 1.0),\n",
       "  ('B-3.D4.G2', 1.0),\n",
       "  ('A3.G3.C#4.A2.E4', 1.0),\n",
       "  ('A3.G3.C#4.A2', 1.0),\n",
       "  ('F#3.A3.D4.D2.F#4', 1.0),\n",
       "  ('F#3.A3.D4.D2', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('D3.F#4.C4.D4', 0.5),\n",
       "  ('B-4.D3', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('D4.B3.F4.G3.G4', 1.0),\n",
       "  ('D4.B3.F4.G3', 1.0),\n",
       "  ('F4.B4.G4.D5.G2.G5', 1.0),\n",
       "  ('F4.B4.G4.D5.G2', 1.0),\n",
       "  ('G4.C5.E-5.C2.G5.C3', 1.0),\n",
       "  ('G4.C5.E-5.G3', 1.0),\n",
       "  ('D4.G4.D2.D5.D3', 1.0),\n",
       "  ('D4.G4.G3', 1.0),\n",
       "  ('C4.G4.E-2.C5.E-3', 1.0),\n",
       "  ('C4.G4.G3', 1.0),\n",
       "  ('B-3.D4.D2.G4.D3', 1.0),\n",
       "  ('B-3.D4.G3', 1.0),\n",
       "  ('B2.G5', 0.5),\n",
       "  ('C3', 0.5),\n",
       "  ('D3.G4.C5.E-5', 0.5),\n",
       "  ('C3', 0.5),\n",
       "  ('G4.C5.E-5.G3', 1.0),\n",
       "  ('C#3.D5', 0.5),\n",
       "  ('D3', 0.5),\n",
       "  ('E-3.D4.G4', 0.5),\n",
       "  ('D3', 0.5),\n",
       "  ('D4.G4.G3', 1.0),\n",
       "  ('D3.C5', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('F3.C4.G4', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('G3.C4.G4', 0.75),\n",
       "  ('E-3.E3', 0.25),\n",
       "  ('F3.E3.F3.G4', 0.25),\n",
       "  ('E3.F3', 0.1666666666666572),\n",
       "  ('E3.F3', 0.08333333333334281),\n",
       "  ('E3.F3', 0.25),\n",
       "  ('E3.F3.B-3.C#4', 0.25),\n",
       "  ('E3.F3', 0.1666666666666572),\n",
       "  ('E3.F3', 0.08333333333334281),\n",
       "  ('E3.F3', 0.25),\n",
       "  ('E3.F3.B-3.C#4', 0.25),\n",
       "  ('E3.F3', 0.1666666666666572),\n",
       "  ('E3.E-3', 0.08333333333334281),\n",
       "  ('E3', 0.25),\n",
       "  ('B-3.D4.F4.F3', 1.0),\n",
       "  ('B-3.D4', 1.0),\n",
       "  ('F4.D4.B-4.E2.E3.D5', 1.0),\n",
       "  ('F4.D4.B-4.F2.F3', 1.0),\n",
       "  ('E-4.A4.D5.F#2.F#3', 0.25),\n",
       "  ('E-4.A4.C5', 1.0),\n",
       "  ('E-4.A4.B4', 1.0),\n",
       "  ('E-4.A4.C5', 1.0),\n",
       "  ('E-4.A4.D5.F2.F3', 1.0),\n",
       "  ('E-4.A4.E-5.F#2.F#3', 1.0),\n",
       "  ('E-4.B-4.E-5.G2.G3', 1.0),\n",
       "  ('E-4.B-4', 1.0),\n",
       "  ('D4.B-4.D5', 1.0),\n",
       "  ('D4.B-4', 1.0),\n",
       "  ('D4.B-4', 1.0),\n",
       "  ('C4.F#4.B-4.D2.D3', 0.25),\n",
       "  ('C4.F#4.A4', 1.0),\n",
       "  ('C4.F#4.G#4', 1.0),\n",
       "  ('C4.F#4.A4', 1.0),\n",
       "  ('C4.F#4.B-4', 1.0),\n",
       "  ('C4.F#4.C5', 1.0),\n",
       "  ('C4.G4.C5.E-2.E-3', 1.0),\n",
       "  ('C4.G4', 1.0),\n",
       "  ('B-3.G4.B-4', 1.0),\n",
       "  ('B-3.G4', 1.0),\n",
       "  ('B-3.G4', 1.0),\n",
       "  ('E-4.G4.A4.C2.C3', 1.0),\n",
       "  ('E-4.G4', 1.0),\n",
       "  ('E-4.G4', 1.0),\n",
       "  ('D4.F#4.A4.D2.D3', 1.0),\n",
       "  ('D4.F#4', 1.0),\n",
       "  ('A4.D4.F#4', 0.25),\n",
       "  ('E4.A3.C#2.C#3.A4', 0.75),\n",
       "  ('A3.C#2.C#3.E3.A3.E4.A4', 0.75),\n",
       "  ('E3.A3.E4.E3.A3.E4.A4', 0.75),\n",
       "  ('E3.A3.E4.E3.A3.E4.A4', 0.75),\n",
       "  ('E3.A3.E4.E3.A3.E4.A4', 0.75),\n",
       "  ('E3.A3.E4.E3.A3.E4.A4.A4', 0.75),\n",
       "  ('B-4.E3.A3.E4.A4.C2.C3.A4', 0.25),\n",
       "  ('A4', 0.25),\n",
       "  ('G#4', 0.25),\n",
       "  ('A4.F3.A3.E-4.F4', 0.25),\n",
       "  ('G5', 0.25),\n",
       "  ('E5', 0.25),\n",
       "  ('F5', 0.25),\n",
       "  ('F5.F3.A3.E-4.F4', 0.25),\n",
       "  ('G6', 0.25),\n",
       "  ('F6', 0.25),\n",
       "  ('E6', 0.25),\n",
       "  ('E-6.F3.A3.E-4.F4', 0.25),\n",
       "  ('C6.A5', 0.1666666666666572),\n",
       "  ('B-5.D6', 0.08333333333334281),\n",
       "  ('C6', 0.25),\n",
       "  ('B-5.F3.A3.E-4.F4', 0.25),\n",
       "  ('G5.F5', 0.1666666666666572),\n",
       "  ('D5.E-5', 0.08333333333334281),\n",
       "  ('E5', 0.25),\n",
       "  ('F5.F3.A3.E-4.F4', 0.25),\n",
       "  ('F#5.A5', 0.1666666666666572),\n",
       "  ('G5.F5', 0.08333333333334281),\n",
       "  ('E-5', 0.25),\n",
       "  ('C#5.B-1.B-2', 0.5),\n",
       "  ('B-1.B-2.D3.F3.B-3.D5', 0.5),\n",
       "  ('D3.F3.B-3.F3.B-3.D4.D5', 0.5),\n",
       "  ('F3.B-3.D4.E-2.D5', 0.5),\n",
       "  ('E-2.C5.E-3.G3.C4.D5', 0.5),\n",
       "  ('C5.E-3.G3.C4.D5.G4.G3.C4.E-4', 0.5),\n",
       "  ('D5.G4.G3.C4.E-4.D2.B-4', 0.5),\n",
       "  ('D2.G3.B-3.E-4.B-4', 0.5),\n",
       "  ('G3.B-3.E-4.G3.B-4.D4.B-3', 0.5),\n",
       "  ('G3.B-4.D4.B-3.F#4.D3', 0.5),\n",
       "  ('B-4.D4.B-3.C#4.B-3.F#4.D3', 1.0),\n",
       "  ('A3.C4', 1.0),\n",
       "  ('G2.G4', 0.5),\n",
       "  ('G4.B-4.B-5.F4.B-3.G3', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('F5.E-4', 0.5),\n",
       "  ('F#2.E-5', 0.5),\n",
       "  ('E-5.A4.A5.E-4.F#3.A3', 0.5),\n",
       "  ('C#5', 0.5),\n",
       "  ('E-5.D4', 0.5),\n",
       "  ('G2.D5', 0.5),\n",
       "  ('D5.B-4.B-5.F4.B-3.G3', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('F5.E-4', 0.5),\n",
       "  ('F2.E-5', 0.5),\n",
       "  ('E-5.G#4.G#5.E-4.B3.F3.G#3', 0.5),\n",
       "  ('C#5', 0.5),\n",
       "  ('E-5.D4', 0.5),\n",
       "  ('E-2.D5', 0.5),\n",
       "  ('D5.G4.G5.D4.G3.E-3', 0.5),\n",
       "  ('B4', 0.5),\n",
       "  ('D5.C4', 0.5),\n",
       "  ('C2.C5', 0.5),\n",
       "  ('C5.C4.C5.B-3.E-3', 0.5),\n",
       "  ('G#4', 0.5),\n",
       "  ('B-4.A3', 0.5),\n",
       "  ('D2.A4', 0.5),\n",
       "  ('A4.E-4.E-5.G3.C4.D3', 0.5),\n",
       "  ('B4', 0.5),\n",
       "  ('D5.F#3', 0.5),\n",
       "  ('D2.C5', 0.5),\n",
       "  ('C5.E-4.G4.A3', 0.5),\n",
       "  ('D4', 0.5),\n",
       "  ('C#4.F#4.A4.D3', 0.5),\n",
       "  ('C4', 0.5),\n",
       "  ('G3.G2.B-3.G4.B-4', 0.5),\n",
       "  ('B-3.G4.B-4.F4.B-4.B-5.G3', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('F5.B-3.E-4', 0.5),\n",
       "  ('F#3.F#2.E-5', 0.5),\n",
       "  ('E-5.E-4.A4.A5.F#3', 0.5),\n",
       "  ('C#5', 0.5),\n",
       "  ('E-5.A3.D4', 0.5),\n",
       "  ('G3.G2.D5', 0.5),\n",
       "  ('D5.F4.B-4.B-5.G3', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('F5.B-3.E-4', 0.5),\n",
       "  ('E-5', 0.5),\n",
       "  ('F3.F2', 0.5),\n",
       "  ('rest', 0.5),\n",
       "  ('E-4.C#5.G#4.G#5.F3', 0.5),\n",
       "  ('E-5.G#3.B3.D4', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('E-3.E-2', 0.5),\n",
       "  ('rest', 0.5),\n",
       "  ('D4.B4.G4.G5.E-3', 0.5),\n",
       "  ('D5.G3.C4', 0.5),\n",
       "  ('C5', 0.5),\n",
       "  ('C3.C2', 0.5),\n",
       "  ('rest', 0.5),\n",
       "  ('B-3.G#4.C4.C5.C3', 0.5),\n",
       "  ('B-4.E-3.A3', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('D2', 0.5),\n",
       "  ('rest', 0.5),\n",
       "  ('A2.G3.B4.E-4.E-5', 0.5),\n",
       "  ('D5.C3.F#3', 0.5),\n",
       "  ('C5', 0.5),\n",
       "  ('D2', 0.5),\n",
       "  ('rest', 0.5),\n",
       "  ('A2.E-3.C#4.F#3.F#4', 0.5),\n",
       "  ('E-4.C3.D3', 0.5),\n",
       "  ('D4', 0.5),\n",
       "  ('G4.B-3.G2', 0.5),\n",
       "  ('C4.G3.G1.G2', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('A3', 0.25),\n",
       "  ('G3', 0.25),\n",
       "  ('B-3.G4.D3.G2', 0.5),\n",
       "  ('C4.G3.E-3.G2', 0.25),\n",
       "  ('B-3.D3', 0.25),\n",
       "  ('A3', 0.25),\n",
       "  ('G3', 0.25),\n",
       "  ('F4.C3.G2.E-3', 0.25),\n",
       "  ('C4', 0.25),\n",
       "  ('E-4.A3', 0.25),\n",
       "  ('F#3', 0.25),\n",
       "  ('C4', 0.25),\n",
       "  ('E-4', 0.25),\n",
       "  ('F#4', 0.25),\n",
       "  ('C5', 0.25),\n",
       "  ('F5.G2.G3', 0.25),\n",
       "  ('E-5', 0.25),\n",
       "  ('D5.E-4.E-3', 0.25),\n",
       "  ('C5', 0.25),\n",
       "  ('G5.B-4.D3.D4', 0.25),\n",
       "  ('rest', 0.25),\n",
       "  ('G4.C5.G2.G3', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('A4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('B-4.G5.D4.G3', 0.5),\n",
       "  ('C5.G4.E-4.G3', 0.25),\n",
       "  ('B-4.D4', 0.25),\n",
       "  ('A4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('F5.C4.G3.E-4', 0.25),\n",
       "  ('C5', 0.25),\n",
       "  ('E-5.A4', 0.25),\n",
       "  ('F#4', 0.25),\n",
       "  ('C5', 0.25),\n",
       "  ('E-5', 0.25),\n",
       "  ('F#5', 0.25),\n",
       "  ('C6', 0.25),\n",
       "  ('F6.G3.G4', 0.25),\n",
       "  ('E-6', 0.25),\n",
       "  ('D6.E-5.E-4', 0.25),\n",
       "  ('C6', 0.25),\n",
       "  ('G6.B-5.G4.D5', 0.25),\n",
       "  ('rest', 0.5),\n",
       "  ('G6.C7', 0.25),\n",
       "  ('B-6', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('B-6.F#6.E-5.E-4.F#4.A4', 0.25),\n",
       "  ('A6', 0.25),\n",
       "  ('D6.D5', 0.25),\n",
       "  ('F#5.B-5', 0.25),\n",
       "  ('A5.D4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('C6.G5.G3', 0.25),\n",
       "  ('B-5', 0.25),\n",
       "  ('D6.D4', 0.25),\n",
       "  ('G6.C7', 0.25),\n",
       "  ('B-6.D5', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('B-6.F#6.F#4.E-4.E-5.A4', 0.25),\n",
       "  ('A6', 0.25),\n",
       "  ('D6.D5', 0.25),\n",
       "  ('F#5.B-5', 0.25),\n",
       "  ('A5.D4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('G5.C6.G3', 0.25),\n",
       "  ('B-5', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('G6.C7', 0.25),\n",
       "  ('B-6.D3', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('B-6.F#6.F#3.E-4.A3.E-3', 0.25),\n",
       "  ('A6', 0.25),\n",
       "  ('D6.D4', 0.25),\n",
       "  ('F#5.B-5', 0.25),\n",
       "  ('A5.D3', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('C5.G4.G2', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5.D3', 0.25),\n",
       "  ('C6.G5', 0.25),\n",
       "  ('B-5.D4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('B-5.F#5.E-4.E-3.F#3.A3', 0.25),\n",
       "  ('A5', 0.25),\n",
       "  ('D5.D4', 0.25),\n",
       "  ('B-4.F#4', 0.25),\n",
       "  ('A4.D3', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('C5.G4.G2', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5.B-3.D3', 0.25),\n",
       "  ('C6.G5', 0.25),\n",
       "  ('B-5', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('A5.D5.C#2.C#3', 0.25),\n",
       "  ('G5', 0.25),\n",
       "  ('B-4.D2.D3', 0.25),\n",
       "  ('E-5.B-4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('C5.G4.G2.G1', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D4.B-2', 0.25),\n",
       "  ('D4.A4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('E-4.B-3.C#2', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('G3.D2', 0.25),\n",
       "  ('G3.C4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('G3.C4.G2.G1', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('G3.D4', 0.25),\n",
       "  ('C4.C2', 0.25),\n",
       "  ('E-3', 0.25),\n",
       "  ('G3.C4.G1.G2', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('G3.D4', 0.25),\n",
       "  ('C4.C2', 0.25),\n",
       "  ('E-3', 0.25),\n",
       "  ('G3.C4.G1.G2', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('D4.G3', 0.25),\n",
       "  ('C4.C2', 0.25),\n",
       "  ('E-3', 0.25),\n",
       "  ('G3.C4.G2.G1', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('D4.G3', 0.25),\n",
       "  ('C4.C2', 0.25),\n",
       "  ('E-3', 0.25),\n",
       "  ('G3.G1.G2', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('G5', 0.25),\n",
       "  ('B-5.D4.G4', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('G6.D4.G3', 0.25),\n",
       "  ('D7', 0.25),\n",
       "  ('B-6.D4.G4', 0.25),\n",
       "  ('D7', 0.25),\n",
       "  ('G6', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('B-5', 0.25),\n",
       "  ('G5', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3.D2', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('G3.G1.G2', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3.D2', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('F#3.F#1.F#2', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('F#4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('F#4', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3.D2', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('G3.G2.G1', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('G5', 0.25),\n",
       "  ('B-5.G3.D3', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('G6.G2.D3', 0.25),\n",
       "  ('D7', 0.25),\n",
       "  ('B-6.D3.G3', 0.25),\n",
       "  ('D7', 0.25),\n",
       "  ('G6', 0.25),\n",
       "  ('D6', 0.25),\n",
       "  ('B-5', 0.25),\n",
       "  ('G5', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('G4', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3.D2', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('F#3.F#2.F#1', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('F#4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('F#4', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3.D2', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('F3.F2.F1', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('F4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('B-4', 0.25),\n",
       "  ('D5', 0.25),\n",
       "  ('F4', 0.25),\n",
       "  ('D4', 0.25),\n",
       "  ('B-3.G2', 0.25),\n",
       "  ('D3', 0.25),\n",
       "  ('F3.A2', 0.25),\n",
       "  ('C4', 0.25),\n",
       "  ('A3', 0.25),\n",
       "  ('C4', 0.25),\n",
       "  ('F4', 0.25),\n",
       "  ('A4', 0.25),\n",
       "  ('C5', 0.25),\n",
       "  ('F5', 0.25),\n",
       "  ('A5.C4.F4', 0.25),\n",
       "  ('C6', 0.25),\n",
       "  ('F6.C4.F3', 0.25),\n",
       "  ('A6', 0.25),\n",
       "  ('F7.F4.C4', 0.5),\n",
       "  ('F4.C4.F3.C4', 0.5),\n",
       "  ('F4.C4', 2.5),\n",
       "  ('F3.C4', 0.5),\n",
       "  ('C4.F4', 1.0),\n",
       "  ('F3.C4', 0.5),\n",
       "  ('F4.C4', 0.5),\n",
       "  ('F3.C4', 0.5),\n",
       "  ('B-1.C4.F4', 1.0),\n",
       "  ('B-2', 1.0),\n",
       "  ('G#3', 1.0),\n",
       "  ('F3.G4.D4', 1.0),\n",
       "  ('C4', 1.0),\n",
       "  ('G4.B-3', 1.0),\n",
       "  ('E-2.E-4', 1.0),\n",
       "  ('B-2', 1.0),\n",
       "  ('E-3', 1.0),\n",
       "  ('G3', 1.0),\n",
       "  ('B-3.E-5', 1.0),\n",
       "  ('E-4.D5', 0.5),\n",
       "  ('C4.G4.C5', 1.0),\n",
       "  ('E-4', 1.0),\n",
       "  ('F3', 1.0),\n",
       "  ('E-4.D5.A4', 1.0),\n",
       "  ('G4', 1.0),\n",
       "  ('D5.F4', 1.0),\n",
       "  ('B-2.B-4', 1.0),\n",
       "  ('F3', 1.0),\n",
       "  ('B-3', 1.0),\n",
       "  ('D4.B-5', 1.0),\n",
       "  ('F4', 1.0),\n",
       "  ('B-5.B-4', 1.0),\n",
       "  ('C4.B-5', 1.0),\n",
       "  ('E-4', 1.0),\n",
       "  ('G#5.F3', 1.0),\n",
       "  ('G#5.B-3', 1.0),\n",
       "  ('G5.D4', 1.0),\n",
       "  ('F#5.G#4', 1.0),\n",
       "  ('B-3.F#5', 1.0),\n",
       "  ('D4', 1.0),\n",
       "  ('E-3.G5', 1.0),\n",
       "  ('G#3', 1.0),\n",
       "  ('C4.G#5', 1.0),\n",
       "  ('G4.E-5', 0.5),\n",
       "  ('G#3.G5', 1.0),\n",
       "  ('C4', 1.0),\n",
       "  ('D3.F5', 1.0),\n",
       "  ('G3', 1.0),\n",
       "  ('B3.G5', 1.0),\n",
       "  ('F4.D5', 0.5),\n",
       "  ('F5.G3', 1.0),\n",
       "  ('E-5.C4', 1.0),\n",
       "  ('D5.C3', 1.0),\n",
       "  ('C5.F3', Fraction(1, 3)),\n",
       "  ('D5', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('B4.A3', 0.5),\n",
       "  ('C5', 0.5),\n",
       "  ('D5.E-4', 0.5),\n",
       "  ('E-5.C4', 0.08333333333333331),\n",
       "  ('F3.C4.F4', 1.0),\n",
       "  ('G#3', 1.0),\n",
       "  ('B-2', 1.0),\n",
       "  ('G#3.G4.D4', 1.0),\n",
       "  ('C4', 1.0),\n",
       "  ('G4.B-3', 1.0),\n",
       "  ('E-2.E-4', 1.0),\n",
       "  ('B-2', 1.0),\n",
       "  ('E-3', 1.0),\n",
       "  ('G3', 1.0),\n",
       "  ('B-3.E-5', 1.0),\n",
       "  ('E-4.D5', 0.5),\n",
       "  ('C4.G4.C5', 1.0),\n",
       "  ('E-4', 1.0),\n",
       "  ('F3', 1.0),\n",
       "  ('E-4.D5.A4', 1.0),\n",
       "  ('G4', 1.0),\n",
       "  ('D5.F4', 1.0),\n",
       "  ('B-2.B-4', 1.0),\n",
       "  ('F3', 1.0),\n",
       "  ('B-3', 1.0),\n",
       "  ('D4.B-5', 1.0),\n",
       "  ('F4', 1.0),\n",
       "  ('B-5.B-4', 1.0),\n",
       "  ('C2.B-5', 1.0),\n",
       "  ('E4', 1.0),\n",
       "  ('B-3', 1.0),\n",
       "  ('C5.E4.G3', 1.0),\n",
       "  ('D5.B-3', 1.0),\n",
       "  ('E-5.F3', Fraction(1, 3)),\n",
       "  ('F5', Fraction(1, 3)),\n",
       "  ('E-5', Fraction(1, 3)),\n",
       "  ('D5.E-4', 1.0),\n",
       "  ('E-5.A3', 1.0),\n",
       "  ('G5.B-2', 1.0),\n",
       "  ('D4.G5', 1.0),\n",
       "  ('F5.G#3', Fraction(1, 3)),\n",
       "  ('E-5', 0.25),\n",
       "  ('rest', 0.08333333333331439),\n",
       "  ('E-2.E-5', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-4', 1.0),\n",
       "  ('G4', 0.5),\n",
       "  ('G#4', 0.5),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('D5', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('E-2.B-4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-4', 1.0),\n",
       "  ('G4', 0.5),\n",
       "  ('G#4', 0.5),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('D5', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('E-2.B-4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('G#3', 0.5),\n",
       "  ('F3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G#3', 0.5),\n",
       "  ('D4', 1.0),\n",
       "  ('F4', 0.5),\n",
       "  ('G#4', 0.5),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('D5', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('E-2.B-4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('E-4.G3.C#4', Fraction(1, 3)),\n",
       "  ('G4', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('E-2.G#4', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('G#3.B3', 0.5),\n",
       "  ('E-4', 0.5),\n",
       "  ('F4', 0.5),\n",
       "  ('E-2.G4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-4', 1.0),\n",
       "  ('G4', 0.5),\n",
       "  ('G#4', 0.5),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('D5', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('E-2.B-4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('G3.B-5', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-4', 1.0),\n",
       "  ('rest', 0.3333333333333144),\n",
       "  ('G4', Fraction(1, 3)),\n",
       "  ('G#4', Fraction(1, 3)),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('D5', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('E-2.B-4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('G#3.B-5', 0.5),\n",
       "  ('F3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G#3', 0.5),\n",
       "  ('D4', 1.0),\n",
       "  ('rest', 0.3333333333333144),\n",
       "  ('F4', Fraction(1, 3)),\n",
       "  ('G#4', Fraction(1, 3)),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('D5', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('E-2.B-4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('E-4.G3.C#4', Fraction(1, 3)),\n",
       "  ('G4', Fraction(1, 3)),\n",
       "  ('C5', Fraction(1, 3)),\n",
       "  ('B-4', Fraction(1, 3)),\n",
       "  ('E-2.G#4', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('G#3.B3', 0.5),\n",
       "  ('E-4', 0.5),\n",
       "  ('F4', 0.5),\n",
       "  ('E-2.G4', 0.5),\n",
       "  ('B-2', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('E-4.G4', 0.5),\n",
       "  ('B-4', 0.5),\n",
       "  ('G5', 0.5),\n",
       "  ('F5', 0.5),\n",
       "  ('E-5', 0.5),\n",
       "  ('G2.D5', 0.5),\n",
       "  ('D3', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G3', 0.5),\n",
       "  ('D4', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('G4.B-4', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('B-5', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('E-5', 0.5),\n",
       "  ('B-2.E5', 0.5),\n",
       "  ('F3', 0.5),\n",
       "  ('D4.F5', 0.5),\n",
       "  ('B-3', 0.5),\n",
       "  ('F4', 0.5),\n",
       "  ('D4', 0.5),\n",
       "  ('B-4.D5', 0.5),\n",
       "  ('F5', 0.5),\n",
       "  ('D6', 0.5),\n",
       "  ('C6', 0.5),\n",
       "  ('B-5', 0.5),\n",
       "  ('G#5.D3', 0.5),\n",
       "  ('A5.A3', 0.5),\n",
       "  ('F6.F4', 0.5),\n",
       "  ('E6.D4', 0.5),\n",
       "  ('D6.A4', 0.5),\n",
       "  ('A5.F4', 0.5),\n",
       "  ('D5', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('F4', 0.5),\n",
       "  ('D4', 0.5),\n",
       "  ('E4', Fraction(1, 3)),\n",
       "  ('G4.F4', 0.08333333333331439),\n",
       "  ('F4.E3', 0.08333333333331439),\n",
       "  ('E2.E3', 1.0),\n",
       "  ('E2', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('E2.D4.E4', 0.5),\n",
       "  ('G#4', 0.5),\n",
       "  ('C5.E2', 0.5),\n",
       "  ('B4', 0.5),\n",
       "  ('C4.E4.E2.A4', 1.0),\n",
       "  ('C4.E4.E2', 1.0),\n",
       "  ('A4.E3.A3.E5.E4', 1.0),\n",
       "  ('A4.A3.E3', 1.0),\n",
       "  ('A4.F4.A3.E3.D5.D4', 1.0),\n",
       "  ('F4.A4.E3.A3', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('E2.D4.E4', 0.5),\n",
       "  ('G#4', 0.5),\n",
       "  ('C5.E2', 0.5),\n",
       "  ('B4', 0.5),\n",
       "  ('C4.E4.E2.A4', 1.0),\n",
       "  ('C4.E4.E2', 1.0),\n",
       "  ('E4.A3.C4.E2.F#4.F#3', 1.0),\n",
       "  ('E4.A3.C4.E2', 1.0),\n",
       "  ('B3.E4.E2.G#4.G#3', 1.0),\n",
       "  ('B3.E4.E2', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('E2.G#4.D4.E4', 0.5),\n",
       "  ('C5.E2', 0.5),\n",
       "  ('B4', 0.5),\n",
       "  ('C4.E4.E2.A4', 1.0),\n",
       "  ('C4.E4.E2', 1.0),\n",
       "  ('C5.A4.C4.E3.A3.E5.E4', 1.0),\n",
       "  ('A4.C5.E3.C4.A3', 1.0),\n",
       "  ('E4.A4.C5.E3.C4.A3.F#5.F#4', 1.0),\n",
       "  ('A4.C5.E4.C4.E3.A3', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('E2.G#4.C4.E4', 0.5),\n",
       "  ('B4.E2', 0.5),\n",
       "  ('A4', 0.5),\n",
       "  ('E4.C4.E2.A4', 1.0),\n",
       "  ('C4.E4.E2', 1.0),\n",
       "  ('E4.A4.C5.E3.C4.A3.F#5.F#4', 1.0),\n",
       "  ('E4.A4.C5.A3.E3.C4', 1.0),\n",
       "  ('A4.C5.E-5.E3.C4.A3.G#5.G#4', 1.0),\n",
       "  ('E-5.C5.A4.A3.E3.C4', 1.0),\n",
       "  ('F#5.F#4.A4.E-5.C5.A3.E3.C4', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('G#5.G#4', 0.5),\n",
       "  ('E-5.C5.A4.E3.C4.A3.G#5.G#4', 1.0),\n",
       "  ('E-5.C5.A4.E3.C4.A3', 1.0),\n",
       "  ('F#5.F#4.A4.C5.E-5.C4.E3.A3', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('G#5.G#4', 0.5),\n",
       "  ('E-5.C5.A4.E3.A3.C4.G#5.G#4', 1.0),\n",
       "  ('E-5.C5.A4.E3.C4.A3', 1.0),\n",
       "  ('F#5.F#4.E-5.A4.C5.A3.E3.C4', 1.0),\n",
       "  ('rest', 0.5),\n",
       "  ('G#5.G#4', 0.5),\n",
       "  ('F#5.C5.E-5.A3.E3.C4.A5.A4', 1.0),\n",
       "  ('F#5.E-5.C5.C4.E3.A3', 1.0),\n",
       "  ('A3.E3.C4.G#4.G#5.F#5.E-5.C5', Fraction(2, 3)),\n",
       "  ('C4.E3.A3.E-5.C5.F#5.A5.A4', 0.75),\n",
       "  ('E3.A3.E-5.C5.F#5.A5.A4.E1.E2.B4.D5.B5.G#5.E5', 0.749999999999962),\n",
       "  ('A3.E-5.C5.F#5.A5.A4.E1.E2.E3.D4.G#3.B4.D5.B5.G#5.E5', 0.5),\n",
       "  ('E1.E2.E3.D4.G#3.D4.E4.G#3.B4.D5.B5.G#5.E5', 0.25),\n",
       "  ('E3.D4.G#3.D4.E4.G#3.B4.D5.B5.G#5.E5', 0.5),\n",
       "  ('D4.E4.G#3.G#4.B3.D4.B4.D5.B5.G#5.E5.E5.D5.G#5.C#6', 0.75),\n",
       "  ('G#4.B3.D4.E4.G#3.D4.B4.D5.B5.G#5.E5.E5.D5.G#5.C#6', 0.75),\n",
       "  ('E4.G#3.D4.D5.E5.G#5.C#6.D4.E3.G#3.B4.D5.B5.G#5.E5.E5.D5.G#5.C#6', 0.75),\n",
       "  ('D5.E5.G#5.C#6.D4.E3.G#3.A2.A1.B4.D5.B5.G#5.E5.E5.D5.G#5.C#6.A5.C#5.A4',\n",
       "   0.75),\n",
       "  ('A2.A1.B4.D5.B5.G#5.E5.E5.D5.G#5.C#6.E3.C#4.A5.C#5.A4', 0.75),\n",
       "  ('B4.D5.B5.G#5.E5.E5.D5.G#5.C#6.E3.C#4.A3.E4.A5.C#5.A4', 0.75),\n",
       "  ('A3.E4.C#4.A4.A5.C#6.A6.A5.C#5.A4', 0.75),\n",
       "  ('C#4.A4.E4.A3.A5.C#6.A6.A5.C#5.A4', 0.75),\n",
       "  ('E4.A3.G#5.C#6.G#6.C#4.E3.A5.C#6.A6.A5.C#5.A4', 0.75),\n",
       "  ('G#5.C#6.G#6.C#4.E3.B2.B1.A5.C#6.A6.A5.C#5.A4.F#5.C#6.F#6', 0.75),\n",
       "  ('B2.B1.A5.C#6.A6.B3.F#3.A5.C#5.A4.F#5.C#6.F#6', 0.75),\n",
       "  ('A5.C#6.A6.B3.F#3.A5.C#5.A4.E-4.A3.F#5.C#6.F#6', 0.75),\n",
       "  ('A5.C#5.A4.E-4.A3.F#4.B3.B5.E-6.G#6.F#5.C#6.F#6', 0.75),\n",
       "  ('F#4.B3.A3.E-4.B5.E-6.G#6.F#5.C#6.F#6', 0.75),\n",
       "  ('A3.E-4.G#6.E-6.A5.B3.F#3.B5.E-6.G#6.F#5.C#6.F#6', 0.75),\n",
       "  ('G#6.E-6.A5.B3.F#3.E3.E2.B5.E-6.G#6.F#5.C#6.F#6.E6.G#5.E5', 0.75),\n",
       "  ('E3.E2.B5.E-6.G#6.E3.B3.F#5.C#6.F#6.E6.G#5.E5', 0.75),\n",
       "  ('B5.E-6.G#6.E3.B3.F#5.C#6.F#6.G#3.E4.E6.G#5.E5', 0.75),\n",
       "  ('F#5.C#6.F#6.G#3.E4.G#4.B3.E6.E5.E6.G#5.E5', 0.75),\n",
       "  ('G#4.B3.G#3.E4.E6.E5.E6.G#5.E5', 0.75),\n",
       "  ('G#3.E4.E5.E6.B3.E3.E6.E5.E6.G#5.E5', 0.75),\n",
       "  ('E5.E6.B3.E3.F#2.F#3.E6.E5.E6.G#5.E5.E5.E6.A5', 0.75),\n",
       "  ('F#2.F#3.E6.E5.F#3.D4.E6.G#5.E5.E5.E6.A5', 0.75),\n",
       "  ('E6.E5.F#3.D4.E6.G#5.E5.D5.D6.A4.A3.E5.E6.A5', 0.75),\n",
       "  ('D6.D5.E6.G#5.E5.D5.D6.A4.A3.B1.B2.E5.E6.A5', Fraction(1, 3)),\n",
       "  ('E5.E6.E6.G#5.E5.D5.D6.A4.A3.B1.B2.E5.E6.A5', Fraction(1, 3)),\n",
       "  ('D5.D6', Fraction(1, 3)),\n",
       "  ('rest', 0.0),\n",
       "  ('C#5.C#6.F#3.D4', 0.5),\n",
       "  ('D6.D5', 0.5),\n",
       "  ('F#6.F#5.A4.B3', 0.5),\n",
       "  ('E6.E5', 0.5),\n",
       "  ('E3.E2.D5.D6.G#5', 1.0),\n",
       "  ('E3.C#4', 1.0),\n",
       "  ('C#5.C#6.G#3.G#4', 1.0),\n",
       "  ('C#6.C#5.A2.A1', Fraction(1, 3)),\n",
       "  ('D5.D6.A2.A1', Fraction(1, 3)),\n",
       "  ('C#5.C#6', Fraction(1, 3)),\n",
       "  ('rest', 0.0),\n",
       "  ('C5.C6.E3.C#4', 0.5),\n",
       "  ('C#6.C#5', 0.5),\n",
       "  ('E5.E6.G#4.A3', 0.5),\n",
       "  ('D5.D6', 0.5),\n",
       "  ('C#5.C#6.D2.D3.F#5', 1.0),\n",
       "  ('D3.B3.B4.B5', 1.0),\n",
       "  ('F#3.F#4.B-4.B-5', 0.5),\n",
       "  ('B5.B4.C#2.C#3.F5', 0.75),\n",
       "  ('C5.C6.C#4.G#3', 0.5),\n",
       "  ('C#6.C#5.F4.B3', 0.25),\n",
       "  ('G#4.G#5', 0.75),\n",
       "  ('B4.B5.F#2.F#1.C#5.F#5', 1.0),\n",
       "  ('C#4.A5.A4.F#3', 1.0),\n",
       "  ('G#5.A3.F#4', 0.25),\n",
       "  ('F#5.G#5', 0.25),\n",
       "  ('A4.F#5.E-5.B4.B1.B2', 1.0),\n",
       "  ('A4.F5.E-5.B4.B3.F3', 0.5),\n",
       "  ('B4.A4.E-5.F#5', 0.5),\n",
       "  ('A4.B4.E-5.G#5.E-4.A3', 0.5),\n",
       "  ('B4.E-5.A4.A5', 0.5),\n",
       "  ('E2.E1.G#4.E4.D4.B4', 1.0),\n",
       "  ('G#3.B2', 1.0),\n",
       "  ('B3.E3', 1.0),\n",
       "  ('G#3.D4.C#5.E5.E4.G#4', 1.0),\n",
       "  ('B3.E3', 1.0),\n",
       "  ('D4.E5.E4.G#4.C#5.B2.G#3', 1.0),\n",
       "  ('C#4.A3.A4.A1.A2', 1.0),\n",
       "  ('E3.C#4', 1.0),\n",
       "  ('C#5.A4.A5.A3.E4', 1.0),\n",
       "  ('C#4.A4.A6.A5.C#6', 1.0),\n",
       "  ('E4.A3', 1.0),\n",
       "  ('G#6.C#6.G#5.C#4.E3', 1.0),\n",
       "  ('B1.B2.C#6.F#5.F#6', 1.0),\n",
       "  ('B3.F#3', 1.0),\n",
       "  ('E-4.A3', 1.0),\n",
       "  ('B3.F#4.G#6.E-6.B5', 1.0),\n",
       "  ('A3.E-4', 1.0),\n",
       "  ('G#6.E-6.A5.B3.F#3', 1.0),\n",
       "  ('E3.E2.G#5.E5.E6', 1.0),\n",
       "  ('E3.B3', 1.0),\n",
       "  ('G#3.E4', 1.0),\n",
       "  ('B3.G#4.E6.E7', 1.0),\n",
       "  ('G#3.E4', 1.0),\n",
       "  ('B3.E3.E6.E7', 0.25),\n",
       "  ('E6.E7.E3.E2', 0.5),\n",
       "  ('E3.E2.B-3.E3', 0.5),\n",
       "  ('B-3.E3.C#4.F#3', 0.5),\n",
       "  ('C#4.F#3.B-3.E4.F#4.F#5', 0.5),\n",
       "  ('B-3.E4.F#3.C#4.F#4.F#5', 0.5),\n",
       "  ('F#3.C#4.E3.B-3.F#4.F#5', 0.5),\n",
       "  ('E3.B-3.E1.E2.F#4.F#5', 0.5),\n",
       "  ('F#5.F#4.E1.E2.E3.B-3.F#4.F#5', 0.25),\n",
       "  ('G#4.G#5', 0.25),\n",
       "  ('F#4.F#5', 0.5),\n",
       "  ('F4.F5.F#3.C#4', 0.5),\n",
       "  ('F#4.F#5', 0.5),\n",
       "  ('G#4.G#5.E4.B-3', 0.5),\n",
       "  ('B-4.B-5', 0.5),\n",
       "  ('B4.B5.C#4.F#3', 0.5),\n",
       "  ('C#5.C#6', 0.5),\n",
       "  ('D6.D5.E3.B-3', 0.5),\n",
       "  ('E5.E6', 0.5),\n",
       "  ('F#5.F#6.E2.E3', 0.5),\n",
       "  ('E2.E3.E3.C4', 0.5),\n",
       "  ('E3.C4.G#3.E-4', 0.5),\n",
       "  ('G#3.E-4.F#4.C4.G#4.G#5', 0.5),\n",
       "  ('F#4.C4.G#3.E-4.G#4.G#5', 0.5),\n",
       "  ('G#3.E-4.G#4.G#5.G#5.G#4.E3.C4', 0.5),\n",
       "  ('G#4.G#5.G#5.G#4.E3.C4.E2.E1.G#4', 0.5),\n",
       "  ('G#5', 0.5),\n",
       "  ('A5.E3.B3', 0.5),\n",
       "  ('G#5', 0.5),\n",
       "  ('G4.G5.G#3.E-4', 0.5),\n",
       "  ('G#4.G#5', 0.5),\n",
       "  ('B-4.B-5.C4.F#4', 0.5),\n",
       "  ('C5.C6', 0.5),\n",
       "  ('C#5.C#6.E-4.G#3', 0.5),\n",
       "  ('E-5.E-6', 0.5),\n",
       "  ('E5.E6.C4.E3', 0.5),\n",
       "  ('F#6.F#5', 0.5),\n",
       "  ('G#5.G#6.E2.E3', 0.5),\n",
       "  ('E2.E3.E3.C#4', 0.5),\n",
       "  ('E3.C#4.G#3.E4', 0.5),\n",
       "  ('G#3.E4.E-2.E-3.G#4.G#5', 0.5),\n",
       "  ('E-2.E-3.E-3.B3.G#4.G#5', 0.5),\n",
       "  ('E-3.B3.G#4.G#5.G#4.G#5.E-4.G#3', 0.5),\n",
       "  ('G#5.G#4.G#4.G#5.G#4.G#5.E-4.G#3.C#2.C#3', 0.25),\n",
       "  ('B-4.B-5', 0.25),\n",
       "  ('G#4.G#5', 0.5),\n",
       "  ('G4.G5.G#3.C#3', 0.5),\n",
       "  ('G#4.G#5', 0.5),\n",
       "  ('B-5.B-4.C#4.E3', 0.5),\n",
       "  ('B4.B5', 0.5),\n",
       "  ('C#5.C#6.G#1.G#2', 0.5),\n",
       "  ('E-5.E-6', 0.5),\n",
       "  ('F6.F5.B2.G#3', 0.5),\n",
       "  ('G5.G6', 0.5),\n",
       "  ('G#5.G#6.E-3.B3', 0.5),\n",
       "  ('B-5.B-6', 0.5),\n",
       "  ('B5.B6.F2.F1', 0.25),\n",
       "  ('F2.F1.G#6.B6', 0.25),\n",
       "  ('F6', 0.25),\n",
       "  ('E-6', 0.25),\n",
       "  ('B5.F3.B3', 0.16666666666662877),\n",
       "  ('G#5.B5.E-4.G#4', 0.25),\n",
       "  ('F5', 0.25),\n",
       "  ('E-5', 0.25),\n",
       "  ('B4', 0.25),\n",
       "  ('B4.G#4', 0.25),\n",
       "  ('F4', 0.25),\n",
       "  ('E-4', 0.25),\n",
       "  ('B3', 0.25),\n",
       "  ('G#3.B3', 0.25),\n",
       "  ('F3', 0.25),\n",
       "  ('E-3', 0.25),\n",
       "  ('B2', 0.25),\n",
       "  ('B3.G#3', 0.25),\n",
       "  ('F3', 0.25),\n",
       "  ('E-3', 0.25),\n",
       "  ...]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the data\n",
    "notes_by_song[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_offset = {'C': 0, 'D': 2, 'E': 4, 'F': 5, 'G': 7, 'A': 9, 'B': 11}  # relative offsets from\n",
    "                                                                        # C in the right-dir\n",
    "piano_offset = 3  # C1 is the 4th key on the piano (index of 3)\n",
    "\n",
    "max_octave = 8    # Last key on the piano is C8\n",
    "notes_in_octave = 12\n",
    "\n",
    "def note_to_piano_idx(a_note):\n",
    "    \"\"\"Simply convert a note in the format {Letter}{Octave} to the 0-based index number of its corresponding\n",
    "    position on the piano\"\"\"\n",
    "    a_note, octave = a_note[:-1], int(a_note[-1])\n",
    "    if (int(octave) > max_octave):\n",
    "        print(\"WARNING: octave = \", octave)\n",
    "        return np.array([])\n",
    "    if (len(a_note) > 1):  \n",
    "        if (a_note[1] == '-'): # a flat!\n",
    "            return piano_offset + rel_offset[a_note[0]] + notes_in_octave * (octave - 1) - 1\n",
    "        elif (a_note[1] == '#'): # a sharp!\n",
    "            return piano_offset + rel_offset[a_note[0]] + notes_in_octave * (octave - 1) + 1\n",
    "        else:\n",
    "            print(\"Waring: note = \", a_note)\n",
    "    return piano_offset + rel_offset[a_note[0]] + notes_in_octave * (octave - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keys_piano = 88\n",
    "\n",
    "def transpose_sequence(sequence, transposition):\n",
    "    \"\"\" Perform a right-shift on the keys' part of the vectors\n",
    "      Effectively, this outputs a new sequence repesenting\n",
    "      a song but transposed.\n",
    "      The size of the shift is transposition\"\"\"\n",
    "    if (transposition == 0):\n",
    "        return sequence\n",
    "    shift = transposition\n",
    "    sequence, durations = sequence[:, :-1], sequence[:, -1]\n",
    "    for i in range(len(sequence)):\n",
    "        sequence[i] = np.concatenate((sequence[i][-shift:], \\\n",
    "                                      sequence[i][:-shift]))\n",
    "    return np.insert(sequence, len(sequence[0]), durations, axis = 1)\n",
    "\n",
    "def songs_to_sequences(songs, augmentation_count = None):\n",
    "    \"\"\"Converts the sequences within songs to vector format (an 89 element NumPy Ndarray \n",
    "    where the last element is the normalized duration (in quarter notes), and the rest of \n",
    "    the elements are 1 for key on and 0 for key off.)\"\"\"\n",
    "    sequences = []\n",
    "    indices = None\n",
    "    for song in songs:\n",
    "        sequence = []\n",
    "        durations = []\n",
    "        for element in song[0]:\n",
    "            vector = np.zeros(n_keys_piano)  # The current boolean array with which keys\n",
    "                                             # are being pressed. Will add an additional\n",
    "                                             # 89th element for the duration (normalized\n",
    "                                             # to be the fraction of this notes duration\n",
    "                                             # to that of the longest in the file)\n",
    "            cur_note, duration = element  # in units of quarter-notes\n",
    "            if ('.' in cur_note): # chord\n",
    "                notes = cur_note.split('.')\n",
    "                for cur_note in notes:   \n",
    "                    vector[note_to_piano_idx(cur_note)] = 1   # chords are already formatted with piano index\n",
    "            elif (cur_note != 'rest'): # a note\n",
    "                vector[note_to_piano_idx(cur_note)] = 1\n",
    "            sequence.append(vector)\n",
    "            durations.append(float(duration))\n",
    "        durations /= np.max(durations)  # normalize durations to the maximum in the song\n",
    "        sequence = np.array(sequence)\n",
    "        sequence = np.insert(sequence, len(sequence[0]), durations, axis = 1)\n",
    "        if (augmentation_count):   # Perform data augmentation as in Hewahi, AlSaigal, and AlJanahi 2019\n",
    "            transpositions = np.random.permutation(11)[:augmentation_count] \n",
    "            transpositions = np.insert(transpositions, 0, 0)\n",
    "            # 11 possible key transpositions\n",
    "            for transposition in transpositions:\n",
    "                sequences.append(transpose_sequence(sequence, transposition))\n",
    "        else:\n",
    "            sequences.append(sequence)        \n",
    "                              \n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call songs_to_sequences on (notes_by_song)\n",
    "np.set_printoptions(threshold=maxsize)\n",
    "chopin_sequences = songs_to_sequences(notes_by_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to compactify multiple rests occurring in sequence, as well\n",
    "# as remove rests of zero duration\n",
    "\n",
    "def compactify_sequences(sequences):\n",
    "    compacted_sequences = []\n",
    "    for sequence in sequences:\n",
    "        total_duration = 0\n",
    "        song_sequences = []\n",
    "        last_was_rest = False # bool to control the updates to total_duration\n",
    "                              # and appends to song_sequences\n",
    "        for vector in sequence:\n",
    "            if (vector[:-1].sum() == 0): # rest\n",
    "                if (last_was_rest):\n",
    "                    total_duration += vector[-1]\n",
    "                else:\n",
    "                    total_duration = vector[-1]\n",
    "                    last_was_rest = True\n",
    "            else:\n",
    "                if (last_was_rest):\n",
    "                    if (total_duration != 0):  # ignore rests of zero duration\n",
    "                        song_sequences.append(np.concatenate((np.zeros(88), [total_duration])))\n",
    "                    last_was_rest = False\n",
    "                song_sequences.append(vector)\n",
    "        compacted_sequences.append(song_sequences)\n",
    "    \n",
    "    return compacted_sequences      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 627 entries for index: 3\n",
      "After: 622 entries for index: 3\n"
     ]
    }
   ],
   "source": [
    "print('Before: {} entries for index: {}'.format(len(chopin_sequences[index]), index))\n",
    "compacted_sequences = compactify_sequences(chopin_sequences)\n",
    "print('After: {} entries for index: {}'.format(len(compacted_sequences[index]), index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, let's use the keys to transpose\n",
    "def transpose_sequences(sequences, keys_by_song):\n",
    "    transposed_sequences = []  # All sequences transposed to the key of C major\n",
    "    for i in range(len(sequences)):\n",
    "        if (keys_by_song[i][0] == 'C'):\n",
    "            transposed_sequences.append(sequences[i])\n",
    "        else:\n",
    "            notes, durations = sequences[i][:, :-1], sequences[i][:, -1]\n",
    "            transposition = ord('C') - ord(keys_by_song[i][0])\n",
    "            transposed_sequence = transpose_sequence(notes, transposition)\n",
    "            transposed_sequences.append(np.insert(transposed_sequence,\\\n",
    "                                                n_keys_piano, durations, axis = 1))   \n",
    "    return transposed_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_chopin_sequences = transpose_sequences(chopin_sequences, keys_by_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now would be a good time to convert some sequences back into MIDI format and listen to them\n",
    "# We can make sure the timing is the same (relative) between the original and the new, and\n",
    "# confirm that the transpose worked\n",
    "\n",
    "def convert_to_midi(sequence, output_file = 'music.mid'):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    all_notes = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#']\n",
    "\n",
    "    # create note, chord, and rest objects\n",
    "    for vector in sequence:\n",
    "        converted_duration = duration.Duration()\n",
    "        converted_duration.quarterLength = vector[-1]     # conveted from seconds, assuming \n",
    "                                                          #  bpm = 60 (so beats are eqaul to seconds)\n",
    "        if (np.sum(vector[:-1]) > 1):  # chord\n",
    "            indices_in_chord = np.argsort(vector[:-1])[-int(np.sum(vector[:-1])):]\n",
    "            notes_in_chord = [all_notes[i % 12] + str((i // 12) + 1) for i in indices_in_chord]\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(current_note)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            new_chord.duration = converted_duration\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        elif (np.sum(vector[:-1]) == 1):   # note\n",
    "            index = np.argmax(vector[:-1])\n",
    "            new_note = all_notes[index % 12] + str((index // 12) + 1)\n",
    "            new_note = note.Note(new_note)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            new_note.duration = converted_duration\n",
    "            output_notes.append(new_note)\n",
    "        \n",
    "        elif (np.sum(vector[:-1]) == 0):   # rest\n",
    "            new_rest = note.Rest()\n",
    "            new_rest.offset = offset\n",
    "            new_rest.duration = converted_duration\n",
    "            output_notes.append(new_rest)\n",
    "        offset += vector[-1]\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp = output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(chopin_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this midi file is exactly the first few bars of the first imported song (but transposed to C).\n",
    "\n",
    "Now to apply a window function to get the data into the shape needed for LSTM input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_inputs(sequences, window_size = 16):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(sequences)):\n",
    "        if (len(sequences[i]) < window_size + 1):\n",
    "            print(\"Skipping index \", i, \" because the song is too short. Try a shorter window_size to include it.\")\n",
    "            continue\n",
    "        for j in range(len(sequences[i]) - window_size - 1):\n",
    "            X.append(sequences[i][j:j + window_size])\n",
    "            y.append(sequences[i][j + window_size + 1])\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sequences_to_inputs(transposed_chopin_sequences)\n",
    "\n",
    "# let's shuffle these inputs\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20241, 89)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20241, 16, 89)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230585600"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(n_lstm_layers = 4, n_dense_layers = 3, n_lstm_nodes = 512, dropout_rate = 0.6, leaky_alpha = None):\n",
    "    model = Sequential()\n",
    "    for i in range(n_lstm_layers - 1):\n",
    "        model.add(LSTM(n_lstm_nodes, return_sequences = True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(n_lstm_nodes))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_lstm_nodes // 2))\n",
    "    if (leaky_alpha):\n",
    "        model.add(LeakyReLU(alpha = leaky_alpha))   # default is 0.3\n",
    "    else:\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(n_dense_layers - 1):\n",
    "        model.add(Dense(n_lstm_nodes // 2))\n",
    "        model.add(Dropout(0.6))\n",
    "    model.add(Dense(89))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'RMSProp', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14168, 16, 89)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm(n_lstm_layers = 3, n_dense_layers = 2, n_lstm_nodes = 512, dropout_rate = 0.4)\n",
    "mc = ModelCheckpoint('models/chopin/best_chopin_model_3_2_512_pt4.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12283, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 156s - loss: 0.2324 - accuracy: 0.0148 - val_loss: 0.1228 - val_accuracy: 0.0141\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.12283\n",
      "29/29 - 140s - loss: 0.1446 - accuracy: 0.0172 - val_loss: 0.1243 - val_accuracy: 0.0141\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12283 to 0.11949, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 141s - loss: 0.1353 - accuracy: 0.0152 - val_loss: 0.1195 - val_accuracy: 0.0141\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11949\n",
      "29/29 - 146s - loss: 0.1304 - accuracy: 0.0159 - val_loss: 0.1260 - val_accuracy: 0.0141\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11949\n",
      "29/29 - 143s - loss: 0.1288 - accuracy: 0.0141 - val_loss: 0.1210 - val_accuracy: 0.0141\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11949\n",
      "29/29 - 141s - loss: 0.1264 - accuracy: 0.0143 - val_loss: 0.1197 - val_accuracy: 0.0141\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11949 to 0.11793, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 147s - loss: 0.1253 - accuracy: 0.0139 - val_loss: 0.1179 - val_accuracy: 0.0141\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11793\n",
      "29/29 - 134s - loss: 0.1272 - accuracy: 0.0133 - val_loss: 0.1183 - val_accuracy: 0.0141\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11793 to 0.11670, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1235 - accuracy: 0.0132 - val_loss: 0.1167 - val_accuracy: 0.0141\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11670 to 0.11670, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1225 - accuracy: 0.0135 - val_loss: 0.1167 - val_accuracy: 0.0141\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11670 to 0.11503, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1217 - accuracy: 0.0134 - val_loss: 0.1150 - val_accuracy: 0.0141\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11503\n",
      "29/29 - 136s - loss: 0.1205 - accuracy: 0.0154 - val_loss: 0.1197 - val_accuracy: 0.0141\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11503 to 0.11433, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1190 - accuracy: 0.0165 - val_loss: 0.1143 - val_accuracy: 0.0141\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11433 to 0.11298, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1176 - accuracy: 0.0169 - val_loss: 0.1130 - val_accuracy: 0.0146\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11298 to 0.11250, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1165 - accuracy: 0.0188 - val_loss: 0.1125 - val_accuracy: 0.0163\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11250 to 0.11044, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1149 - accuracy: 0.0179 - val_loss: 0.1104 - val_accuracy: 0.0167\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11044 to 0.10938, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1140 - accuracy: 0.0203 - val_loss: 0.1094 - val_accuracy: 0.0197\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10938 to 0.10897, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 137s - loss: 0.1128 - accuracy: 0.0216 - val_loss: 0.1090 - val_accuracy: 0.0159\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.10897 to 0.10697, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1113 - accuracy: 0.0220 - val_loss: 0.1070 - val_accuracy: 0.0232\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10697 to 0.10656, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1107 - accuracy: 0.0231 - val_loss: 0.1066 - val_accuracy: 0.0216\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10656 to 0.10517, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 134s - loss: 0.1093 - accuracy: 0.0252 - val_loss: 0.1052 - val_accuracy: 0.0232\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10517 to 0.10364, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 137s - loss: 0.1090 - accuracy: 0.0253 - val_loss: 0.1036 - val_accuracy: 0.0288\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10364 to 0.10328, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1078 - accuracy: 0.0284 - val_loss: 0.1033 - val_accuracy: 0.0253\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10328 to 0.10263, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1069 - accuracy: 0.0309 - val_loss: 0.1026 - val_accuracy: 0.0259\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10263\n",
      "29/29 - 137s - loss: 0.1057 - accuracy: 0.0335 - val_loss: 0.1030 - val_accuracy: 0.0338\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10263\n",
      "29/29 - 136s - loss: 0.1051 - accuracy: 0.0386 - val_loss: 0.1035 - val_accuracy: 0.0472\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10263 to 0.10108, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1045 - accuracy: 0.0376 - val_loss: 0.1011 - val_accuracy: 0.0392\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10108 to 0.10007, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1036 - accuracy: 0.0419 - val_loss: 0.1001 - val_accuracy: 0.0352\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10007 to 0.09933, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1027 - accuracy: 0.0414 - val_loss: 0.0993 - val_accuracy: 0.0460\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09933 to 0.09863, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1022 - accuracy: 0.0419 - val_loss: 0.0986 - val_accuracy: 0.0445\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09863\n",
      "29/29 - 137s - loss: 0.1015 - accuracy: 0.0497 - val_loss: 0.0993 - val_accuracy: 0.0464\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09863 to 0.09816, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1013 - accuracy: 0.0485 - val_loss: 0.0982 - val_accuracy: 0.0589\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09816 to 0.09735, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1007 - accuracy: 0.0513 - val_loss: 0.0973 - val_accuracy: 0.0551\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09735 to 0.09729, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 137s - loss: 0.0995 - accuracy: 0.0535 - val_loss: 0.0973 - val_accuracy: 0.0575\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09729\n",
      "29/29 - 138s - loss: 0.0995 - accuracy: 0.0556 - val_loss: 0.0974 - val_accuracy: 0.0477\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09729 to 0.09657, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 134s - loss: 0.0989 - accuracy: 0.0542 - val_loss: 0.0966 - val_accuracy: 0.0621\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09657 to 0.09543, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.0983 - accuracy: 0.0590 - val_loss: 0.0954 - val_accuracy: 0.0584\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09543 to 0.09533, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0978 - accuracy: 0.0601 - val_loss: 0.0953 - val_accuracy: 0.0583\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09533\n",
      "29/29 - 135s - loss: 0.0972 - accuracy: 0.0626 - val_loss: 0.0956 - val_accuracy: 0.0700\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09533 to 0.09485, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0965 - accuracy: 0.0679 - val_loss: 0.0949 - val_accuracy: 0.0645\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09485 to 0.09482, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 137s - loss: 0.0963 - accuracy: 0.0660 - val_loss: 0.0948 - val_accuracy: 0.0578\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09482 to 0.09399, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.0957 - accuracy: 0.0734 - val_loss: 0.0940 - val_accuracy: 0.0655\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09399 to 0.09394, saving model to best_chopin_model_3_2_512_pt4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 140s - loss: 0.0953 - accuracy: 0.0705 - val_loss: 0.0939 - val_accuracy: 0.0655\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09394\n",
      "29/29 - 136s - loss: 0.0947 - accuracy: 0.0746 - val_loss: 0.0940 - val_accuracy: 0.0721\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09394 to 0.09316, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.0943 - accuracy: 0.0747 - val_loss: 0.0932 - val_accuracy: 0.0698\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09316\n",
      "29/29 - 135s - loss: 0.0938 - accuracy: 0.0787 - val_loss: 0.0934 - val_accuracy: 0.0829\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09316\n",
      "29/29 - 143s - loss: 0.0933 - accuracy: 0.0859 - val_loss: 0.0933 - val_accuracy: 0.0764\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09316 to 0.09295, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 151s - loss: 0.0928 - accuracy: 0.0872 - val_loss: 0.0930 - val_accuracy: 0.0663\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09295\n",
      "29/29 - 135s - loss: 0.0923 - accuracy: 0.0894 - val_loss: 0.0930 - val_accuracy: 0.0869\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09295 to 0.09193, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0915 - accuracy: 0.0924 - val_loss: 0.0919 - val_accuracy: 0.1009\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 50, \\\n",
    "                    validation_data = (X_test, y_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.09193\n",
      "29/29 - 135s - loss: 0.0911 - accuracy: 0.1010 - val_loss: 0.0923 - val_accuracy: 0.0980\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09193 to 0.09156, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0908 - accuracy: 0.1016 - val_loss: 0.0916 - val_accuracy: 0.0991\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.09156\n",
      "29/29 - 133s - loss: 0.0900 - accuracy: 0.1080 - val_loss: 0.0923 - val_accuracy: 0.0866\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09156 to 0.09138, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 134s - loss: 0.0894 - accuracy: 0.1102 - val_loss: 0.0914 - val_accuracy: 0.0945\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09138\n",
      "29/29 - 137s - loss: 0.0892 - accuracy: 0.1124 - val_loss: 0.0917 - val_accuracy: 0.1028\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09138\n",
      "29/29 - 134s - loss: 0.0883 - accuracy: 0.1166 - val_loss: 0.0918 - val_accuracy: 0.1022\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09138 to 0.09108, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0876 - accuracy: 0.1219 - val_loss: 0.0911 - val_accuracy: 0.1098\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09108 to 0.09038, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 134s - loss: 0.0868 - accuracy: 0.1317 - val_loss: 0.0904 - val_accuracy: 0.1081\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09038\n",
      "29/29 - 134s - loss: 0.0862 - accuracy: 0.1325 - val_loss: 0.0909 - val_accuracy: 0.1159\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09038 to 0.09018, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0858 - accuracy: 0.1397 - val_loss: 0.0902 - val_accuracy: 0.1142\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09018\n",
      "29/29 - 136s - loss: 0.0852 - accuracy: 0.1432 - val_loss: 0.0922 - val_accuracy: 0.1302\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09018 to 0.09007, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0845 - accuracy: 0.1480 - val_loss: 0.0901 - val_accuracy: 0.1196\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09007\n",
      "29/29 - 135s - loss: 0.0839 - accuracy: 0.1515 - val_loss: 0.0904 - val_accuracy: 0.1231\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09007\n",
      "29/29 - 136s - loss: 0.0835 - accuracy: 0.1549 - val_loss: 0.0908 - val_accuracy: 0.1249\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09007 to 0.08950, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0824 - accuracy: 0.1616 - val_loss: 0.0895 - val_accuracy: 0.1281\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08950\n",
      "29/29 - 134s - loss: 0.0819 - accuracy: 0.1658 - val_loss: 0.0898 - val_accuracy: 0.1295\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08950\n",
      "29/29 - 135s - loss: 0.0813 - accuracy: 0.1720 - val_loss: 0.0903 - val_accuracy: 0.1289\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08950\n",
      "29/29 - 136s - loss: 0.0806 - accuracy: 0.1767 - val_loss: 0.0912 - val_accuracy: 0.1299\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08950\n",
      "29/29 - 137s - loss: 0.0799 - accuracy: 0.1794 - val_loss: 0.0902 - val_accuracy: 0.1372\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08950\n",
      "29/29 - 135s - loss: 0.0793 - accuracy: 0.1885 - val_loss: 0.0907 - val_accuracy: 0.1384\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 20, \\\n",
    "                    validation_data = (X_test, y_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Next, we'll see if adding more layers and dropout (effectively using the same hyperparameters as in the best model from Hewahi, AlSaigal, and AlJanahi et al. 2019 ([link](https://www.tandfonline.com/doi/pdf/10.1080/25765299.2019.1649972?needAccess=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm(n_lstm_layers = 4, n_dense_layers = 3, n_lstm_nodes = 512, dropout_rate = 0.6)\n",
    "mc = ModelCheckpoint('models/chopin/best_chopin_model_recreate.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13469, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 206s - loss: 0.2863 - accuracy: 0.0139 - val_loss: 0.1347 - val_accuracy: 0.0141\n",
      "Epoch 2/75\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13469 to 0.12133, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 188s - loss: 0.1558 - accuracy: 0.0166 - val_loss: 0.1213 - val_accuracy: 0.0141\n",
      "Epoch 3/75\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12133\n",
      "29/29 - 186s - loss: 0.1417 - accuracy: 0.0165 - val_loss: 0.1636 - val_accuracy: 0.0141\n",
      "Epoch 4/75\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12133\n",
      "29/29 - 190s - loss: 0.1369 - accuracy: 0.0150 - val_loss: 0.1378 - val_accuracy: 0.0141\n",
      "Epoch 5/75\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12133\n",
      "29/29 - 186s - loss: 0.1299 - accuracy: 0.0126 - val_loss: 0.1279 - val_accuracy: 0.0141\n",
      "Epoch 6/75\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12133\n",
      "29/29 - 188s - loss: 0.1268 - accuracy: 0.0137 - val_loss: 0.1235 - val_accuracy: 0.0141\n",
      "Epoch 7/75\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12133\n",
      "29/29 - 186s - loss: 0.1251 - accuracy: 0.0133 - val_loss: 0.1215 - val_accuracy: 0.0141\n",
      "Epoch 8/75\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12133 to 0.12060, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 188s - loss: 0.1236 - accuracy: 0.0132 - val_loss: 0.1206 - val_accuracy: 0.0141\n",
      "Epoch 9/75\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12060\n",
      "29/29 - 186s - loss: 0.1232 - accuracy: 0.0132 - val_loss: 0.1209 - val_accuracy: 0.0141\n",
      "Epoch 10/75\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12060 to 0.12014, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 186s - loss: 0.1226 - accuracy: 0.0132 - val_loss: 0.1201 - val_accuracy: 0.0141\n",
      "Epoch 11/75\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12014 to 0.11906, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 188s - loss: 0.1220 - accuracy: 0.0132 - val_loss: 0.1191 - val_accuracy: 0.0141\n",
      "Epoch 12/75\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11906\n",
      "29/29 - 187s - loss: 0.1214 - accuracy: 0.0132 - val_loss: 0.1207 - val_accuracy: 0.0141\n",
      "Epoch 13/75\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11906 to 0.11797, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 187s - loss: 0.1212 - accuracy: 0.0132 - val_loss: 0.1180 - val_accuracy: 0.0141\n",
      "Epoch 14/75\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11797\n",
      "29/29 - 186s - loss: 0.1210 - accuracy: 0.0132 - val_loss: 0.1184 - val_accuracy: 0.0141\n",
      "Epoch 15/75\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11797\n",
      "29/29 - 185s - loss: 0.1984 - accuracy: 0.0133 - val_loss: 0.1188 - val_accuracy: 0.0141\n",
      "Epoch 16/75\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11797\n",
      "29/29 - 185s - loss: 0.1197 - accuracy: 0.0132 - val_loss: 0.1185 - val_accuracy: 0.0141\n",
      "Epoch 17/75\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11797 to 0.11764, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 189s - loss: 0.1197 - accuracy: 0.0132 - val_loss: 0.1176 - val_accuracy: 0.0141\n",
      "Epoch 18/75\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11764\n",
      "29/29 - 189s - loss: 0.1198 - accuracy: 0.0132 - val_loss: 0.1178 - val_accuracy: 0.0141\n",
      "Epoch 19/75\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11764\n",
      "29/29 - 186s - loss: 0.1200 - accuracy: 0.0132 - val_loss: 0.1183 - val_accuracy: 0.0141\n",
      "Epoch 20/75\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11764\n",
      "29/29 - 189s - loss: 0.1201 - accuracy: 0.0132 - val_loss: 0.1178 - val_accuracy: 0.0141\n",
      "Epoch 21/75\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11764\n",
      "29/29 - 187s - loss: 0.1197 - accuracy: 0.0132 - val_loss: 0.1187 - val_accuracy: 0.0141\n",
      "Epoch 22/75\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11764 to 0.11748, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 188s - loss: 0.1198 - accuracy: 0.0132 - val_loss: 0.1175 - val_accuracy: 0.0141\n",
      "Epoch 23/75\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.11748\n",
      "29/29 - 189s - loss: 0.1193 - accuracy: 0.0132 - val_loss: 0.1181 - val_accuracy: 0.0141\n",
      "Epoch 24/75\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11748\n",
      "29/29 - 188s - loss: 0.1192 - accuracy: 0.0132 - val_loss: 0.1176 - val_accuracy: 0.0141\n",
      "Epoch 25/75\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11748\n",
      "29/29 - 187s - loss: 0.1191 - accuracy: 0.0134 - val_loss: 0.1181 - val_accuracy: 0.0141\n",
      "Epoch 26/75\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.11748 to 0.11702, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 189s - loss: 0.1191 - accuracy: 0.0132 - val_loss: 0.1170 - val_accuracy: 0.0141\n",
      "Epoch 27/75\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.11702 to 0.11687, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 188s - loss: 0.1188 - accuracy: 0.0132 - val_loss: 0.1169 - val_accuracy: 0.0141\n",
      "Epoch 28/75\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11687\n",
      "29/29 - 188s - loss: 0.1189 - accuracy: 0.0134 - val_loss: 0.1173 - val_accuracy: 0.0141\n",
      "Epoch 29/75\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.11687 to 0.11674, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 189s - loss: 0.1186 - accuracy: 0.0133 - val_loss: 0.1167 - val_accuracy: 0.0141\n",
      "Epoch 30/75\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11674\n",
      "29/29 - 188s - loss: 0.1185 - accuracy: 0.0134 - val_loss: 0.1177 - val_accuracy: 0.0141\n",
      "Epoch 31/75\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11674\n",
      "29/29 - 191s - loss: 0.1183 - accuracy: 0.0135 - val_loss: 0.1171 - val_accuracy: 0.0141\n",
      "Epoch 32/75\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11674\n",
      "29/29 - 188s - loss: 0.1183 - accuracy: 0.0133 - val_loss: 0.1171 - val_accuracy: 0.0141\n",
      "Epoch 33/75\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11674\n",
      "29/29 - 187s - loss: 0.1182 - accuracy: 0.0135 - val_loss: 0.1168 - val_accuracy: 0.0141\n",
      "Epoch 34/75\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11674\n",
      "29/29 - 188s - loss: 0.1181 - accuracy: 0.0133 - val_loss: 0.1170 - val_accuracy: 0.0141\n",
      "Epoch 35/75\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11674\n",
      "29/29 - 187s - loss: 0.1182 - accuracy: 0.0132 - val_loss: 0.1172 - val_accuracy: 0.0141\n",
      "Epoch 36/75\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11674\n",
      "29/29 - 189s - loss: 0.1182 - accuracy: 0.0135 - val_loss: 0.1172 - val_accuracy: 0.0141\n",
      "Epoch 37/75\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.11674 to 0.11627, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 188s - loss: 0.1179 - accuracy: 0.0134 - val_loss: 0.1163 - val_accuracy: 0.0141\n",
      "Epoch 38/75\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11627\n",
      "29/29 - 187s - loss: 0.1176 - accuracy: 0.0135 - val_loss: 0.1164 - val_accuracy: 0.0141\n",
      "Epoch 39/75\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11627\n",
      "29/29 - 187s - loss: 0.1176 - accuracy: 0.0137 - val_loss: 0.1164 - val_accuracy: 0.0141\n",
      "Epoch 40/75\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11627\n",
      "29/29 - 187s - loss: 0.1172 - accuracy: 0.0137 - val_loss: 0.1167 - val_accuracy: 0.0141\n",
      "Epoch 41/75\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.11627 to 0.11522, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 187s - loss: 0.1171 - accuracy: 0.0136 - val_loss: 0.1152 - val_accuracy: 0.0141\n",
      "Epoch 42/75\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11522\n",
      "29/29 - 191s - loss: 0.1172 - accuracy: 0.0141 - val_loss: 0.1156 - val_accuracy: 0.0141\n",
      "Epoch 43/75\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.11522 to 0.11520, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 196s - loss: 0.1167 - accuracy: 0.0142 - val_loss: 0.1152 - val_accuracy: 0.0143\n",
      "Epoch 44/75\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11520\n",
      "29/29 - 184s - loss: 0.1166 - accuracy: 0.0144 - val_loss: 0.1159 - val_accuracy: 0.0141\n",
      "Epoch 45/75\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.11520 to 0.11509, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 187s - loss: 0.1223 - accuracy: 0.0141 - val_loss: 0.1151 - val_accuracy: 0.0154\n",
      "Epoch 46/75\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.11509 to 0.11455, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 186s - loss: 0.1168 - accuracy: 0.0153 - val_loss: 0.1146 - val_accuracy: 0.0141\n",
      "Epoch 47/75\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11455\n",
      "29/29 - 186s - loss: 0.1162 - accuracy: 0.0157 - val_loss: 0.1146 - val_accuracy: 0.0147\n",
      "Epoch 48/75\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11455\n",
      "29/29 - 187s - loss: 0.1160 - accuracy: 0.0157 - val_loss: 0.1148 - val_accuracy: 0.0154\n",
      "Epoch 49/75\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.11455 to 0.11393, saving model to best_chopin_model_recreate.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 190s - loss: 0.1158 - accuracy: 0.0155 - val_loss: 0.1139 - val_accuracy: 0.0151\n",
      "Epoch 50/75\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11393\n",
      "29/29 - 185s - loss: 0.1156 - accuracy: 0.0157 - val_loss: 0.1147 - val_accuracy: 0.0141\n",
      "Epoch 51/75\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11393\n",
      "29/29 - 186s - loss: 0.1158 - accuracy: 0.0159 - val_loss: 0.1149 - val_accuracy: 0.0141\n",
      "Epoch 52/75\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11393\n",
      "29/29 - 186s - loss: 0.1153 - accuracy: 0.0158 - val_loss: 0.1141 - val_accuracy: 0.0147\n",
      "Epoch 53/75\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11393\n",
      "29/29 - 188s - loss: 0.1151 - accuracy: 0.0149 - val_loss: 0.1140 - val_accuracy: 0.0147\n",
      "Epoch 54/75\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11393\n",
      "29/29 - 188s - loss: 0.1153 - accuracy: 0.0152 - val_loss: 0.1152 - val_accuracy: 0.0141\n",
      "Epoch 55/75\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11393\n",
      "29/29 - 188s - loss: 0.1157 - accuracy: 0.0154 - val_loss: 0.1141 - val_accuracy: 0.0151\n",
      "Epoch 56/75\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.11393 to 0.11341, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 189s - loss: 0.1151 - accuracy: 0.0166 - val_loss: 0.1134 - val_accuracy: 0.0157\n",
      "Epoch 57/75\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.11341 to 0.11334, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 189s - loss: 0.1147 - accuracy: 0.0169 - val_loss: 0.1133 - val_accuracy: 0.0155\n",
      "Epoch 58/75\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.11334 to 0.11212, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 187s - loss: 0.1144 - accuracy: 0.0178 - val_loss: 0.1121 - val_accuracy: 0.0186\n",
      "Epoch 59/75\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11212\n",
      "29/29 - 188s - loss: 0.1145 - accuracy: 0.0170 - val_loss: 0.1147 - val_accuracy: 0.0183\n",
      "Epoch 60/75\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11212\n",
      "29/29 - 189s - loss: 0.1143 - accuracy: 0.0171 - val_loss: 0.1125 - val_accuracy: 0.0181\n",
      "Epoch 61/75\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.11212\n",
      "29/29 - 186s - loss: 0.1139 - accuracy: 0.0177 - val_loss: 0.1128 - val_accuracy: 0.0165\n",
      "Epoch 62/75\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.11212\n",
      "29/29 - 187s - loss: 0.1138 - accuracy: 0.0166 - val_loss: 0.1124 - val_accuracy: 0.0176\n",
      "Epoch 63/75\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.11212 to 0.11190, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 188s - loss: 0.1137 - accuracy: 0.0170 - val_loss: 0.1119 - val_accuracy: 0.0147\n",
      "Epoch 64/75\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.11190\n",
      "29/29 - 186s - loss: 0.1133 - accuracy: 0.0178 - val_loss: 0.1138 - val_accuracy: 0.0144\n",
      "Epoch 65/75\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.11190 to 0.11075, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 200s - loss: 0.1132 - accuracy: 0.0167 - val_loss: 0.1107 - val_accuracy: 0.0189\n",
      "Epoch 66/75\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.11075\n",
      "29/29 - 196s - loss: 0.1131 - accuracy: 0.0169 - val_loss: 0.1113 - val_accuracy: 0.0183\n",
      "Epoch 67/75\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.11075\n",
      "29/29 - 188s - loss: 0.1130 - accuracy: 0.0176 - val_loss: 0.1115 - val_accuracy: 0.0159\n",
      "Epoch 68/75\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.11075 to 0.11066, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 186s - loss: 0.1128 - accuracy: 0.0167 - val_loss: 0.1107 - val_accuracy: 0.0173\n",
      "Epoch 69/75\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.11066\n",
      "29/29 - 185s - loss: 0.1131 - accuracy: 0.0165 - val_loss: 0.1118 - val_accuracy: 0.0157\n",
      "Epoch 70/75\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.11066\n",
      "29/29 - 185s - loss: 0.1128 - accuracy: 0.0169 - val_loss: 0.1116 - val_accuracy: 0.0175\n",
      "Epoch 71/75\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.11066\n",
      "29/29 - 185s - loss: 0.1126 - accuracy: 0.0168 - val_loss: 0.1114 - val_accuracy: 0.0195\n",
      "Epoch 72/75\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.11066\n",
      "29/29 - 185s - loss: 0.1122 - accuracy: 0.0176 - val_loss: 0.1109 - val_accuracy: 0.0155\n",
      "Epoch 73/75\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.11066\n",
      "29/29 - 188s - loss: 0.1121 - accuracy: 0.0176 - val_loss: 0.1111 - val_accuracy: 0.0168\n",
      "Epoch 74/75\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.11066 to 0.11043, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 187s - loss: 0.1123 - accuracy: 0.0172 - val_loss: 0.1104 - val_accuracy: 0.0147\n",
      "Epoch 75/75\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.11043 to 0.11042, saving model to best_chopin_model_recreate.h5\n",
      "29/29 - 185s - loss: 0.1117 - accuracy: 0.0174 - val_loss: 0.1104 - val_accuracy: 0.0175\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 75, \\\n",
    "                    validation_data = (X_test, y_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still learning but looks like it may be vanishing gradient. Let's see what happens when we take the last model and apply it to the longer window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_32, y_32 = sequences_to_inputs(transposed_chopin_sequences, window_size = 32)\n",
    "\n",
    "# let's shuffle these inputs\n",
    "X_32, y_32 = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_32_train, X_32_test, y_32_train, y_32_test = train_test_split(X_32, y_32, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm(n_lstm_layers = 3, n_dense_layers = 2, n_lstm_nodes = 512, dropout_rate = 0.4)\n",
    "mc = ModelCheckpoint('models/chopin/best_chopin_model_32_3_2_512_pt4.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12155, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 155s - loss: 0.2410 - accuracy: 0.0146 - val_loss: 0.1216 - val_accuracy: 0.0131\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12155 to 0.11923, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 137s - loss: 0.1441 - accuracy: 0.0170 - val_loss: 0.1192 - val_accuracy: 0.0131\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11923\n",
      "29/29 - 135s - loss: 0.1340 - accuracy: 0.0179 - val_loss: 0.1234 - val_accuracy: 0.0131\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11923\n",
      "29/29 - 136s - loss: 0.1294 - accuracy: 0.0148 - val_loss: 0.1204 - val_accuracy: 0.0131\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11923 to 0.11754, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 134s - loss: 0.1272 - accuracy: 0.0146 - val_loss: 0.1175 - val_accuracy: 0.0131\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11754\n",
      "29/29 - 137s - loss: 0.1254 - accuracy: 0.0147 - val_loss: 0.1184 - val_accuracy: 0.0131\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11754\n",
      "29/29 - 135s - loss: 0.1239 - accuracy: 0.0143 - val_loss: 0.1189 - val_accuracy: 0.0131\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11754 to 0.11723, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1226 - accuracy: 0.0139 - val_loss: 0.1172 - val_accuracy: 0.0131\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11723 to 0.11484, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1232 - accuracy: 0.0139 - val_loss: 0.1148 - val_accuracy: 0.0131\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11484\n",
      "29/29 - 137s - loss: 0.1202 - accuracy: 0.0146 - val_loss: 0.1185 - val_accuracy: 0.0131\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11484\n",
      "29/29 - 135s - loss: 0.1199 - accuracy: 0.0148 - val_loss: 0.1230 - val_accuracy: 0.0131\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11484 to 0.11459, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 134s - loss: 0.1187 - accuracy: 0.0155 - val_loss: 0.1146 - val_accuracy: 0.0131\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.11459 to 0.11446, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1181 - accuracy: 0.0172 - val_loss: 0.1145 - val_accuracy: 0.0131\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11446 to 0.11364, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1170 - accuracy: 0.0167 - val_loss: 0.1136 - val_accuracy: 0.0130\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11364 to 0.11106, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1163 - accuracy: 0.0178 - val_loss: 0.1111 - val_accuracy: 0.0130\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11106 to 0.10988, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1148 - accuracy: 0.0159 - val_loss: 0.1099 - val_accuracy: 0.0144\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10988\n",
      "29/29 - 136s - loss: 0.1142 - accuracy: 0.0166 - val_loss: 0.1103 - val_accuracy: 0.0189\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10988\n",
      "29/29 - 135s - loss: 0.1132 - accuracy: 0.0161 - val_loss: 0.1114 - val_accuracy: 0.0170\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.10988 to 0.10919, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1126 - accuracy: 0.0160 - val_loss: 0.1092 - val_accuracy: 0.0130\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10919 to 0.10812, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 137s - loss: 0.1121 - accuracy: 0.0164 - val_loss: 0.1081 - val_accuracy: 0.0130\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10812\n",
      "29/29 - 135s - loss: 0.1104 - accuracy: 0.0177 - val_loss: 0.1082 - val_accuracy: 0.0167\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.10812 to 0.10606, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1099 - accuracy: 0.0178 - val_loss: 0.1061 - val_accuracy: 0.0179\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10606 to 0.10532, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 136s - loss: 0.1090 - accuracy: 0.0183 - val_loss: 0.1053 - val_accuracy: 0.0151\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10532\n",
      "29/29 - 135s - loss: 0.1088 - accuracy: 0.0192 - val_loss: 0.1057 - val_accuracy: 0.0186\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10532\n",
      "29/29 - 136s - loss: 0.1077 - accuracy: 0.0210 - val_loss: 0.1075 - val_accuracy: 0.0171\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10532 to 0.10386, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1070 - accuracy: 0.0207 - val_loss: 0.1039 - val_accuracy: 0.0207\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10386 to 0.10349, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 144s - loss: 0.1061 - accuracy: 0.0237 - val_loss: 0.1035 - val_accuracy: 0.0261\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10349 to 0.10260, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1056 - accuracy: 0.0241 - val_loss: 0.1026 - val_accuracy: 0.0218\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10260 to 0.10157, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.1049 - accuracy: 0.0277 - val_loss: 0.1016 - val_accuracy: 0.0157\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10157 to 0.10105, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 140s - loss: 0.1041 - accuracy: 0.0307 - val_loss: 0.1010 - val_accuracy: 0.0223\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10105\n",
      "29/29 - 135s - loss: 0.1033 - accuracy: 0.0321 - val_loss: 0.1011 - val_accuracy: 0.0327\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10105 to 0.09921, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 138s - loss: 0.1029 - accuracy: 0.0351 - val_loss: 0.0992 - val_accuracy: 0.0293\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09921\n",
      "29/29 - 136s - loss: 0.1016 - accuracy: 0.0387 - val_loss: 0.0999 - val_accuracy: 0.0304\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09921 to 0.09880, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 150s - loss: 0.1014 - accuracy: 0.0379 - val_loss: 0.0988 - val_accuracy: 0.0322\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09880\n",
      "29/29 - 162s - loss: 0.1011 - accuracy: 0.0389 - val_loss: 0.0989 - val_accuracy: 0.0320\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09880 to 0.09845, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 170s - loss: 0.1010 - accuracy: 0.0399 - val_loss: 0.0984 - val_accuracy: 0.0309\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09845 to 0.09794, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 171s - loss: 0.1003 - accuracy: 0.0441 - val_loss: 0.0979 - val_accuracy: 0.0287\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09794 to 0.09767, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 152s - loss: 0.1002 - accuracy: 0.0412 - val_loss: 0.0977 - val_accuracy: 0.0384\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09767\n",
      "29/29 - 135s - loss: 0.0995 - accuracy: 0.0428 - val_loss: 0.0980 - val_accuracy: 0.0389\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09767 to 0.09726, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 135s - loss: 0.0993 - accuracy: 0.0487 - val_loss: 0.0973 - val_accuracy: 0.0528\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09726 to 0.09666, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 137s - loss: 0.0990 - accuracy: 0.0492 - val_loss: 0.0967 - val_accuracy: 0.0431\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09666 to 0.09635, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 188s - loss: 0.0986 - accuracy: 0.0537 - val_loss: 0.0963 - val_accuracy: 0.0490\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09635 to 0.09595, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 192s - loss: 0.0978 - accuracy: 0.0540 - val_loss: 0.0959 - val_accuracy: 0.0568\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09595 to 0.09549, saving model to best_chopin_model_3_2_512_pt4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 335s - loss: 0.0977 - accuracy: 0.0520 - val_loss: 0.0955 - val_accuracy: 0.0476\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09549\n",
      "29/29 - 251s - loss: 0.0974 - accuracy: 0.0524 - val_loss: 0.0956 - val_accuracy: 0.0488\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09549\n",
      "29/29 - 217s - loss: 0.0969 - accuracy: 0.0562 - val_loss: 0.0961 - val_accuracy: 0.0600\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09549 to 0.09495, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 222s - loss: 0.0964 - accuracy: 0.0605 - val_loss: 0.0949 - val_accuracy: 0.0552\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09495 to 0.09458, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 210s - loss: 0.0959 - accuracy: 0.0593 - val_loss: 0.0946 - val_accuracy: 0.0520\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09458\n",
      "29/29 - 205s - loss: 0.0955 - accuracy: 0.0595 - val_loss: 0.0954 - val_accuracy: 0.0652\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09458\n",
      "29/29 - 194s - loss: 0.0950 - accuracy: 0.0636 - val_loss: 0.0949 - val_accuracy: 0.0663\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_32_train, y_32_train, batch_size = 512, epochs = 50, \\\n",
    "                    validation_data = (X_32_test, y_32_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.09458\n",
      "29/29 - 178s - loss: 0.0950 - accuracy: 0.0638 - val_loss: 0.0947 - val_accuracy: 0.0628\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09458 to 0.09324, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 178s - loss: 0.0943 - accuracy: 0.0688 - val_loss: 0.0932 - val_accuracy: 0.0741\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.09324\n",
      "29/29 - 167s - loss: 0.0938 - accuracy: 0.0706 - val_loss: 0.0936 - val_accuracy: 0.0605\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09324\n",
      "29/29 - 180s - loss: 0.0936 - accuracy: 0.0710 - val_loss: 0.0936 - val_accuracy: 0.0674\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09324 to 0.09277, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 178s - loss: 0.0930 - accuracy: 0.0763 - val_loss: 0.0928 - val_accuracy: 0.0748\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09277 to 0.09227, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 175s - loss: 0.0923 - accuracy: 0.0801 - val_loss: 0.0923 - val_accuracy: 0.0754\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09227\n",
      "29/29 - 174s - loss: 0.0917 - accuracy: 0.0850 - val_loss: 0.0927 - val_accuracy: 0.0786\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09227 to 0.09180, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 176s - loss: 0.0916 - accuracy: 0.0849 - val_loss: 0.0918 - val_accuracy: 0.0772\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09180\n",
      "29/29 - 176s - loss: 0.0910 - accuracy: 0.0870 - val_loss: 0.0920 - val_accuracy: 0.0769\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09180 to 0.09135, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 175s - loss: 0.0904 - accuracy: 0.0927 - val_loss: 0.0913 - val_accuracy: 0.0781\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09135\n",
      "29/29 - 171s - loss: 0.0899 - accuracy: 0.0949 - val_loss: 0.0916 - val_accuracy: 0.0770\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09135 to 0.09119, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 179s - loss: 0.0895 - accuracy: 0.0988 - val_loss: 0.0912 - val_accuracy: 0.0850\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09119\n",
      "29/29 - 193s - loss: 0.0888 - accuracy: 0.1032 - val_loss: 0.0919 - val_accuracy: 0.0916\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09119 to 0.09089, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 188s - loss: 0.0884 - accuracy: 0.1049 - val_loss: 0.0909 - val_accuracy: 0.1058\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09089 to 0.09051, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 174s - loss: 0.0878 - accuracy: 0.1113 - val_loss: 0.0905 - val_accuracy: 0.0903\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09051\n",
      "29/29 - 172s - loss: 0.0872 - accuracy: 0.1121 - val_loss: 0.0908 - val_accuracy: 0.1022\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09051\n",
      "29/29 - 176s - loss: 0.0866 - accuracy: 0.1200 - val_loss: 0.0912 - val_accuracy: 0.1087\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09051 to 0.08975, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 177s - loss: 0.0863 - accuracy: 0.1220 - val_loss: 0.0898 - val_accuracy: 0.1025\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08975\n",
      "29/29 - 172s - loss: 0.0857 - accuracy: 0.1218 - val_loss: 0.0907 - val_accuracy: 0.1025\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08975\n",
      "29/29 - 185s - loss: 0.0849 - accuracy: 0.1281 - val_loss: 0.0898 - val_accuracy: 0.1060\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_32_train, y_32_train, batch_size = 512, epochs = 20, \\\n",
    "                    validation_data = (X_32_test, y_32_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.08975 to 0.08878, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 183s - loss: 0.0845 - accuracy: 0.1332 - val_loss: 0.0888 - val_accuracy: 0.1148\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.08878\n",
      "29/29 - 174s - loss: 0.0839 - accuracy: 0.1385 - val_loss: 0.0898 - val_accuracy: 0.1071\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.08878\n",
      "29/29 - 179s - loss: 0.0834 - accuracy: 0.1433 - val_loss: 0.0890 - val_accuracy: 0.1254\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08878 to 0.08871, saving model to best_chopin_model_3_2_512_pt4.h5\n",
      "29/29 - 172s - loss: 0.0828 - accuracy: 0.1443 - val_loss: 0.0887 - val_accuracy: 0.1174\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.08871\n",
      "29/29 - 175s - loss: 0.0822 - accuracy: 0.1491 - val_loss: 0.0894 - val_accuracy: 0.1174\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.08871\n",
      "29/29 - 180s - loss: 0.0813 - accuracy: 0.1554 - val_loss: 0.0898 - val_accuracy: 0.1302\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.08871\n",
      "29/29 - 183s - loss: 0.0808 - accuracy: 0.1622 - val_loss: 0.0893 - val_accuracy: 0.1342\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.08871\n",
      "29/29 - 177s - loss: 0.0804 - accuracy: 0.1633 - val_loss: 0.0895 - val_accuracy: 0.1275\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08871\n",
      "29/29 - 174s - loss: 0.0797 - accuracy: 0.1685 - val_loss: 0.0894 - val_accuracy: 0.1374\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08871\n",
      "29/29 - 172s - loss: 0.0791 - accuracy: 0.1732 - val_loss: 0.0896 - val_accuracy: 0.1329\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-1ac28f5fe932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_32_train, y_32_train, batch_size = 512, epochs = 20, \\\n\u001b[0;32m----> 2\u001b[0;31m                     validation_data = (X_32_test, y_32_test), verbose = 2, callbacks = [mc])\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_32_train, y_32_train, batch_size = 512, epochs = 20, \\\n",
    "                    validation_data = (X_32_test, y_32_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a longer window, we got to a slightly lower loss (.08871 vs .08950) but the training took a bit longer (best after Epoch 74 compared to Epoch 65 before). I am a bit disappointed but I still like a window of 32 better (and possibly even longer) since in music, the memory should last longer than 16 vectors, which at normal BPM (Beats Per Minute) amounts to about a few seconds. \n",
    "\n",
    "I am also surprised by how fast each the training time per Epoch was. It was about the same as before. Why is this this the case?\n",
    "\n",
    "Next, I will try using the Leaky ReLU activation function to combat the vanishing gradient problem. This function has just one parameter, alpha, which controls the multiplier (alpha = 0 is the same as regular ReLU). I will begin with keras' default value of 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying LeakyReLU with default alpha = 0.3 with 4 LSTM and 3 Dense layers again:\n",
    "model = lstm(n_lstm_layers = 4, n_dense_layers = 3, n_lstm_nodes = 512, dropout_rate = 0.6, leaky_alpha = 0.3)\n",
    "mc = ModelCheckpoint('best_chopin_model_LRLpt3_recreate.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15351, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 219s - loss: 0.3453 - accuracy: 0.0140 - val_loss: 0.1535 - val_accuracy: 0.0141\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.15351\n",
      "29/29 - 206s - loss: 0.1595 - accuracy: 0.0181 - val_loss: 0.1536 - val_accuracy: 0.0141\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15351 to 0.12294, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 210s - loss: 0.1437 - accuracy: 0.0173 - val_loss: 0.1229 - val_accuracy: 0.0141\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12294 to 0.12121, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 215s - loss: 0.1350 - accuracy: 0.0146 - val_loss: 0.1212 - val_accuracy: 0.0141\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12121\n",
      "29/29 - 210s - loss: 0.1279 - accuracy: 0.0137 - val_loss: 0.1215 - val_accuracy: 0.0141\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12121 to 0.11992, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 210s - loss: 0.1260 - accuracy: 0.0137 - val_loss: 0.1199 - val_accuracy: 0.0141\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.11992 to 0.11816, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 219s - loss: 0.1252 - accuracy: 0.0135 - val_loss: 0.1182 - val_accuracy: 0.0141\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11816 to 0.11809, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 210s - loss: 0.1235 - accuracy: 0.0134 - val_loss: 0.1181 - val_accuracy: 0.0141\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11809\n",
      "29/29 - 214s - loss: 0.1229 - accuracy: 0.0132 - val_loss: 0.1203 - val_accuracy: 0.0141\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11809 to 0.11794, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 208s - loss: 0.1222 - accuracy: 0.0133 - val_loss: 0.1179 - val_accuracy: 0.0141\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11794 to 0.11793, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 229s - loss: 0.1218 - accuracy: 0.0132 - val_loss: 0.1179 - val_accuracy: 0.0141\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11793\n",
      "29/29 - 172s - loss: 0.1212 - accuracy: 0.0132 - val_loss: 0.1189 - val_accuracy: 0.0141\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11793\n",
      "29/29 - 166s - loss: 0.1227 - accuracy: 0.0132 - val_loss: 0.1196 - val_accuracy: 0.0141\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11793 to 0.11743, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 167s - loss: 0.1209 - accuracy: 0.0132 - val_loss: 0.1174 - val_accuracy: 0.0141\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11743\n",
      "29/29 - 183s - loss: 0.1204 - accuracy: 0.0132 - val_loss: 0.1178 - val_accuracy: 0.0141\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11743 to 0.11690, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 173s - loss: 0.1216 - accuracy: 0.0132 - val_loss: 0.1169 - val_accuracy: 0.0141\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11690\n",
      "29/29 - 184s - loss: 0.1199 - accuracy: 0.0133 - val_loss: 0.1186 - val_accuracy: 0.0141\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11690\n",
      "29/29 - 194s - loss: 0.1201 - accuracy: 0.0132 - val_loss: 0.1181 - val_accuracy: 0.0141\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11690\n",
      "29/29 - 188s - loss: 0.1199 - accuracy: 0.0132 - val_loss: 0.1175 - val_accuracy: 0.0141\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.11690 to 0.11667, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 228s - loss: 0.1197 - accuracy: 0.0133 - val_loss: 0.1167 - val_accuracy: 0.0141\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11667\n",
      "29/29 - 214s - loss: 0.1196 - accuracy: 0.0133 - val_loss: 0.1167 - val_accuracy: 0.0141\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.11667\n",
      "29/29 - 206s - loss: 0.1195 - accuracy: 0.0133 - val_loss: 0.1175 - val_accuracy: 0.0141\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.11667\n",
      "29/29 - 198s - loss: 0.1193 - accuracy: 0.0135 - val_loss: 0.1178 - val_accuracy: 0.0141\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.11667 to 0.11664, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 201s - loss: 0.1191 - accuracy: 0.0135 - val_loss: 0.1166 - val_accuracy: 0.0141\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11664\n",
      "29/29 - 208s - loss: 0.1190 - accuracy: 0.0134 - val_loss: 0.1168 - val_accuracy: 0.0141\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11664\n",
      "29/29 - 230s - loss: 0.1188 - accuracy: 0.0132 - val_loss: 0.1182 - val_accuracy: 0.0141\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.11664 to 0.11641, saving model to best_chopin_model_LRLpt3_recreate.h5\n",
      "29/29 - 198s - loss: 0.1187 - accuracy: 0.0135 - val_loss: 0.1164 - val_accuracy: 0.0141\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11641\n",
      "29/29 - 214s - loss: 0.1187 - accuracy: 0.0133 - val_loss: 0.1166 - val_accuracy: 0.0141\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11641\n",
      "29/29 - 199s - loss: 0.1186 - accuracy: 0.0135 - val_loss: 0.1171 - val_accuracy: 0.0141\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-b7090ceff02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, batch_size = 512, epochs = 50, \\\n\u001b[0;32m----> 2\u001b[0;31m                     validation_data = (X_test, y_test), verbose = 2, callbacks = [mc])\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 50, \\\n",
    "                    validation_data = (X_test, y_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to perform even worse. Now let's try with alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm(n_lstm_layers = 4, n_dense_layers = 3, n_lstm_nodes = 512, dropout_rate = 0.6, leaky_alpha = 0.1)\n",
    "mc = ModelCheckpoint('best_chopin_model_LRLpt1_recreate.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17031, saving model to best_chopin_model_LRLpt1_recreate.h5\n",
      "29/29 - 215s - loss: 0.2997 - accuracy: 0.0156 - val_loss: 0.1703 - val_accuracy: 0.0141\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17031 to 0.12196, saving model to best_chopin_model_LRLpt1_recreate.h5\n",
      "29/29 - 204s - loss: 0.1539 - accuracy: 0.0156 - val_loss: 0.1220 - val_accuracy: 0.0141\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12196\n",
      "29/29 - 201s - loss: 0.1408 - accuracy: 0.0166 - val_loss: 0.1503 - val_accuracy: 0.0141\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12196\n",
      "29/29 - 199s - loss: 0.1331 - accuracy: 0.0147 - val_loss: 0.1247 - val_accuracy: 0.0141\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12196\n",
      "29/29 - 208s - loss: 0.1283 - accuracy: 0.0145 - val_loss: 0.1225 - val_accuracy: 0.0141\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12196 to 0.12183, saving model to best_chopin_model_LRLpt1_recreate.h5\n",
      "29/29 - 204s - loss: 0.1262 - accuracy: 0.0132 - val_loss: 0.1218 - val_accuracy: 0.0141\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12183 to 0.12097, saving model to best_chopin_model_LRLpt1_recreate.h5\n",
      "29/29 - 206s - loss: 0.1253 - accuracy: 0.0133 - val_loss: 0.1210 - val_accuracy: 0.0141\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12097 to 0.12087, saving model to best_chopin_model_LRLpt1_recreate.h5\n",
      "29/29 - 197s - loss: 0.1246 - accuracy: 0.0133 - val_loss: 0.1209 - val_accuracy: 0.0141\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12087\n",
      "29/29 - 201s - loss: 0.1240 - accuracy: 0.0132 - val_loss: 0.1213 - val_accuracy: 0.0141\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12087 to 0.12087, saving model to best_chopin_model_LRLpt1_recreate.h5\n",
      "29/29 - 202s - loss: 0.1235 - accuracy: 0.0132 - val_loss: 0.1209 - val_accuracy: 0.0141\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12087 to 0.12084, saving model to best_chopin_model_LRLpt1_recreate.h5\n",
      "29/29 - 199s - loss: 0.1232 - accuracy: 0.0132 - val_loss: 0.1208 - val_accuracy: 0.0141\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12084\n",
      "29/29 - 203s - loss: 0.1232 - accuracy: 0.0132 - val_loss: 0.1208 - val_accuracy: 0.0141\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12084\n",
      "29/29 - 202s - loss: 0.1230 - accuracy: 0.0132 - val_loss: 0.1221 - val_accuracy: 0.0141\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12084\n",
      "29/29 - 187s - loss: 0.1229 - accuracy: 0.0132 - val_loss: 0.1209 - val_accuracy: 0.0141\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12084\n",
      "29/29 - 205s - loss: 0.1227 - accuracy: 0.0132 - val_loss: 0.1211 - val_accuracy: 0.0141\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12084\n",
      "29/29 - 217s - loss: 0.1227 - accuracy: 0.0132 - val_loss: 0.1210 - val_accuracy: 0.0141\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.12084\n",
      "29/29 - 240s - loss: 0.1226 - accuracy: 0.0132 - val_loss: 0.1210 - val_accuracy: 0.0141\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12084\n",
      "29/29 - 176s - loss: 0.1225 - accuracy: 0.0132 - val_loss: 0.1209 - val_accuracy: 0.0141\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12084\n",
      "29/29 - 166s - loss: 0.1226 - accuracy: 0.0132 - val_loss: 0.1208 - val_accuracy: 0.0141\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12084\n",
      "29/29 - 166s - loss: 0.1223 - accuracy: 0.0132 - val_loss: 0.1213 - val_accuracy: 0.0141\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12084\n",
      "29/29 - 165s - loss: 0.1223 - accuracy: 0.0132 - val_loss: 0.1209 - val_accuracy: 0.0141\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-b7090ceff02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, batch_size = 512, epochs = 50, \\\n\u001b[0;32m----> 2\u001b[0;31m                     validation_data = (X_test, y_test), verbose = 2, callbacks = [mc])\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 50, \\\n",
    "                    validation_data = (X_test, y_test), verbose = 2, callbacks = [mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs even worse. Could try to increase alpha but that will decrease the effect of having a non-linear\n",
    "activation function. Better to try something else now.\n",
    "\n",
    "The best model is still best_chopin_model_3_2_512_pt4.h5. Let's go to even fewer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_musical_sequence(model, no_of_timesteps = 16, index = None, threshold = 0.5):\n",
    "    if (index is None):\n",
    "        index = np.random.randint(0, len(X_test) - 1)\n",
    "        print('random index is ', index)\n",
    "    random_music = X_test[index]\n",
    "    original_random_music = random_music.copy()\n",
    "    predictions_new = []\n",
    "    for i in range(no_of_timesteps):\n",
    "        random_music = random_music.reshape(1, no_of_timesteps, n_keys_piano + 1)                    \n",
    "        prob = model.predict(random_music)[0]\n",
    "        y_pred = [0 if p < threshold else 1 for p in prob[:-1]]\n",
    "        y_pred = np.insert(y_pred, len(y_pred), prob[-1])\n",
    "        # print(prob)\n",
    "        predictions_new.append(y_pred)\n",
    "        random_music = np.insert(random_music, len(random_music), y_pred, axis = 0)[1:, :]\n",
    "    \n",
    "    return original_random_music, np.array(predictions_new).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random index is  5617\n"
     ]
    }
   ],
   "source": [
    "random_chopin_sequence, new_chopin_sequence = generate_musical_sequence(model)\n",
    "convert_to_midi(random_chopin_sequence, 'random.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_to_midi(new_chopin_sequence, 'new.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
