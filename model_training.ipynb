{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data we created in data_read_and_process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('train_and_val/X_train.npy')\n",
    "X_val = np.load('train_and_val/X_val.npy')\n",
    "y_train = np.load('train_and_val/y_train.npy')\n",
    "y_val = np.load('train_and_val/y_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(n_lstm_layers = 2, n_dense_layers = 3, n_lstm_nodes = 512, dropout_rate = 0.6):\n",
    "    \"\"\"Generate a keras Sequential model of the form as described in Figure 16 of\n",
    "    https://www.tandfonline.com/doi/full/10.1080/25765299.2019.1649972\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_lstm_nodes, return_sequences = True, input_shape = (16, 89,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(1, n_lstm_layers - 1):\n",
    "        model.add(LSTM(n_lstm_nodes, return_sequences = True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(n_lstm_nodes))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_lstm_nodes // 2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(n_dense_layers - 1):\n",
    "        model.add(Dense(n_lstm_nodes // 2))\n",
    "        model.add(Dropout(0.6))\n",
    "    model.add(Dense(89))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'RMSProp')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss and Metrics\n",
    "\n",
    "\\begin{equation*}\n",
    "bce\\_loss = \\frac{1}{N} (\\sum_{i=1}^{N} y_i log(p(y_i)) + (1 - y_i) log(1 - p(y_i)))\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "maestro\\_loss = 2 * Harshness \\lvert\\frac{d_{true} - d_{pred}}{d_{true} + d{_{pred}}}\\rvert\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\\begin{equation*}\n",
    "total\\_loss = MIN(2 * bce\\_loss, bce\\_loss + maestro\\_loss)\n",
    "\\end{equation*}\n",
    "\n",
    "where N = num_keys_piano, <b>Harshness</b> is a constant to be determined, and <b>d</b> gives the normalized duration. I'll call it the <b>Maestro Loss Function</b> since it pays special attention to the timing of the notes. It is usually composed of a Binary Cross Entropy Term with an additional term proportional to the relative error in duration between $d_{true}$ and $d_{pred}$. However, we limit the total_loss to be less than twice the bce_loss. We also define custom metrics, read the docstrings for their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def maestro_loss_wr(harshness): \n",
    "    \"\"\"A loss function which, in addition to penalizing for misclassification on the \n",
    "    first n_keys_piano elements, includes a term proportional to the relative\n",
    "    error in the prediction of the last element (which repesents the duration). \n",
    "    The proportionality constant is the 'harshness' of the maestro in regards to\n",
    "    timing.\"\"\"\n",
    "    def maestro_loss(ytrue, ypred):\n",
    "        # Standard binary cross-entropy\n",
    "        bce_loss = - K.mean(ytrue[:, :-1] * K.log(ypred[:, :-1]) + (1 - ytrue[:, :-1]) * \\\n",
    "                     K.log(1 - ypred[:, :-1]))\n",
    "\n",
    "        # Duration error term\n",
    "        dur_loss = 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "        \n",
    "        if (dur_loss > bce_loss):   # Often times, ytrue[:, -1] elements will be zero\n",
    "            return bce_loss * 2     # This may spike dur_loss. To control, I limit it\n",
    "                                    # so that it never exceeds the bce_loss.\n",
    "        return bce_loss + dur_loss\n",
    "    \n",
    "    return maestro_loss\n",
    "\n",
    "def precision_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified precision excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    pred_positives = K.sum(K.round(ypred[:, :-1]))\n",
    "    return true_positives / (pred_positives + K.epsilon())\n",
    "\n",
    "def recall_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified recall excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(ytrue[:, :-1] * ypred[:, :-1]))\n",
    "    poss_positives = K.sum(ytrue[:, :-1])\n",
    "    return true_positives / (poss_positives + K.epsilon())\n",
    "\n",
    "def f1_score_mod(ytrue, ypred):\n",
    "    \"\"\"Just a modified f1_score excluding the last element (which is not a classification)\"\"\"\n",
    "\n",
    "    precision = precision_mod(ytrue, ypred)\n",
    "    recall = recall_mod(ytrue, ypred)   \n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "def dur_error(ytrue, ypred):\n",
    "    \"\"\"A new metric that only gives information on the error in duration predictions\"\"\"\n",
    "    \n",
    "    return 2 * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / (ytrue[:, -1] + ypred[:, -1] + \\\n",
    "                                                         K.epsilon())))\n",
    "\n",
    "def maestro_dur_loss_wr(harshness):\n",
    "    \"\"\"The second term of the maestro loss, based purely on error in duration predictions.\n",
    "    To be used as a metric in order to decompose the loss components during analysis\"\"\"\n",
    "    def maestro_dur_loss(ytrue, ypred):\n",
    "\n",
    "        return 2 * harshness * K.mean(K.abs((ytrue[:, -1] - ypred[:, -1]) / \\\n",
    "                                      (ytrue[:, -1] + ypred[:, -1] + K.epsilon())))\n",
    "    return maestro_dur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cols_dict(history):\n",
    "    \"\"\"return a mapping of desired column names to the corresponding columns in the\n",
    "    history dictionary (previously history.history where history is the return value\n",
    "    of model.train)\"\"\"\n",
    "    return {'maestro_loss': history['loss'], 'f1_score': history['f1_score_mod'], \\\n",
    " 'precision': history['precision_mod'], 'recall': history['recall_mod'], \\\n",
    " 'dur_error': history['dur_error'], 'dur_loss': history['maestro_dur_loss'], \\\n",
    " 'val_maestro_loss': history['val_loss'], 'val_f1_score': history['val_f1_score_mod'], \\\n",
    " 'val_precision': history['val_precision_mod'], 'val_recall': history['val_recall_mod'], \\\n",
    " 'val_dur_error': history['val_dur_error'], 'val_dur_loss': history['val_maestro_dur_loss']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17842, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 316s - loss: 0.2826 - f1_score_mod: 0.0105 - recall_mod: 0.0211 - precision_mod: 0.0767 - dur_error: 1.0439 - maestro_dur_loss: 0.1044 - val_loss: 0.1784 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5199 - val_maestro_dur_loss: 0.0520\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.17842\n",
      "50/50 - 330s - loss: 0.2047 - f1_score_mod: 2.6882e-05 - recall_mod: 1.3468e-05 - precision_mod: 0.0067 - dur_error: 0.7001 - maestro_dur_loss: 0.0700 - val_loss: 0.1869 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.6552 - val_maestro_dur_loss: 0.0655\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17842 to 0.17361, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 318s - loss: 0.1916 - f1_score_mod: 0.0017 - recall_mod: 8.3039e-04 - precision_mod: 0.2526 - dur_error: 0.6322 - maestro_dur_loss: 0.0632 - val_loss: 0.1736 - val_f1_score_mod: 0.0000e+00 - val_recall_mod: 0.0000e+00 - val_precision_mod: 0.0000e+00 - val_dur_error: 0.5475 - val_maestro_dur_loss: 0.0547\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17361 to 0.16120, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 322s - loss: 0.1842 - f1_score_mod: 0.0103 - recall_mod: 0.0052 - precision_mod: 0.4989 - dur_error: 0.5974 - maestro_dur_loss: 0.0597 - val_loss: 0.1612 - val_f1_score_mod: 0.0111 - val_recall_mod: 0.0056 - val_precision_mod: 0.7126 - val_dur_error: 0.4687 - val_maestro_dur_loss: 0.0469\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16120 to 0.16019, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 359s - loss: 0.1789 - f1_score_mod: 0.0273 - recall_mod: 0.0141 - precision_mod: 0.5440 - dur_error: 0.5869 - maestro_dur_loss: 0.0587 - val_loss: 0.1602 - val_f1_score_mod: 0.0454 - val_recall_mod: 0.0237 - val_precision_mod: 0.6235 - val_dur_error: 0.4838 - val_maestro_dur_loss: 0.0484\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16019 to 0.15673, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 343s - loss: 0.1749 - f1_score_mod: 0.0400 - recall_mod: 0.0208 - precision_mod: 0.5800 - dur_error: 0.5790 - maestro_dur_loss: 0.0579 - val_loss: 0.1567 - val_f1_score_mod: 0.0513 - val_recall_mod: 0.0270 - val_precision_mod: 0.5989 - val_dur_error: 0.4805 - val_maestro_dur_loss: 0.0481\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15673 to 0.15159, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 341s - loss: 0.1701 - f1_score_mod: 0.0509 - recall_mod: 0.0266 - precision_mod: 0.6287 - dur_error: 0.5552 - maestro_dur_loss: 0.0555 - val_loss: 0.1516 - val_f1_score_mod: 0.0597 - val_recall_mod: 0.0315 - val_precision_mod: 0.6519 - val_dur_error: 0.4453 - val_maestro_dur_loss: 0.0445\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15159\n",
      "50/50 - 371s - loss: 0.1716 - f1_score_mod: 0.0600 - recall_mod: 0.0317 - precision_mod: 0.6188 - dur_error: 0.5724 - maestro_dur_loss: 0.0572 - val_loss: 0.1741 - val_f1_score_mod: 0.0918 - val_recall_mod: 0.0496 - val_precision_mod: 0.6525 - val_dur_error: 0.6625 - val_maestro_dur_loss: 0.0662\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.15159 to 0.14695, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 383s - loss: 0.1654 - f1_score_mod: 0.0716 - recall_mod: 0.0381 - precision_mod: 0.6379 - dur_error: 0.5365 - maestro_dur_loss: 0.0536 - val_loss: 0.1469 - val_f1_score_mod: 0.0591 - val_recall_mod: 0.0309 - val_precision_mod: 0.7605 - val_dur_error: 0.4114 - val_maestro_dur_loss: 0.0411\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14695 to 0.14092, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 407s - loss: 0.1633 - f1_score_mod: 0.0848 - recall_mod: 0.0455 - precision_mod: 0.6486 - dur_error: 0.5243 - maestro_dur_loss: 0.0524 - val_loss: 0.1409 - val_f1_score_mod: 0.0773 - val_recall_mod: 0.0409 - val_precision_mod: 0.7468 - val_dur_error: 0.3703 - val_maestro_dur_loss: 0.0370\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.14092 to 0.13729, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 349s - loss: 0.1618 - f1_score_mod: 0.0898 - recall_mod: 0.0484 - precision_mod: 0.6527 - dur_error: 0.5181 - maestro_dur_loss: 0.0518 - val_loss: 0.1373 - val_f1_score_mod: 0.1023 - val_recall_mod: 0.0553 - val_precision_mod: 0.7111 - val_dur_error: 0.3449 - val_maestro_dur_loss: 0.0345\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13729\n",
      "50/50 - 345s - loss: 0.1604 - f1_score_mod: 0.1008 - recall_mod: 0.0550 - precision_mod: 0.6466 - dur_error: 0.5164 - maestro_dur_loss: 0.0516 - val_loss: 0.1459 - val_f1_score_mod: 0.1026 - val_recall_mod: 0.0555 - val_precision_mod: 0.7280 - val_dur_error: 0.4314 - val_maestro_dur_loss: 0.0431\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13729\n",
      "50/50 - 364s - loss: 0.1586 - f1_score_mod: 0.1115 - recall_mod: 0.0611 - precision_mod: 0.6592 - dur_error: 0.5047 - maestro_dur_loss: 0.0505 - val_loss: 0.1493 - val_f1_score_mod: 0.1216 - val_recall_mod: 0.0668 - val_precision_mod: 0.6922 - val_dur_error: 0.4752 - val_maestro_dur_loss: 0.0475\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13729 to 0.13462, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 338s - loss: 0.1570 - f1_score_mod: 0.1188 - recall_mod: 0.0655 - precision_mod: 0.6638 - dur_error: 0.5008 - maestro_dur_loss: 0.0501 - val_loss: 0.1346 - val_f1_score_mod: 0.1360 - val_recall_mod: 0.0756 - val_precision_mod: 0.6985 - val_dur_error: 0.3428 - val_maestro_dur_loss: 0.0343\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13462\n",
      "50/50 - 340s - loss: 0.1542 - f1_score_mod: 0.1259 - recall_mod: 0.0699 - precision_mod: 0.6590 - dur_error: 0.4820 - maestro_dur_loss: 0.0482 - val_loss: 0.1399 - val_f1_score_mod: 0.1336 - val_recall_mod: 0.0742 - val_precision_mod: 0.6874 - val_dur_error: 0.3974 - val_maestro_dur_loss: 0.0397\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13462\n",
      "50/50 - 334s - loss: 0.1534 - f1_score_mod: 0.1382 - recall_mod: 0.0774 - precision_mod: 0.6649 - dur_error: 0.4811 - maestro_dur_loss: 0.0481 - val_loss: 0.1410 - val_f1_score_mod: 0.1418 - val_recall_mod: 0.0796 - val_precision_mod: 0.6696 - val_dur_error: 0.4052 - val_maestro_dur_loss: 0.0405\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.13462 to 0.13274, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 336s - loss: 0.1516 - f1_score_mod: 0.1459 - recall_mod: 0.0822 - precision_mod: 0.6677 - dur_error: 0.4715 - maestro_dur_loss: 0.0471 - val_loss: 0.1327 - val_f1_score_mod: 0.1524 - val_recall_mod: 0.0856 - val_precision_mod: 0.7101 - val_dur_error: 0.3366 - val_maestro_dur_loss: 0.0337\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13274\n",
      "50/50 - 331s - loss: 0.1504 - f1_score_mod: 0.1515 - recall_mod: 0.0857 - precision_mod: 0.6663 - dur_error: 0.4647 - maestro_dur_loss: 0.0465 - val_loss: 0.1386 - val_f1_score_mod: 0.1598 - val_recall_mod: 0.0905 - val_precision_mod: 0.7004 - val_dur_error: 0.4045 - val_maestro_dur_loss: 0.0405\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.13274 to 0.12696, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 335s - loss: 0.1495 - f1_score_mod: 0.1568 - recall_mod: 0.0890 - precision_mod: 0.6752 - dur_error: 0.4601 - maestro_dur_loss: 0.0460 - val_loss: 0.1270 - val_f1_score_mod: 0.1638 - val_recall_mod: 0.0928 - val_precision_mod: 0.7120 - val_dur_error: 0.2989 - val_maestro_dur_loss: 0.0299\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12696\n",
      "50/50 - 338s - loss: 0.1472 - f1_score_mod: 0.1652 - recall_mod: 0.0945 - precision_mod: 0.6759 - dur_error: 0.4477 - maestro_dur_loss: 0.0448 - val_loss: 0.1301 - val_f1_score_mod: 0.1534 - val_recall_mod: 0.0860 - val_precision_mod: 0.7314 - val_dur_error: 0.3244 - val_maestro_dur_loss: 0.0324\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.12696 to 0.12565, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 355s - loss: 0.1470 - f1_score_mod: 0.1676 - recall_mod: 0.0961 - precision_mod: 0.6750 - dur_error: 0.4481 - maestro_dur_loss: 0.0448 - val_loss: 0.1256 - val_f1_score_mod: 0.1782 - val_recall_mod: 0.1024 - val_precision_mod: 0.6987 - val_dur_error: 0.2959 - val_maestro_dur_loss: 0.0296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12565\n",
      "50/50 - 350s - loss: 0.1454 - f1_score_mod: 0.1732 - recall_mod: 0.0997 - precision_mod: 0.6720 - dur_error: 0.4394 - maestro_dur_loss: 0.0439 - val_loss: 0.1269 - val_f1_score_mod: 0.1830 - val_recall_mod: 0.1057 - val_precision_mod: 0.6950 - val_dur_error: 0.3084 - val_maestro_dur_loss: 0.0308\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12565\n",
      "50/50 - 361s - loss: 0.1450 - f1_score_mod: 0.1827 - recall_mod: 0.1058 - precision_mod: 0.6834 - dur_error: 0.4421 - maestro_dur_loss: 0.0442 - val_loss: 0.1267 - val_f1_score_mod: 0.1782 - val_recall_mod: 0.1018 - val_precision_mod: 0.7334 - val_dur_error: 0.3151 - val_maestro_dur_loss: 0.0315\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12565 to 0.12464, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 345s - loss: 0.1437 - f1_score_mod: 0.1868 - recall_mod: 0.1086 - precision_mod: 0.6789 - dur_error: 0.4357 - maestro_dur_loss: 0.0436 - val_loss: 0.1246 - val_f1_score_mod: 0.1864 - val_recall_mod: 0.1072 - val_precision_mod: 0.7293 - val_dur_error: 0.2981 - val_maestro_dur_loss: 0.0298\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12464 to 0.12445, saving model to models/best_maestro_model_2_1_512_pt4.h5\n",
      "50/50 - 353s - loss: 0.1430 - f1_score_mod: 0.1893 - recall_mod: 0.1103 - precision_mod: 0.6818 - dur_error: 0.4330 - maestro_dur_loss: 0.0433 - val_loss: 0.1244 - val_f1_score_mod: 0.1839 - val_recall_mod: 0.1055 - val_precision_mod: 0.7366 - val_dur_error: 0.3015 - val_maestro_dur_loss: 0.0302\n",
      "Epoch 26/100\n"
     ]
    }
   ],
   "source": [
    "model = lstm(n_lstm_layers = 2, n_dense_layers = 1, n_lstm_nodes = 512, dropout_rate = 0.4)\n",
    "opt = RMSprop()\n",
    "model.compile(loss = maestro_loss_wr(0.1), optimizer = opt, metrics = [f1_score_mod, recall_mod, precision_mod, dur_error, maestro_dur_loss_wr(0.1)])\n",
    "mc = ModelCheckpoint('models/best_maestro_model_2_1_512_pt4.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 512, epochs = 100, \\\n",
    "                    validation_data = (X_val, y_val), verbose = 2, callbacks = [mc, TerminateOnNaN()])\n",
    "if (len(history.history['val_loss']) < len(history.history['loss'])):  # a NaN during training\n",
    "    for key, value in history.history.items():\n",
    "        if (key[:3] == 'val'):          # pd.DataFrame requires value lengths to be equal\n",
    "            value.append(np.nan)  \n",
    "df = pd.DataFrame(generate_cols_dict(history.history))\n",
    "df.index.name = 'Epochs'\n",
    "df.to_csv('model_data/best_maestro_model_2_1_512_pt4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
